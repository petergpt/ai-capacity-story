Name,Status,Certainty,Single cluster?,Max OP/s (log),H100 equivalents,Chip type (primary),Chip quantity (primary),Country,Owner,First Operational Date,Note,Sector,Power Capacity (MW),Hardware Cost,Location,Users,Hardware note,Quote,First Operational Date Note,Certainty Note,Energy Efficiency (log),Builds Upon,Superseded by,Possible Duplicate,Possible Duplicate Of,Chip type (secondary),Chip quantity (secondary),Total number of AI chips,GPU Supplier (primary),GPU supplier (secondary),Include in Standard Analysis,Exclude,Rank when first operational,16-bit OP/s (log),Max OP/s,8-bit OP/s,16-bit OP/s,32-bit OP/s,Calculated Power Capacity (MW),Reported Power Capacity (MW),Energy Efficiency,Calculated Cost,Reported Cost,Reported Cost (Inflation adjusted),Cost Quote,Noteworthy,Decommissioned Date (if applicable),Largest existing cluster when first operational,% of largest cluster when first operational,Source 1,Source 2,Source 3,Source 4,Source 5,latitude,longitude
Abu Dhabi UAE/USA 5GW Campus Phase 2,Planned,Likely,Unclear,22.60314437262018,20262758.969474595,,,United Arab Emirates,,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Public/Private,5000,,"Abu Dhabi, United Arab Emirates","US Companies,G42",Uncertain,"""Breaking ground on 1GW AI Datacenter, part of a planned 5GW UAE-US artificial intelligence campus; largest such deployment outside of the US""
""The campus, which will span 10 square miles within the Emirate, will be built by G42 and operated in partnership with several US companies.""",Planned 2030,,12.604226053084469,Stargate UAE Phase 2,,,,,,,Unknown,,,,,22.303196057420486,4.01e+22,4.01e+22,2.01e+22,,,5000,4020000000000,,,,,True,,,,https://web.archive.org/web/20250515200336/https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu,,,,,24.453524,54.377438
Meta $200B Campus Rumor,Planned,Unlikely,Yes,22.60314437262018,20262758.969474595,,,United States of America,Meta AI,,"FLOP/s is a very rough estimate based on stated power capacity  (~5-7GW) and the completion year. Plans for this cluster are very much still rumors/vauge, and it's highly likely that they're changed. 
2030 is a rough estiamted date for when this might be completed",Private,5000,200000000000,,Meta,Uncertain,"""could cost over $200 billion""",Planned 2030,,12.604226053084469,,,,,,,,Unknown,,,,,22.303196057420486,4.01e+22,4.01e+22,2.01e+22,,,5000,4020000000000,,200000000000,200000000000,$200B,,,,,https://web.archive.org/web/20250226194100/https://finance.yahoo.com/news/meta-weighs-200b-ai-data-134016347.html,,,,,39.381266,-97.922211
DataVolt Neom 1.5 GW Phase 2,Planned,Likely,Unclear,22.00432137378264,5103587.670615824,,,Saudi Arabia,DataVolt,,Planned year is a very rough guess,Private,1500,,"27°33'21.6""N 35°32'18.4""E",,Uncertain,"""At the LEAP 2025 event this week DataVolt and Neom signed a $5bn agreement to develop a 1.5GW data center campus in Neom’s Oxagon industrial area.
The first 300MW phase of the site will reportedly go live in 2028""",Planned 2031,,12.523746466811563,DataVolt Neom 1.5 GW Phase 1,,,,,,,Unknown,,,,,21.699837725867244,1.01e+22,1.01e+22,5.01e+21,,,1500,3339999999999.9995,,5000000000,,,,,,,http://web.archive.org/web/20250404231606/https://www.datacenterdynamics.com/en/news/datavolt-plans-15gw-data-center-campus-in-neoms-oxagon/,,,,,24.677954,46.63337
Meta Louisiana Datacenter,Planned,Likely,Unclear,22.00432137378264,5103587.670615824,,,United States of America,Meta AI,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Private,2260,,"Holly Ridge
Louisiana 71269",Meta,Uncertain,"""Mark Zuckerberg said the site would total more than 2GW at full build-out and would be used to train its Llama AI models.""
""This will include the construction of three combined-cycle combustion turbines with a combined capacity of 2,260MW""",Planned 2030,,12.345729286719843,,,,,,,,Unknown,,,,,21.699837725867244,1.01e+22,1.01e+22,5.01e+21,,,2260,2216814159292.035,,,,,,,,,https://archive.ph/rS3ef,,,,,32.477649,-91.758659
OpenAI Stargate Abilene Oracle OCI Supercluster Phase 3,Planned,Likely,Yes,22.00432137378264,5103587.670615824,,,United States of America,Oracle,,"""That cluster is expected to be leased to Oracle, which in turn will lease it to Microsoft, who will rent it to OpenAI""
Crusoe is involved in this
FLOP/s figure is a very rough estimate based off some rumored specifications about chip performance",Private,2200,,"5502 Spinks Rd, Abilene, TX 79601","Cloud,Microsoft,OpenAI","VR200,GB300,GB200","""The initial plan here was a 1GW facility; SA now thinks it’s going to have a total of 2.2GW of power supplied and a critical IT capacity of 1.8GW.""
""SA estimates the campus will house a previously-announced 100k GB200 cluster, as well as a 200k GB300 cluster and a 400k VR200 cluster""",Planned 2027,Low confidence since plans are still somewhat up in the air,12.357415045045038,OpenAI Stargate Abilene Oracle OCI Supercluster Phase 2,,,,,,,Unknown,,,,,21.699837725867244,1.01e+22,1.01e+22,5.01e+21,,,2200,2277272727272.727,,,,,True,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://archive.ph/UeNZs,https://web.archive.org/web/20250124161138/https://www.transformernews.ai/p/unravelling-the-stargate-spin,,,32.49429,-99.79316
South Korea Planned 3GW Cluster,Planned,Likely,Unclear,22.00432137378264,5103587.670615824,,,Korea (Republic of),,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Private,3000,,southwest South Korea,,Uncertain,"If completed as envisioned by its backers, the data center will cost as much as $35 billion and pack up to 3 gigawatts of power.",Planned 2028,,12.222716471147582,,,,,,,,Unknown,,,,,21.699837725867244,1.01e+22,1.01e+22,5.01e+21,,,3000,1669999999999.9998,,,,,,,,,https://www.wsj.com/tech/ai/ai-data-center-with-up-to-3-gigawatts-of-power-is-envisioned-for-south-korea-5141bd77,,,,,36.536236,128.168944
Stargate UAE Phase 2,Planned,Likely,Unclear,21.699837725867244,2531581.6069094315,,,United Arab Emirates,Stargate (OpenAI),,"Part of the broader planned 5GW UAE/USA cluster
Planned date is a rough guess
FLOP/s is a very very rough guess based on power capacity and expected completion date",Public/Private,1000,,"Abu Dhabi, United Arab Emirates",OpenAI,Uncertain,"""Breaking ground on 1GW AI Datacenter, part of a planned 5GW UAE-US artificial intelligence campus; largest such deployment outside of the US""
""The campus, which will span 10 square miles within the Emirate, will be built by G42 and operated in partnership with several US companies.""
""Stargate UAE is a next-generation AI infrastructure cluster that will run in the newly established 5-gigawatt UAE–U.S. AI Campus in Abu Dhabi. Stargate UAE, a 1-gigawatt compute cluster""",Planned 2029,,12.399673721481038,Stargate UAE Phase 1,,,,,,,Unknown,,,,,21.399673721481037,5.01e+21,5.01e+21,2.51e+21,,,1000,2510000000000,,,,,,,,,https://web.archive.org/web/20250515200336/https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu,https://archive.ph/hQWdM,https://archive.ph/PfVwI,,,24.453524,54.377438
xAI Colossus 2 Memphis,Planned,Likely,Yes,21.64303792468132,2221222.8398507982,NVIDIA GB200 NVL2,800000,United States of America,xAI,,"Very unclear what the actual details of this will end up being. So far we know that
- it was 200k Hoppers in March 2025, and Elon claims that it will scale up to at a minimum 1 million GPUs. Presumably, the remaining GPUs (so at least 800k) will be Blackwell GPUs
- Elon has claimed that it will be at least a Gigawatt of power. The million GPUs would imply ~2GW
- Elon has claimed that this will be complete in 6-9 months, so presumably early-mid 2026
- Slightly unclear whether this is an extension of the earlier 200k Colossus supercomputer, or if it's a new facility entirely, but Elon says that Colossus 2 will be near Memphis, which is where the original Colossus is",Private,2202.2,,"3231 Riverport Rd, Memphis, TN 38109",xAI,"Uncertain,H100,Blackwell","The expansion, already underway, will incorporate a minimum of one million Graphics Processing Units (GPUs)",Planned 2026,,11.999147218793906,xAI Colossus Memphis Phase 2,,,,NVIDIA H100 SXM5 80GB,200000,1000000,NVIDIA,NVIDIA,,,,21.34200397709543,4.3958e+21,4.3958e+21,2.19788e+21,1.09894e+21,2202.2,,998038325311.0526,,,,,True,,,,https://web.archive.org/web/20241205161745/https://memphischamber.com/blog/general/xai-memphis-announces-expansion-of-supercomputer-with-addition-of-tech-companies-in-digital-delta/,https://www.cnbc.com/2025/05/20/elon-musk-says-ai-could-run-into-power-capacity-issues-by-middle-of-next-year.html,,,,35.069154,-90.118494
"""OpenAI/Microsoft Mt Pleasant, Wisconsin Phase 2""",Planned,Likely,Yes,21.544068044350272,1768569.9848668606,NVIDIA GB200 NVL2,700000,United States of America,"OpenAI,Microsoft",,"Rough estimate of chip count from stated power capacity of datacenter. 

Estimate from Semianalysis that Microsoft plans to build four of the 300MW buildings
What type of GPUs is unclear, but I'm assuming they're B200s",Private,1500,,"M39X+XG, Mt Pleasant, WI 53177","OpenAI,Microsoft","GB200,Uncertain","""1.5GW within 2.5 years""
""Microsoft plans to build four 300MW buildings... 1.5GW is like a million GPUs""",Planned 2026,"They announced Jan 2025 that they are currently pausing some planned construction
Estimate from Semianalysis that Microsoft plans to build four of the 300MW buildings, which translates to ~1 million GPUs. Type of GPU uncertain",12.066946789630613,"OpenAI/Microsoft Mt Pleasant, Wisconsin Phase 1",,,,,,700000,NVIDIA,,,,,21.24303804868629,3.5e+21,3.5e+21,1.75e+21,875000000000000000000,1681.6800000000003,1500,1166666666666.6667,,,,,,,,,https://web.archive.org/web/20240717135010/https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer,https://web.archive.org/web/20240915193816/https://www.datacenterdynamics.com/en/news/microsoft-openai-consider-100bn-5gw-stargate-ai-data-center-report/,https://www.youtube.com/watch?v=hobvps-H38o,,,42.697906,-87.890036
HUMAIN Saudi Arabia Phase 2,Planned,Likely,Unclear,21.47856649559384,1520970.186985496,,,Saudi Arabia,Humain,,"Seems to be partnering with AMD and NVIDIA
FLOP/s is a very rough estimate based on stated power capacity and the completion year",Public/Private,500,,,,Uncertain,"""HUMAIN is making a major investment to build AI factories in the Kingdom of Saudi Arabia with a projected capacity of up to 500 megawatts powered by several hundred thousand of NVIDIA’s most advanced GPUs over the next five years. The first phase of deployment will be an 18,000 NVIDIA GB300 Grace Blackwell AI supercomputer with NVIDIA InfiniBand networking.""",Planned 2030,,12.47741068790725,HUMAIN Saudi Arabia/NVIDIA Phase 1,,,,,,,Unknown,,,,,21.176380692243267,3.01e+21,3.01e+21,1.501e+21,,,500,3002000000000,,,,,,,,,https://archive.ph/vvAxQ,https://archive.ph/CFcnD,http://web.archive.org/web/20250517061228/https://ir.amd.com/news-events/press-releases/detail/1250/amd-and-humain-form-strategic-10b-collaboration-to-advance-global-ai,,,23.384784,44.652426
Fluidstack France Gigawatt Campus,Planned,Likely,Yes,21.397940008672034,1263264.2749049,NVIDIA GB200 NVL2,500000,France,,,Unclear if the 500k chips refers to GPUs or entire chips (which have 2 GPUs),Public/Private,1000,,France,Mistral,"GB200,Uncertain","""France and UAE to invest billions into 1GW European AI data center""
""The facility’s Phase 1 will ultimately host close to 500,000 next-generation AI chips.""",Planned 2028,,12.096910013008056,,,,,,,500000,NVIDIA,,,,,21.096910013008053,2.5e+21,2.5e+21,1.25e+21,625000000000000000000,1201.2,1000,1250000000000,,,,,,,,,https://web.archive.org/web/20250211011508/https://www.datacenterdynamics.com/en/news/france-and-uae-to-invest-billions-into-1gw-european-ai-data-center/,https://web.archive.org/web/20250210220905/https://www.hpcwire.com/off-the-wire/fluidstack-to-build-1gw-ai-supercomputer-in-france/,,,,47.824905,2.618787
Reliance Industries Supercomputer,Planned,Likely,Yes,21.35218251811136,1136937.847414411,NVIDIA GB200 NVL2,450000,India,Reliance Industries,,Chip count estimated from power capacity. Chip type uncertain,Private,1000,,"Jamnagar, India","Reliance Jio,Cloud",GB200,will supply its Blackwell AI processors for a one-gigawatt data centre Reliance is building in the western state of Gujarat,Planned 2027,,12.05115252244738,,,,,,,450000,NVIDIA,,,,,21.05115252244738,2.25e+21,2.25e+21,1.125e+21,562500000000000000000,1081.08,1000,1125000000000,,,,,,,,,https://archive.ph/tTvHo,,,,,22.470276,70.07109
OpenAI Stargate Abilene Oracle OCI Supercluster Phase 2,Planned,Likely,Yes,21.301031624265235,1010615.2097167542,NVIDIA GB300,200001,United States of America,Oracle,,"Unclear exact FLOPs for GB300, but sources indicate that it is 1.5x a GB200. 
""That cluster is expected to be leased to Oracle, which in turn will lease it to Microsoft, who will rent it to OpenAI""
Crusoe is involved in this",Private,800.8028028000001,,"5502 Spinks Rd, Abilene, TX 79601","Cloud,Microsoft,OpenAI","GB200,GB300","""SA estimates the campus will house a previously-announced 100k GB200 cluster, as well as a 200k GB300 cluster and a 400k VR200 cluster""",Planned 2026,Low confidence since plans are still somewhat up in the air,12.096476044101964,OpenAI Stargate Abilene Oracle OCI Supercluster Phase 1,,,,NVIDIA GB200 NVL2,100000,300001,NVIDIA,NVIDIA,,,,21.00000162860125,2.0000075e+21,2.0000075e+21,1.00000375e+21,125000000000000000000,800.8028028000001,,1248751560937.9683,,,,,,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://archive.ph/UeNZs,https://web.archive.org/web/20250124161138/https://www.transformernews.ai/p/unravelling-the-stargate-spin,,,32.49429,-99.79316
OpenAI/Microsoft Atlanta,Planned,Likely,Yes,21.130333768495003,682162.7084486468,NVIDIA B200,300000,United States of America,"OpenAI,Microsoft",,"Rough estimate of chip count from stated power capacity of datacenter. 
Assuming chips are B200s",Private,700,12521119482.929527,"1435 Hwy 54 W, Fayetteville, GA 30214","OpenAI,Microsoft","B200,Uncertain","""700MW within 1.5 years""",Planned 2026,Rought estimate from power capacity,11.984205732816767,,,,,,,300000,NVIDIA,,,,,20.82930377283102,1.35e+21,1.35e+21,675000000000000000000,337500000000000000000,600.6,700,964285714285.7142,12521119482.929527,,,,,,,,https://www.youtube.com/watch?v=hobvps-H38o (this cluster mentioned at 15:01),,,,,33.45079,-84.52385
DataVolt Neom 1.5 GW Phase 1,Planned,Likely,Unclear,21.00432137378264,510358.7670615824,,,Saudi Arabia,DataVolt,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Private,300,,"27°33'21.6""N 35°32'18.4""E",,Uncertain,"""At the LEAP 2025 event this week DataVolt and Neom signed a $5bn agreement to develop a 1.5GW data center campus in Neom’s Oxagon industrial area.
The first 300MW phase of the site will reportedly go live in 2028""",Planned 2028,,12.222716471147583,,,,,,,,Unknown,,,,,20.699837725867244,1.01e+21,1.01e+21,501000000000000000000,,,300,1670000000000,,,,,,,,,http://web.archive.org/web/20250404231606/https://www.datacenterdynamics.com/en/news/datavolt-plans-15gw-data-center-campus-in-neoms-oxagon/,,,,,24.677954,46.63337
Applied Digital Ellendale Possible Phase 3,Planned,Likely,Yes,20.954242509439325,454775.1389657682,NVIDIA GB200 NVL2,180000,United States of America,Applied Digital,,"It's unclear whether all the GPUs will be intereconnected. The CEO says that it's designed in a way that they can be all interconnected, though it's unclear if they will be. It is also somewhat unclear what type of GPUs they will be using, though I am assuming Blackwell GPUs here
Will be rented to CoreWeave
Number of GPUs estimated from power capacity
CoreWeave holds the option for this building",Private,400,3290034407.1518517,"9685 87th Ave SE, Ellendale, ND 58436","Cloud,CoreWeave",Uncertain,"Applied Digital expects the first 100 MW data center for CoreWeave to be ready for service in the fourth quarter of calendar 2025. The second building, which is expected to house a 150 MW data center, is currently under construction and is expected to be ready for service in the middle of 2026. Additionally, CoreWeave holds an option for the third 150 MW building, which is currently in the planning stages, anticipated to be ready for service in 2027.",Planned 2027,,12.05115252244738,Applied Digital CoreWeave Ellendale Phase 2,,,,,,180000,NVIDIA,,,,,20.653212513775344,900000000000000000000,900000000000000000000,450000000000000000000,225000000000000000000,432.4320000000001,400,1125000000000,3290034407.1518517,,,,,,,,https://web.archive.org/web/20240622003111/https://marketscale.com/industries/software-and-technology/applied-digital-and-nvidia-are-solving-ai-applications-efficiency-crisis-with-next-gen-data-centers/,https://web.archive.org/web/20250603030527/https://ir.applieddigital.com/news-events/press-releases/detail/123/applied-digital-announces-250mw-ai-data-center-lease-with,,,,46.01282,-98.572441
Nebius New Jersey,Planned,Likely,Yes,20.875061263391697,378979.2824714702,NVIDIA GB200 NVL2,150000,United States of America,Nebius AI,,Rough estimate of chip count from stated power capacity of datacenter. ,Private,300,,"New Jersey, USA",Cloud,"GB200,Blackwell,Uncertain","""AI infrastructure company Nebius has announced plans to build a 300MW data center in New Jersey.""
""Nebius today confirmed that its recently announced data center in New Jersey will be dedicated solely to NVIDIA Blackwell-architecture GPUs""",Planned 2026,,12.096910013008056,,,,,,,150000,NVIDIA,,,,,20.574031267727715,750000000000000000000,750000000000000000000,375000000000000000000,187500000000000000000,360.36,300,1250000000000,,,,,,,,,https://archive.ph/QP6zP#selection-1349.0-1349.96,https://archive.ph/N4MzN,,,,40.150248,-74.389317
"""OpenAI/Microsoft Mt Pleasant, Wisconsin Phase 1""",Planned,Likely,Yes,20.875061263391697,378979.2824714702,NVIDIA GB200 NVL2,150000,United States of America,"OpenAI,Microsoft",,"Rough estimate of chip count from stated power capacity of datacenter. 
Estimate from Semianalysis that that this is a 300MW building
What type of GPUs is unclear, but I'm assuming they're GB200s",Private,300,,"M39X+XG, Mt Pleasant, WI 53177","OpenAI,Microsoft","GB200,Uncertain","Ahead of Stargate, the companies plan to build a Phase 4 supercomputer in Mount Pleasant, Wisconsin, with operations starting in 2026.",Planned 2026,Rough estiamate of number of GPUs from power consupmtion estimate. Type of GPU uncertain,12.096910013008056,,,,,,,150000,NVIDIA,,,,,20.574031267727715,750000000000000000000,750000000000000000000,375000000000000000000,187500000000000000000,360.36,300,1250000000000,,,,,,,,,https://web.archive.org/web/20240717135010/https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer,https://web.archive.org/web/20240915193816/https://www.datacenterdynamics.com/en/news/microsoft-openai-consider-100bn-5gw-stargate-ai-data-center-report/,https://www.youtube.com/watch?v=hobvps-H38o,,,42.697906,-87.890036
Sesterce Grand Est France A,Planned,Likely,Yes,20.875061263391697,378979.2824714702,NVIDIA GB200 NVL2,150000,France,Sesterce,,"Unclear what type of GPUs it will use
It seems very likely that they're using Blackwell. However, given that, the power capacity they cite does not line up with the chip counts they cite. I will assume that the MW figure they cite is accurate, and extrapolate chip counts from there",Private,300,,"Grand Est, Fance",Cloud,Uncertain,"Sesterce is aiming to add 600MW of capacity across two data centers in Grand Est with 500,000 GPUs by 2028, and 1.2 GW with more than one million GPUs by 2030.",Planned 2028,,12.096910013008056,,,,,,,150000,NVIDIA,,,,,20.574031267727715,750000000000000000000,750000000000000000000,375000000000000000000,187500000000000000000,360.36,300,1250000000000,,,,,,,,,https://archive.ph/33xgc,,,,,48.584614,7.750713
Sesterce Grand Est France B,Planned,Likely,Yes,20.875061263391697,378979.2824714702,NVIDIA GB200 NVL2,150000,France,Sesterce,,"Unclear what type of GPUs it will use
It seems very likely that they're using Blackwell. However, given that, the power capacity they cite does not line up with the chip counts they cite. I will assume that the MW figure they cite is accurate, and extrapolate chip counts from there",Private,300,,"Grand Est, Fance",Cloud,Uncertain,"Sesterce is aiming to add 600MW of capacity across two data centers in Grand Est with 500,000 GPUs by 2028, and 1.2 GW with more than one million GPUs by 2030.",Planned 2028,,12.096910013008056,,,,,,,150000,NVIDIA,,,,,20.574031267727715,750000000000000000000,750000000000000000000,375000000000000000000,187500000000000000000,360.36,300,1250000000000,,,,,,,,,https://archive.ph/33xgc,,,,,48.584614,7.750713
Sesterce Southern France 250MW,Planned,Likely,Yes,20.77815125038364,303183.42597717623,NVIDIA GB200 NVL2,120000,France,Sesterce,,"Unclear what type of GPUs it will use
It seems very likely that they're using Blackwell. However, given that, the power capacity they cite does not line up with the chip counts they cite. I will assume that the MW figure they cite is accurate, and extrapolate chip counts from there",Private,250,,Southern France,Cloud,Uncertain,"the company has said it will establish a 250MW supercomputer with 200,000 GPUs in southern France",Planned 2028,,12.079181246047623,,,,,,,120000,NVIDIA,,,,,20.47712125471966,600000000000000000000,600000000000000000000,300000000000000000000,150000000000000000000,288.288,250,1200000000000,,,,,,,,,https://archive.ph/33xgc,,,,,43.696262,7.264145
Oracle OCI Supercluster B200s,Planned,Likely,Yes,20.77072244006302,298041.43507260404,NVIDIA B200,131072,United States of America,Oracle,,Uncertain if this will actually be built,Private,262.40614400000004,6564672691.466155,,Cloud,B200,"OCI Supercluster with up to 131,072 NVIDIA B200 Tensor Core GPUs",Planned 2025,,12.05071844496806,,,,,,,131072,NVIDIA,,,,,20.46969244439904,589824000000000000000,589824000000000000000,294912000000000000000,147456000000000000000,262.40614400000004,,1123876123876.1238,6564672691.466155,,,,,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,,,,,39.381266,-97.922211
Applied Digital CoreWeave Ellendale Phase 2,Planned,Likely,Yes,20.74036268949424,277918.1404790782,NVIDIA GB200 NVL2,110000,United States of America,Applied Digital,,"It's unclear whether all the GPUs will be intereconnected. The CEO says that it's designed in a way that they can be all interconnected, though it's unclear if they will be. It is also somewhat unclear what type of GPUs they will be using, though I am assuming Blackwell GPUs here
Will be rented to CoreWeave
Number of GPUs estimated from power capacity",Private,250,3290034407.1518517,"9685 87th Ave SE, Ellendale, ND 58436","Cloud,CoreWeave",Uncertain,"Applied Digital expects the first 100 MW data center for CoreWeave to be ready for service in the fourth quarter of calendar 2025. The second building, which is expected to house a 150 MW data center, is currently under construction and is expected to be ready for service in the middle of 2026. Additionally, CoreWeave holds an option for the third 150 MW building, which is currently in the planning stages, anticipated to be ready for service in 2027.",Planned 2026,,12.041392685158224,Applied Digital CoreWeave Ellendale Phase 1,,,,,,110000,NVIDIA,,,,,20.43933269383026,550000000000000000000,550000000000000000000,275000000000000000000,137500000000000000000,264.264,250,1100000000000,3290034407.1518517,,,,,,,,https://web.archive.org/web/20240622003111/https://marketscale.com/industries/software-and-technology/applied-digital-and-nvidia-are-solving-ai-applications-efficiency-crisis-with-next-gen-data-centers/,https://web.archive.org/web/20250603030527/https://ir.applieddigital.com/news-events/press-releases/detail/123/applied-digital-announces-250mw-ai-data-center-lease-with,,,,46.01282,-98.572441
CoreWeave Denton GB200s OpenAI/Microsoft,Planned,Likely,Yes,20.698970004336015,252652.85498098005,NVIDIA GB200 NVL2,100000,United States of America,CoreWeave,,"Rough estimate of chip count from stated power capacity of datacenter. 
Core Scientific is also involved. 
Said to be ""one of the largest GPU supercomputers in North America""
Rumored to be using GB200s and rented to Microsoft for OpenAI",Private,260,,"8161 Jim Christal Rd, Denton, TX 76207","OpenAI,Microsoft",GB200,The new agreement brings an additional $1.2 billion in contracted revenue across Core Scientific’s Denton TX location,Planned 2027,,11.982966660701218,,,,,,,100000,NVIDIA,,,,,20.397940008672037,500000000000000000000,500000000000000000000,250000000000000000000,125000000000000000000,240.24000000000004,260,961538461538.4615,,,,,,,,,https://web.archive.org/web/20250228154647/https://investors.corescientific.com/news-events/press-releases/detail/110/core-scientific-and-coreweave-announce-1-2-billion-expansion-at-denton-tx-site,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,33.214604,-97.206756
Middle East 100k Cluster,Planned,Unlikely,Yes,20.698970004336015,252652.85498098005,NVIDIA GB200 NVL2,100000,,,,We think this is likely the same cluster being administered/operated by Microsoft for G42,Private,240.24000000000004,,,,GB200,There's a 100k GB200 cluster going up in the Middle East,Planned,Alluded to by Semianalysis,12.017294689481112,,,True,,,,100000,NVIDIA,,,,,20.397940008672037,500000000000000000000,500000000000000000000,250000000000000000000,125000000000000000000,240.24000000000004,,1040626040626.0405,,,,,,,,,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,,,,
OpenAI Stargate Abilene Oracle OCI Supercluster Phase 1,Planned,Likely,Yes,20.698970004336015,252652.85498098005,NVIDIA GB200 NVL2,100000,United States of America,Oracle,,"States that it will have over 100k GPUs, implying it will have over 50k GB200s

""That cluster is expected to be leased to Oracle, which in turn will lease it to Microsoft, who will rent it to OpenAI""

Crusoe is involved in this",Private,240.24000000000004,,"5502 Spinks Rd, Abilene, TX 79601","Cloud,Microsoft,OpenAI",GB200,"OCI Supercluster with over 100,000+ GPUs in NVIDIA GB200 Grace Blackwell Superchips",Planned Q2 2025,The first phase is under construction already and is very likely to be completed,12.017294689481112,,,,,,,100000,NVIDIA,,,,,20.397940008672037,500000000000000000000,500000000000000000000,250000000000000000000,125000000000000000000,240.24000000000004,,1040626040626.0405,,,,,,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://archive.ph/UeNZs,,,,32.49429,-99.79316
Stargate UAE Phase 1,Planned,Likely,Unclear,20.698970004336015,252652.85498098005,NVIDIA GB200 NVL2,100000,United Arab Emirates,Stargate (OpenAI),,"Chip amount estimated based on power capacity. Assuming for now that they're using GB200s, but this hasn't been confirmed
Part of the broader planned 5GW UAE/USA cluster",Public/Private,200,,"Abu Dhabi, United Arab Emirates",OpenAI,Uncertain,"""A 1GW Stargate UAE cluster in Abu Dhabi with 200MW expected to go live in 2026""
""Breaking ground on 1GW AI Datacenter, part of a planned 5GW UAE-US artificial intelligence campus; largest such deployment outside of the US""
""The campus, which will span 10 square miles within the Emirate, will be built by G42 and operated in partnership with several US companies.""
""Stargate UAE is a next-generation AI infrastructure cluster that will run in the newly established 5-gigawatt UAE–U.S. AI Campus in Abu Dhabi. Stargate UAE, a 1-gigawatt compute cluster""",Planned 2026,,12.096910013008056,,,,,,,100000,NVIDIA,,,,,20.397940008672037,500000000000000000000,500000000000000000000,250000000000000000000,125000000000000000000,240.24000000000004,200,1250000000000,,,,,,,,,https://web.archive.org/web/20250515200336/https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu,https://archive.ph/hQWdM,https://archive.ph/PfVwI,,,24.453524,54.377438
xAI Colossus Memphis Phase 2,Existing,Likely,Yes,20.597475789870376,200000.000002945,NVIDIA H100 SXM5 80GB,150000,United States of America,xAI,2025-02-18,"GPU types is from a tweet. Unclear exactly when it was first operational. It was confirmed to be operational Feb 17, 2025, but some evidence suggests that it was operational by December 4th, 2024",Private,280.28,7120201208.681584,"3231 Riverport Rd, Memphis, TN 38109",xAI,"H100,H200","This weekend, the 
@xAI
 team brought our Colossus 100k H100 training cluster online.... Moreover, it will double in size to 200k (50k H200s) in a few months.",2025-02-18,,11.848820766415054,xAI Colossus Memphis Phase 1,,,,NVIDIA H200 SXM,50000,200000,NVIDIA,NVIDIA,True,,1,20.29641287523659,395800000000000000000,395800000000000000000,197885000000000000000,98930000000000000000,280.28,,706026116740.4025,7120201208.681584,,,,True,,xAI Colossus Memphis Phase 2,1,https://archive.ph/z39uN,https://web.archive.org/web/20241217184541/https://memphischamber.com/economic-development/xai/,https://web.archive.org/web/20241217184541/https://memphischamber.com/economic-development/xai/,https://www.youtube.com/watch?v=AUAJ82H12qs&t=260s,,35.069154,-90.118494
Project Rainier,Planned,Likely,Yes,20.42618582524451,134815.56341785146,Amazon Trainium2,400000,United States of America,Amazon,,,Private,350,,"55253 Elderberry Rd, New Carlisle, IN 46552",Anthropic,Trainium2,a cluster with 400k Trainium2 chips for Anthropic,Planned 2025,,11.882117780894234,,,,,,,400000,Amazon AWS,,,,,20.42618582524451,266800000000000000000,,266800000000000000000,,400.40000000000003,350,762285714285.7142,,,,,,,,,https://web.archive.org/web/20241204130049/https://semianalysis.com/2024/12/03/amazons-ai-self-sufficiency-trainium2-architecture-networking/,,,,,41.693876,-86.469594
Applied Digital CoreWeave Ellendale Phase 1,Planned,Likely,Yes,20.35218251811136,113693.78474144108,NVIDIA GB200 NVL2,45000,United States of America,Applied Digital,,"It's unclear whether all the GPUs will be intereconnected. The CEO says that it's designed in a way that they can be all interconnected, though it's unclear if they will be. It is also somewhat unclear what type of GPUs they will be using, though I am assuming Blackwell GPUs here
Will be rented to CoreWeave
Number of GPUs estimated from power capacity",Private,100,3290034407.1518517,"9685 87th Ave SE, Ellendale, ND 58436","Cloud,CoreWeave",Uncertain,"Applied Digital expects the first 100 MW data center for CoreWeave to be ready for service in the fourth quarter of calendar 2025. The second building, which is expected to house a 150 MW data center, is currently under construction and is expected to be ready for service in the middle of 2026. Additionally, CoreWeave holds an option for the third 150 MW building, which is currently in the planning stages, anticipated to be ready for service in 2027.",Planned Q4 2025,,12.05115252244738,,,,,,,45000,NVIDIA,,,,,20.051152522447378,225000000000000000000,225000000000000000000,112500000000000000000,56250000000000000000,108.10800000000002,100,1125000000000,3290034407.1518517,,,,,,,,https://web.archive.org/web/20240622003111/https://marketscale.com/industries/software-and-technology/applied-digital-and-nvidia-are-solving-ai-applications-efficiency-crisis-with-next-gen-data-centers/,https://web.archive.org/web/20250603030527/https://ir.applieddigital.com/news-events/press-releases/detail/123/applied-digital-announces-250mw-ai-data-center-lease-with,,,,46.01282,-98.572441
Nscale Loughton,Planned,Likely,Yes,20.35218251811136,113693.78474144108,NVIDIA GB200 NVL2,45000,United Kingdom of Great Britain and Northern Ireland,,,Slightly unclear if they refer to GB200 GPUs or entire chips (2 GPUs),Private,90,,"Loughton, England",Cloud,GB200,"The site is equipped to support 50MW of AI and HPC capacity, with the ability to scale the power allocation up to 90MW, all utilising advanced liquid cooling to support generative AI GPU deployments. The site is scheduled to be live in Q4 2026. This UK facility can house up to 45,000 of the latest Nvidia GB200 GPUs.",Planned Q4 2026,,12.096910013008056,,,,,,,45000,NVIDIA,,,,,20.051152522447378,225000000000000000000,225000000000000000000,112500000000000000000,56250000000000000000,108.10800000000002,90,1250000000000,,,,,,,,,https://web.archive.org/web/20250116140650/https://www.globenewswire.com/news-release/2025/01/13/3008191/0/en/AI-hyperscaler-Nscale-to-invest-2-5-2-billion-in-the-UK-data-centre-industry-over-the-next-three-years.html,,,,,51.64958,0.057071
Anonymized Chinese System,Planned,Unlikely,Unclear,20.30102999566398,100000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,20.30102999566398,199999999999999970000,,199999999999999970000,,,,,,,,,,,,,,,,,,35.486703,101.901875
CoreWeave Muskogee,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,United States of America,CoreWeave,,"Rough estimate of chip count from stated power capacity of datacenter. 
Core Scientific is also involved
NVIDIA GPUs, so presumably GB200s",Private,100,,"Muskogee, Oklahoma 35.693709, -95.386957","CoreWeave,Undisclosed Client","GB200,Uncertain","ground had been broken on a 100MW facility in the Port of Muskogee in West Oklahoma.
Set to go live in 2026, the facility will host CoreWeave’s Nvidia GPUs for an undisclosed client",Planned 2026,,11.999999999999998,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,100,1000000000000,,,,,,,,,https://web.archive.org/web/20250115175242/https://www.datacenterdynamics.com/en/news/coreweave-and-core-scientific-break-ground-on-100mw-data-center-in-muskogee-oklahoma/,,,,,35.748652,-95.371517
EU AI Gigafactory #1,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.09600000000002,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.017294689481112,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,,1040626040626.0404,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,,,
EU AI Gigafactory #2,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.09600000000002,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.017294689481112,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,,1040626040626.0404,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,,,
EU AI Gigafactory #3,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.09600000000002,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.017294689481112,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,,1040626040626.0404,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,,,
EU AI Gigafactory #4,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.09600000000002,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.017294689481112,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,,1040626040626.0404,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,,,
EU AI Gigafactory #5,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.09600000000002,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.017294689481112,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,,1040626040626.0404,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,,,
Sesterce Valence,Planned,Likely,Yes,20.30102999566398,101061.14199239286,NVIDIA GB200 NVL2,40000,France,Sesterce,,"Unclear what type of GPUs they'll use, but I'm assuming it'll be Blackwell GPUs",Private,96.09600000000002,,"Valence, France",Cloud,GB200,"The data center is set to be located in the Rovaltain business park in Valence Romans Agglo, and will house 40,000 GPUs for the training and deployment of AI.",Planned 2026,,12.017294689481112,,,,,,,40000,NVIDIA,,,,,20,200000000000000000000,200000000000000000000,100000000000000000000,50000000000000000000,96.09600000000002,,1040626040626.0404,,,,,,,,,https://archive.ph/33xgc,,,,,44.932907,4.891448
Meta 100k,Existing,Likely,Yes,20.29644579420639,100000.00000147166,NVIDIA H100 SXM5 80GB,100000,United States of America,Meta AI,2024-10-30,Reported as operational in October 2024 earnings call,Private,142.688,3810062229.221386,United States of America,Meta,H100,"Meta Platforms is putting the final touches on one such cluster, which will be a little bigger than 100,000 H100s",2024-10-30,Reported by The Information,11.840984455358647,,,,,,,100000,NVIDIA,,True,,1,19.99537190602816,197900000000000000000,197900000000000000000,98940000000000000000,49470000000000000000,142.688,,693400986768.3337,3810062229.221386,,,,True,,xAI Colossus Memphis Phase 1,1,https://archive.ph/85KB1,https://web.archive.org/web/20240901171617/https://www.semianalysis.com/p/100000-h100-clusters-power-network,https://web.archive.org/web/2/https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen,https://web.archive.org/web/20241228090751/https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen,,39.381266,-97.922211
OpenAI/Microsoft Goodyear Arizona,Existing,Likely,Yes,20.29644579420639,100000.00000147166,NVIDIA H100 SXM5 80GB,100000,United States of America,"Microsoft,OpenAI",2024-10-02,"It seems likely that this became operational in phases. It seems like 75k were likely available sometime mid 2024, and ~160k will be available in the future. 
There is very little official public data about this cluster, it's almost all infrences/rumors
Microsoft appears to own it and rent it to OpenAI
The Semianalysis lecture estimates that there are 32k H100s (48MW) per building. Sattelite data as of Dec 2024 seems to show that 5 building are now complete (though this doesn't imply that the clusters inside have been built or are operational). This would imply that the cluster is growing to ~160k GPUs. It's also unclear if the newer buildings would still have Hoppers or would have Blackwell chips
I personally suspect that this cluster was one of the primary clusters for training GPT4.5",Private,142.688,3802450830.801299,"33°24'28.7""N 112°21'54.3""W
Goodyear, Arizona, USA",OpenAI,H100,"""You already have 100K clusters. OpenAI in Arizona""
""Our data shows one of OpenAI’s next training supercomputers in Arizona was going to have more than 75,000+ GPUs in a singular site by the middle of next year""",2024-10-02,"This estimate is from the SemiAnalysis number of GPUs/building, and satellite imagery showing that they've built 5 buildings",11.840984455358647,,,,,,,100000,NVIDIA,,True,,1,19.99537190602816,197900000000000000000,197900000000000000000,98940000000000000000,49470000000000000000,142.688,,693400986768.3337,3802450830.801299,,,,,,xAI Colossus Memphis Phase 1,1,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,https://web.archive.org/web/20240609051243/https://www.semianalysis.com/p/microsoft-swallows-openais-core-team,https://www.youtube.com/watch?v=hobvps-H38o,,,33.190283,-111.966202
Tesla Cortex Phase 3,Planned,Likely,Yes,20.29644579420639,100000.00000147166,NVIDIA H100 SXM5 80GB,100000,United States of America,Tesla,,,Private,140.14,4700049153.074074,"1 Tesla Rd, Austin, TX 78725",Tesla,H100,This will be ~100k H100/H200 with massive storage for video training of FSD & Optimus.,Planned 2026,From a Musk Tweet,11.848809792870606,Tesla Cortex Phase 2,,,,,20000,120000,NVIDIA,,,,,19.99537190602816,197900000000000000000,197900000000000000000,98940000000000000000,49470000000000000000,140.14,,706008277436.8489,4700049153.074074,,,,,,,,https://web.archive.org/web/20240904184237/https://www.tomshardware.com/desktops/servers/elon-musk-shows-off-cortex-ai-supercluster-first-look-at-teslas-50000-nvidia-h100s,https://web.archive.org/web/20240822222555/https://www.fool.com/earnings/call-transcripts/2024/07/24/tesla-tsla-q2-2024-earnings-call-transcript/,https://archive.ph/0jHdq#selection-479.86-479.172,,,30.222695,-97.618788
xAI Colossus Memphis Phase 1,Existing,Confirmed,Yes,20.29644579420639,100000.00000147166,NVIDIA H100 SXM5 80GB,100000,United States of America,xAI,2024-09-02,,Private,150,3802927639.101294,"3231 Riverport Rd, Memphis, TN 38109",xAI,H100,"""With 100k liquid-cooled H100s on a single RDMA fabric""

""This weekend, the @xAI team brought our Colossus 100k H100 training cluster online.... Moreover, it will double in size to 200k (50k H200s) in a few months""",2024-09-02,Reported to be online by Musk,11.81928064697248,,xAI Colossus Memphis Phase 2,,,,,100000,NVIDIA,,True,,1,19.99537190602816,197900000000000000000,197900000000000000000,98940000000000000000,49470000000000000000,142.688,150,659600000000,3802927639.101294,,,,,,xAI Colossus Memphis Phase 1,1,https://archive.ph/Bh9Tq,https://web.archive.org/web/20240727065656/https://www.commercialappeal.com/story/news/government/city/2024/07/09/elon-musk-xai-memphis-electricity-water-mlgw/74331544007/,https://archive.ph/z39uN,,,35.069154,-90.118494
together.ai 36k GB200s,Planned,Likely,Yes,20.255272505103303,90955.0277931529,NVIDIA GB200 NVL2,36000,United States of America,Together,,,Private,86.4864,,,Cloud,GB200,"This infrastructure backbone that will power the next generation of AI innovations enables the following customer benefits:
36,000 NVIDIA Blackwell GPUs in GB200 NVL72 Cluster starting Q1 2025",Planned Q1 2025,,12.017294689481112,,,,,,,36000,NVIDIA,,,,,19.95424250943932,180000000000000000000,180000000000000000000,90000000000000000000,45000000000000000000,86.4864,,1040626040626.0406,,,,,,,,,https://web.archive.org/web/20241120213534/https://www.together.ai/blog/nvidia-gb200-together-gpu-cluster-36k,,,,,39.381266,-97.922211
HUMAIN Saudi Arabia/NVIDIA Phase 1,Planned,Confirmed,Yes,20.130333768495007,68216.27084486524,NVIDIA GB300,18000,Saudi Arabia,Humain,,,Public/Private,50.45040000000001,,,,GB300,"""HUMAIN is making a major investment to build AI factories in the Kingdom of Saudi Arabia with a projected capacity of up to 500 megawatts powered by several hundred thousand of NVIDIA’s most advanced GPUs over the next five years. The first phase of deployment will be an 18,000 NVIDIA GB300 Grace Blackwell AI supercomputer with NVIDIA InfiniBand networking.""",Planned 2026,,12.126439158906178,,,,,,,18000,NVIDIA,,,,,19.82930377283102,135000000000000000000,135000000000000000000,67500000000000000000,,50.45040000000001,,1337947766519.1948,,,,,,,,,https://archive.ph/vvAxQ,https://archive.ph/CFcnD,,,,23.384784,44.652426
Oracle OCI Supercluster H200s,Existing,Likely,Yes,20.11292572483009,65536.0000009646,NVIDIA H200 SXM,65536,United States of America,Oracle,2024-11-21,"Unclear exactly when this first became operational, if it was ever built. The announcement November 2024 states that the largest supercomputer in the cloud is now generally available, implying that this cluster has been built and is operational. However, it's possible that they're just advertising their capacity to scale H200 clusters to be the largest supercomputer in the cloud. 
It seems very likely that this cluster has been built, but it's not fully certain. I estimate the first operational date to be Q1 2025 to be slightly conservative on the timing",Private,93.51200768000001,2345609100.860053,,Cloud,H200,"We’re excited to announce the general availability of Oracle Cloud Infrastructure (OCI) Supercluster with NVIDIA H200 Tensor Core GPUs. The largest AI supercomputer available in the cloud*, our latest Supercluster scales up to an industry-leading 65,536 GPUs.",2024-11-21,,11.8410283478729,,,,,,,65536,NVIDIA,,True,,4,19.811895729166114,129695744000000000000,129695744000000000000,64847872000000000000,32407552000000000000,93.51200768000001,,693471069746.58,2345609100.860053,,,,,,xAI Colossus Memphis Phase 1,0.6553600000000014,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://web.archive.org/web/20241214043143/https://www.datacenterdynamics.com/en/news/oracles-65000-gpu-supercluster-now-generally-available/,,,,39.381266,-97.922211
Nebius Finland Phase 2,Planned,Likely,Yes,20.07459704459004,60000.00000088352,NVIDIA H100 SXM5 80GB,60000,Finland,Nebius AI,,"Using H100s and H200s, and possibly Blackwell chips",Private,84.084,2820029491.8444443,"Moreenikatu 6, 04600 Mäntsälä, Finland",Cloud,"H100,H200","""The expansion will see Nebius place upwards of 60,000 GPUs at the facility, growing its capacity from 25MW to 75MW.""
""The expansion in Finland will include deployment of NVIDIA H200 Tensor Core GPUs, which will be available to customers from November, in addition to already installed NVIDIA H100 Tensor Core GPUs that form the backbone of Nebius’s fleet of NVIDIA GPUs""",Planned H2 2025,,11.848809792870606,Nebius 8k Finland Phase 1,,,,,,60000,NVIDIA,,,,,19.773523156411805,118740000000000000000,118740000000000000000,59364000000000000000,29682000000000000000,84.084,,706008277436.8489,2820029491.8444443,,,,,,,,https://web.archive.org/web/20241009115430/https://www.datacenterdynamics.com/en/news/nebius-to-triple-capacity-of-mantsala-data-center-in-finland/,https://group.nebius.com/newsroom/nebius-to-triple-capacity-at-finland-data-center-to-75-mw,,,,60.629522,25.26336
G42 Microsoft 100Mw UAE Cluster,Planned,Likely,Yes,20.03680848370064,55000.000000809894,NVIDIA H100 SXM5 80GB,55000,United Arab Emirates,"G42,Microsoft",,"Estimating ""50,000–60,000"" as 55k
Seems likely to be the Ajman cluster, but uncertain here
Assuming they're using H100s here",Public/Private,100,2585027034.1907406,"Ajman, United Arab Emirates","Microsoft,Azure",H100,"Private conversations with Microsoft and G42 executives revealed that G42 and Khazna are building three new data centers in the UAE for Microsoft. The centers will be 30, 30, and 100 megawatts each, the equivalent of nearly 100,000 H100 chips (approximately 15,000, 15,000, and 50,000–60,000 respectively).",Planned Q3 2025,,11.735734595522405,,,,,,,55000,NVIDIA,,,,,19.735734595522405,108845000000000000000,108845000000000000000,54417000000000000000,27208500000000000000,77.077,100,544170000000,2585027034.1907406,,,,,,,,https://web.archive.org/web/20250221170709/https://www.csis.org/analysis/united-arab-emirates-ai-ambitions,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,,25.409328,55.44359
Project Ceiba Phase 2,Planned,Confirmed,Yes,20.015694988526516,52390.096008856155,NVIDIA GB200 NVL2,20736,United States of America,"Amazon,NVIDIA",,,Private,49.81616640000001,,,NVIDIA,GB200,"Alongside the 20,736 B200 GPUs, Ceiba will boast 10,368 Grace CPUs.
Ceiba will be built using the new liquid-cooled GB200 NVL72 platform with fifth generation NVLink",Planned 2025,Plans were officially reported by NVIDIA,12.017294689481112,Project Ceiba Phase 1,,,,,,20736,NVIDIA,,,,,19.714664992862534,103680000000000000000,103680000000000000000,51840000000000000000,25920000000000000000,49.81616640000001,,1040626040626.0405,,,,,,,,,https://web.archive.org/web/20240522175821/https://www.datacenterdynamics.com/en/news/aws-upgrades-project-ceiba-to-feature-20736-nvidia-blackwell-gpus-boosting-power-6x-to-414-ai-exaflops/,https://web.archive.org/web/20240806110553/https://nvidianews.nvidia.com/news/aws-nvidia-generative-ai-innovation,,,,39.381266,-97.922211
Tesla Cortex Phase 1,Existing,Confirmed,Yes,19.995415798542414,50000.00000073623,NVIDIA H100 SXM5 80GB,50000,United States of America,Tesla,2024-11-15,,Private,71.344,1905031114.610693,"1 Tesla Rd, Austin, TX 78725",Tesla,H100,"Tesla completed the deployment of its 50,000 Nvidia H100 GPU training cluster during Q4 2024.",Q4 2024,,11.840984455358647,,,,,,,50000,NVIDIA,,True,,4,19.694341910364177,98950000000000000000,98950000000000000000,49470000000000000000,24735000000000000000,71.344,,693400986768.3337,1905031114.610693,,,,,,xAI Colossus Memphis Phase 1,0.500000000000004,https://web.archive.org/web/20250217061801/https://www.datacenterdynamics.com/en/news/teslas-50000-gpu-cortex-supercomputer-went-live-in-q4-2024/,https://web.archive.org/web/20240822222555/https://www.fool.com/earnings/call-transcripts/2024/07/24/tesla-tsla-q2-2024-earnings-call-transcript/,https://web.archive.org/web/20240904184237/https://www.tomshardware.com/desktops/servers/elon-musk-shows-off-cortex-ai-supercluster-first-look-at-teslas-50000-nvidia-h100s,,,30.222695,-97.618788
Tesla Cortex Phase 2,Planned,Likely,Yes,19.995415798542414,50000.00000073623,NVIDIA H100 SXM5 80GB,50000,United States of America,Tesla,,,Private,70.07,2350024576.537037,"1 Tesla Rd, Austin, TX 78725",Tesla,"H100, 50k H100, additional 20k of Tesla's own hardware is also expected","So it will be an incremental for 50,000 H100s plus 20,000 of our hardware for AI5 at Tesla AI computer",Planned 2025,"Musk gave details about this in an earnings call
Some sources mention that the 50k H100s will come online before Tesla's own chips will",11.848809792870606,Tesla Cortex Phase 1,,,,,20000,70000,NVIDIA,,,,,19.694341910364177,98950000000000000000,98950000000000000000,49470000000000000000,24735000000000000000,70.07,,706008277436.8489,2350024576.537037,,,,,,,,https://web.archive.org/web/20240904184237/https://www.tomshardware.com/desktops/servers/elon-musk-shows-off-cortex-ai-supercluster-first-look-at-teslas-50000-nvidia-h100s,https://web.archive.org/web/20240822222555/https://www.fool.com/earnings/call-transcripts/2024/07/24/tesla-tsla-q2-2024-earnings-call-transcript/,https://web.archive.org/web/20250217061801/https://www.datacenterdynamics.com/en/news/teslas-50000-gpu-cortex-supercomputer-went-live-in-q4-2024/,,,30.222695,-97.618788
Lawrence Livermore NL El Capitan Phase 2,Existing,Confirmed,Yes,19.941311098168587,44143.351592362786,AMD Instinct MI300A,44544,United States of America,US Department of Energy,2024-11-18,,Public,35,607324736.6965789,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","US Government,Lawrence Livermore NL,Scientific Research",AMD MI300A,"the system has 11,136 nodes packed with 44,544 of AMD's MI300A APUs",2024-11-18,,12.096213058154332,Lawrence Livermore NL El Capitan Phase 1,,,,,,44544,AMD,,True,,5,19.640281102504606,87359692800000000000,87359692800000000000,43679846400000000000,21839923200000000000,69.00685209600002,35,1247995611428.5715,1826641892.232672,600000000,607324736.6965789,$600M was quoted as the max cost ,True,,xAI Colossus Memphis Phase 1,0.4414335159171314,https://web.archive.org/web/20241122140255/https://www.tomshardware.com/pc-components/cpus/amd-powered-el-capitan-is-now-the-worlds-fastest-supercomputer-with-1-7-exaflops-of-performance-fastest-intel-machine-falls-to-third-place-on-top500-list,,,,,37.682018,-121.768374
CoreWeave H200s,Existing,Likely,Yes,19.919695084604296,42000.00000061847,NVIDIA H200 SXM,42000,United States of America,CoreWeave,2024-08-28,,Private,59.92896000000001,1499797837.3467495,,CoreWeave,H200,"These instances are deployed in clusters with up to 42,000 GPUs",2024-08-28,,11.8410283478729,,,,,,,42000,NVIDIA,,True,,1,19.618665088940315,83118000000000000000,83118000000000000000,41559000000000000000,20769000000000000000,59.92896000000001,,693471069746.5798,1499797837.3467495,,,,,,CoreWeave H200s,1,https://web.archive.org/web/20250107015942/https://www.coreweave.com/blog/coreweave-is-the-first-cloud-provider-to-deploy-nvidia-h200-tensor-core-gpus,,,,,39.381266,-97.922211
YTL AI Johor,Planned,Likely,Yes,19.88727963453002,38979.28246646573,NVIDIA GB200 NVL2,15428,Malaysia,YTL Power,,"It states the cluster will be over 300E18 FLOP/s, and that a single GB200 NVL72 has 1.4E18 FLOP/s, implying they are referring to FP4. (300/1.4)*36=7,714 GB200s=15,428 GPUs (B200s)",Private,37.064227200000005,,"81000 Kulai, Johor, Malaysia
1.6230215444990799, 103.52989382701362",Cloud,GB200,"YTL is among the first companies to adopt NVIDIA GB200 NVL72, which is a multi-node, liquid-cooled, rack-scale system with fifth-generation NVLink. The supercomputer will be interconnected by NVIDIA Quantum InfiniBand networking platform. The platform acts as a single GPU with 1.4 exaflops of AI performance and 30TB of fast memory and is designed for the most compute-intensive workloads. The YTL AI Supercomputer will surpass more than 300 exaflops of AI compute, making it one of the fastest supercomputers in the world.",Planned Q3 2025,Plans to use Blackwell chips,12.017294689481112,,,,,,,15428,NVIDIA,,,,,19.58624963886604,77140000000000000000,77140000000000000000,38570000000000000000,19285000000000000000,37.064227200000005,,1040626040626.0405,,,,,,,,,https://web.archive.org/web/20240717202025/https://www.ytlaicloud.com/press-releases/ytl-creates-one-of-the-worlds-most-advanced-supercomputers-powered-by-nvidia-grace-blackwell-based-dgx-cloud/,,,,,1.656563,103.606269
Nebius Kansas City Phase 2,Planned,Likely,Yes,19.85504644424969,36191.00555889553,NVIDIA B200,8000,United States of America,Nebius AI,,"Additional capacity was planned to go online Q2 2025
Is using Blackwell GPUs and H200s, unclear what proportion of each. For this, I am assuming have of the MW go to H200s vs B200s",Private,40,1250521491.3171296,"Kansas City, USA",Cloud,"H200,Blackwell","will house thousands of state-of-the-art NVIDIA GPUs, primarily NVIDIA Hopper GPUs in the initial phase, with the energy-efficient NVIDIA Blackwell platform expected to arrive in 2025. The colocation can be expanded from an initial 5 MW up to 40 MW, or about 35 thousand GPUs, at full potential capacity",Planned Q2 2025,,11.95195645725775,Nebius Kansas City Phase 1,,,,NVIDIA H200 SXM,18000,26000,NVIDIA,NVIDIA,,,,19.554016448585713,71622000000000000000,71622000000000000000,35811000000000000000,17901000000000000000,41.2412,40,895275000000,1250521491.3171296,,,,,,,,https://web.archive.org/web/20250109005023/https://group.nebius.com/newsroom/nebius-expands-in-us-with-first-gpu-cluster-in-kansas-city-offices-in-san-francisco-dallas-and-new-york,,,,,39.098485,-94.578631
Tesla Earnings Call Claim,Existing,Unlikely,Unclear,19.84051383855667,35000.00000051537,NVIDIA H100 SXM5 80GB,35000,United States of America,Tesla,2024-03-31,"Unclear if these are all in the same cluster. I personally doublt that it is. Musk diverted 12k H100s intended for Tesla to XAi, and it's unclear how that relates to this cluster. Also, NVIDIA stated that some of Tesla's statements ""did not align with what they had sold Tesla""",Private,49.94080000000001,1249366457.6570702,,Tesla,H100,"We've installed and commissioned, meaning they're actually working 35,000 H100 computers or GPUs",2024-03-31,Likely not a single superocmputer,11.840984455358647,,,,,,,35000,NVIDIA,,,,,19.539439950378437,69265000000000000000,69265000000000000000,34629000000000000000,17314500000000000000,49.94080000000001,,693400986768.3336,1249366457.6570702,,,,,,,,https://web.archive.org/web/20241203032810/https://www.fool.com/earnings/call-transcripts/2024/04/23/tesla-tsla-q1-2024-earnings-call-transcript/,https://web.archive.org/web/20241111165856/https://www.datacenterdynamics.com/en/news/elon-musk-told-nvidia-to-ship-thousands-of-gpus-reserved-for-tesla-to-x-and-xai/,,,,39.381266,-97.922211
Lambda Labs H100/H200,Existing,Likely,No,19.801595772526298,32000.00000047095,NVIDIA H100 SXM5 80GB,32000,United States of America,Lambda Labs,2023-12-05,"Likely in the US since majority of operations happen there https://lambdalabs.com/service/gpu-cloud/faqs
Unclear if the 32k refers to one single supercomputer, the aggregation of many supercomputers, or the potential that they could hypotheticlaly scale up to",Private,46.475519999999996,1056677221.8329314,,Cloud,"H100,H200","Reserved cloud clusters from 64 to 32,000 H100 or H200 SXM GPUs",2023-12-05,"Very little information on this. Unclear if this is one cluster or several, and how many are in the largest cluster",11.833297626692357,,,,,NVIDIA H200 SXM,,32000,NVIDIA,NVIDIA,,,,19.500521884348068,63328000000000000000,63328000000000000000,31660800000000000000,15830400000000000000,46.475519999999996,,681236057175.9069,1056677221.8329314,,,,,,,,https://web.archive.org/web/20231205014622/https://lambdalabs.com/nvidia-h100-nvidia-h200-gpus,https://archive.ph/Zzfyw,https://web.archive.org/web/20240413041051/https://www.futuriom.com/articles/news/lambda-scores-320-million-for-ai-infrastructure-cloud/2024/02,,,39.381266,-97.922211
NVIDIA CoreWeave Eos-DFW Rumored Phase 2,Planned,Likely,Yes,19.801595772526298,32000.00000047095,NVIDIA H100 SXM5 80GB,32000,United States of America,"NVIDIA,CoreWeave",,,Private,44.8448,1504015728.9837036,"1000 Coit Rd, Plano, TX 75075","NVIDIA,CoreWeave",H100,"NVIDIA is doing the same thing in Plano, Texas for a 32,000 GPU cluster that they're building",Planned 2025,Alluded to in Dylan Patel interview,11.848809792870606,NVIDIA CoreWeave Eos-DFW Phase 1,,,,,,32000,NVIDIA,,,,,19.500521884348068,63328000000000000000,63328000000000000000,31660800000000000000,15830400000000000000,44.8448,,706008277436.8489,1504015728.9837036,,,,,,,,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,,,,33.014978,-96.764937
Foxconn Big Innovation Cloud AI factory,Planned,Likely,Yes,19.79588001734407,31581.60687262249,NVIDIA GB300,5000,Taiwan,Foxconn,,Unclear what proportion of the Blackwell GPUs will be GB300s vs GB200s. I assume it will be 50% each,Private,26.026000000000003,,,"Taiwan National Science and Technology Council,TSMC,Foxconn",Blackwell,"the supercomputer will consist of 10,000 Nvidia Blackwell Ultra GPUs... will include the GB300 NVL72",Planned 2026,,12.079442596229956,,,,,NVIDIA GB200 NVL2,5000,10000,NVIDIA,NVIDIA,,,,19.494850021680094,62500000000000000000,62500000000000000000,31250000000000000000,6250000000000000000,26.026000000000003,,1200722354568.5083,,,,,,,,,https://web.archive.org/web/20250519123959/https://www.datacenterdynamics.com/en/news/nvidia-and-foxconn-plan-taiwanese-ai-supercomputer/,,,,,23.777978,120.930229
Sesterce Pegasus,Planned,Likely,Yes,19.719993826367602,26518.4436588038,NVIDIA GB200 NVL2,10496,,Sesterce,,,Private,25.215590400000004,,,Cloud,GB200,"""10496 GPUs""",Planned,,12.017294689481112,,,,,,,10496,NVIDIA,,,,,19.41896383070362,52480000000000000000,52480000000000000000,26240000000000000000,13120000000000000000,25.215590400000004,,1040626040626.0405,,,,,,,,,https://web.archive.org/web/20240823055517/https://www.sesterce.com/reserved-cloud,,,,,,
Google A3 VMs,Planned,Likely,Yes,19.711419142177213,26000.000000382857,NVIDIA H100 SXM5 80GB,26000,United States of America,Google,,"The A3 supercomputer seems to refer to an amalgamation of different clusters available as a virtual machine via the cloud and is operational. 
26k is the size of the largest individual cluster they say they can build as part of this. It is unclear if a cluster of this size has been constructed. If it has, it could have been in 2024",Private,36.4364,1222012779.7992592,,Cloud,H100,"""“For our largest customers, we can build A3 supercomputers up to 26,000 GPUs in a single cluster and are working to build multiple clusters in our largest regions,” a Google spokeswoman said in an email""",Planned 2025,This refers to the largest clusters Google says it can build for its customers,11.848809792870606,,,,,,,26000,NVIDIA,,,,,19.410345253998976,51454000000000000000,51454000000000000000,25724400000000000000,12862200000000000000,36.4364,,706008277436.8489,1222012779.7992592,,,,,,,,https://web.archive.org/web/20240716093004/https://cloud.google.com/blog/products/compute/introducing-a3-supercomputers-with-nvidia-h100-gpus,https://web.archive.org/web/20240829141902/https://www.hpcwire.com/2023/05/10/googles-new-ai-focused-a3-supercomputer-has-26000-gpus/,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,No,19.698970004336015,30000,,,China,,2024-12-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.698970004336015,50000000000000000000,,50000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
ParTec ELBJUWEL,Planned,Confirmed,Yes,19.698970004336015,25265.285498098005,,,Germany,Helmholtz-Zentrum Dresden-Rossendorf (HZDR),,,Public,,,"Bautzner Landstraße 400, 01328 Dresden, Germany","Cloud,Academia,Industry",,with capabilities extending to about 50 exaflops for 8-bit floating-point operations,Planned,,,,,,,,,,Unknown,,,,,19.401400540781545,50000000000000000000,50000000000000000000,25200000000000000000,,,,,,,,,,,,,https://web.archive.org/web/20241117210443/https://thequantuminsider.com/2024/10/22/partec-ag-and-hzdr-to-build-ai-supercomputer-supporting-research-in-ai-quantum-computing-and-hpc/,,,,,51.062848,13.950591
Meta GenAI 2024a,Existing,Confirmed,Yes,19.686956992557814,24576.000000361928,NVIDIA H100 SXM5 80GB,24576,United States of America,Meta AI,2024-03-12,At least one of these was used to train Llama 3,Private,35.067002880000004,880071195.8491452,,Meta,H100,"Today, we’re sharing details on two versions of our 24,576-GPU data center scale cluster at Meta",2024-03-12,From official Meta announcement. At least one of these was used to train Llama 3.1,11.840984455358647,,,,,,,24576,NVIDIA,,True,,1,19.385883104379577,48635904000000000000,48635904000000000000,24315494400000000000,12157747200000000000,35.067002880000004,,693400986768.3336,880071195.8491452,,,,True,,Meta GenAI 2024a,1,http://web.archive.org/web/20240828225612/https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/,https://web.archive.org/web/20240827223115/https://www.semianalysis.com/p/gpt-4-architecture-infrastructure,,,,39.381266,-97.922211
Meta GenAI 2024b,Existing,Confirmed,Yes,19.686956992557814,24576.000000361928,NVIDIA H100 SXM5 80GB,24576,United States of America,Meta AI,2024-03-12,At least one of these was used to train Llama 3,Private,35.067002880000004,880071195.8491452,,Meta,H100,"Today, we’re sharing details on two versions of our 24,576-GPU data center scale cluster at Meta",2024-03-12,From official Meta announcement. At least one of these was used to train Llama 3.1,11.840984455358647,,,,,,,24576,NVIDIA,,True,,1,19.385883104379577,48635904000000000000,48635904000000000000,24315494400000000000,12157747200000000000,35.067002880000004,,693400986768.3336,880071195.8491452,,,,,,Meta GenAI 2024a,1,http://web.archive.org/web/20240828225612/https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/,https://web.archive.org/web/20240827223115/https://www.semianalysis.com/p/gpt-4-architecture-infrastructure,,,,39.381266,-97.922211
"""Jupiter, Jülich""",Planned,Confirmed,Yes,19.672328785751624,23762.00000034991,NVIDIA GH200,23762,Germany,"EuroHPC JU,Julich Supercomputing Center",,,Public,18,290000685.87105626,"Jülich Supercomputing Centre (JSC)
52428 Jülich, Germany",,GH200,"""JUPITER marks the debut of a quad NVIDIA GH200 Grace Hopper Superchip node configuration, based on Eviden’s BullSequana XH3000 liquid-cooled architecture, with a booster module comprising close to 24,000 NVIDIA GH200 Superchips interconnected with the NVIDIA Quantum-2 InfiniBand networking platform.""

""Jupiter will be a new supercomputer built out of 23,762 GH200 nodes""",Planned 2025,,12.116026284984336,,,,,,,23762,NVIDIA,,,,,19.37129879008764,47024998000000000000,47024998000000000000,23512499000000000000,11750309000000000000,33.3000668,18,1306249944444.4443,1089491617.5764012,287000000,290000685.87105626,"EuroHPC says in a statement that the cost of building, delivering, installing, and maintaining the Jupiter machine is €273 million ($287.3 million), and presumably the remaining part of that €500 million is to build or retrofit a datacenter for Jupiter and pay for power and cooling for the machine.",,,,,https://web.archive.org/web/20240825120701/https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchip-powers-jupiter-defining-a-new-class-of-supercomputers-to-propel-ai-for-scientific-discovery,https://archive.is/oyvz9,https://www.nextplatform.com/2023/10/05/details-emerge-on-europes-first-exascale-supercomputer/,https://www.anandtech.com/show/21136/nvidia-at-sc23-h200-accelerator-with-hbm3e-and-jupiter-supercomputer-for-2024,,50.921734,6.358342
Inflection AI Cluster,Planned,Likely,Unclear,19.638868475028602,22000.000000323962,NVIDIA H100 SXM5 80GB,22000,United States of America,Inflection AI,,"This was in the process of being built, but it is unclear if it ever got built",Private,31,1034010813.6762961,,Inflection,H100,"Along with its partners CoreWeave and NVIDIA, Inflection AI is building the largest AI cluster in the world comprising 22,000 NVIDIA H100 Tensor Core GPUs",Planned,Unclear if this is still happening after the shakeup at Inflection A. They stated they're building this on their website in June 2023,11.846432893016095,,,,,,,22000,NVIDIA,,,,,19.337794586850368,43538000000000000000,43538000000000000000,21766800000000000000,10883400000000000000,30.830800000000004,31,702154838709.6774,1034010813.6762961,,,,,,,,https://archive.ph/xKdtC,https://web.archive.org/web/20230708074006/https://wccftech.com/inflection-ai-develops-supercomputer-equipped-with-22000-nvidia-h100-ai-gpus/amp/,,,,39.381266,-97.922211
Oracle OCI MI300x,Existing,Likely,Yes,19.631875024361893,21648.570793648592,AMD Instinct MI300X,16384,United States of America,Oracle,2024-11-15,"Unclear exactly when this first became operational, if it was ever built. The announcement September 2024 states that MI300x clusters are now available, and can scale up to 16k GPUs. It's unclear whether this implies that they have already built a 16k MI300x cluster, or if they are just accepting orders to. It seems very likely that this cluster has been built, but I estimate the first operational date as Q4 2024 to be more conservative",Private,25.0478592,446003052.8923827,,Cloud,MI300X,"OCI Supercluster with AMD Instinct MI300X accelerators provides high-throughput, ultra-low latency RDMA cluster network architecture for up to 16,384 MI300X GPUs",Q4 2024,,11.932057806582943,,,,,,,16384,AMD,,True,,8,19.330828419925634,42842521600000000000,42842521600000000000,21420441600000000000,10710220800000000000,25.0478592,,855180533751.9623,446003052.8923827,,,,,,xAI Colossus Memphis Phase 1,0.2164857079333,https://archive.ph/gdAb4,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,No,19.60205999132796,20000,,,China,,2024-03-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.60205999132796,40000000000000000000,,40000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,19.60205999132796,20000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.60205999132796,40000000000000000000,,40000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Andreessen Horowitz Oxygen,Existing,Likely,Unclear,19.597475789870376,20000.000000294498,NVIDIA H100 SXM5 80GB,20000,United States of America,Andreessen Horowitz,2024-10-23,"They have already secured thousands of chips, but plan to expand it to over 20k. 
Unclear if this is one cluster or many, if all the chips are H100s, and if they own these chips or are just renting them",Private,28.537600000000005,762012445.8442771,,Startups via cloud,H100,"Earlier this year, took its focus on the generative technology further as it began accumulating A.I. chips—including the sought-after Nvidia (NVDA) H100 GPUs—with plans to expand its stockpile to more than 20,000, the Information reported this week",2024-10-23,"Unclear if this is one cluster or many, and if they own these chips or are just renting them",11.840984455358647,,,,,,,20000,NVIDIA,,,,,19.296401901692143,39580000000000000000,39580000000000000000,19788000000000000000,9894000000000000000,28.537600000000005,,693400986768.3336,762012445.8442771,,,,,,,,https://web.archive.org/web/20240724120811/https://observer.com/2024/07/andreessen-horowitz-stocking-ai-chips-win-deals-gpu-shortage/,https://web.archive.org/web/20241110000626/https://techcrunch.com/2024/10/23/andreessen-horowitz-helps-founders-meet-compute-needs-with-oxygen-private-gpu-cluster/,,,,39.381266,-97.922211
AWS EC2 P5 UltraClusters,Existing,Likely,Unclear,19.597475789870376,20000.000000294498,NVIDIA H100 SXM5 80GB,20000,United States of America,Amazon,2023-07-26,Likely several such clusters in the same availability zone,Private,29.047199999999997,719603493.3825144,United States of America,Cloud,H100,"To address customer needs for large-scale and low latency, P5 instances are deployed in the second-generation EC2 UltraClusters, which now provide customers with lower latency across up to 20,000+ NVIDIA H100 Tensor Core GPUs.",2023-07-26,Seems likely these are several clusters,11.833297626692357,,,,,,,20000,NVIDIA,,,,,19.296401901692143,39580000000000000000,39580000000000000000,19788000000000000000,9894000000000000000,29.047199999999997,,681236057175.9069,719603493.3825144,,,,,,,,https://web.archive.org/web/20240823004421/https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/,https://web.archive.org/web/20240826022116/https://aws.amazon.com/ec2/ultraclusters/,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Likely,Unclear,19.60205999132796,20000,,10000,China,,2024-11-15,,Private,30,700000000,China,,,,,,11.823908740944317,,,,,,10000,20000,Anonymized,Anonymized,,,,19.301029995663978,40000000000000000000,40000000000000000000,20000000000000000000,8999999999999999000,30,,666666666666.6666,700000000,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,No,19.60205999132796,20000,,,China,,2024-07-15,,,,,China,,,,,,,,,True,,,,,Anonymized,Anonymized,,,,19.60205999132796,40000000000000000000,,40000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
NexGen Cloud Hyperstack AQ Compute Supercomputer,Existing,Likely,Yes,19.510865733502133,16384.00000024128,NVIDIA H100 SXM5 80GB,16384,Norway,NexGen Cloud,2024-04-19,"They seem to be stating that all 16k GPUs are in the same physical location, but I'm not fully confident in this",Private,23.378001920000003,582815283.1519562,"Follummoveien 94, 3516 Hønefoss, Norway",Cloud,H100,"""With up to 16,384 H100 80G cards operating in a single cluster""

""by June the supercloud will be comprised of 20,000 Nvidia H100 Tensor Core GPUs""",2024-04-19,,11.840984455358647,,,,,,,16384,NVIDIA,,True,,3,19.2097918453239,32423936000000000000,32423936000000000000,16210329600000000000,8105164800000000000,23.378001920000003,,693400986768.3336,582815283.1519562,,,,,,Meta GenAI 2024a,0.6666666666666665,https://web.archive.org/web/20240911111117/https://www.datacenterdynamics.com/en/news/nexgen-cloud-to-host-ai-supercloud-in-norwegian-data-center/,https://archive.ph/XcQQY,https://web.archive.org/web/20240524063011/https://www.hyperstack.cloud/nvidia-h100-sxm,,,60.205078,10.235459
Oracle OCI Supercluster H100s,Existing,Likely,Yes,19.510865733502133,16384.00000024128,NVIDIA H100 SXM5 80GB,16384,United States of America,Oracle,2024-04-15,"Unclear when this was first operational. In November 2023, they state that OCI superclusters ""can scale up to"" 16k H100s. However, it's unclear whether they had actually built a 16k cluster yet, or were just accepting orders. Musk's xAI was said to be renting 16k of these GPUs in May 2024, signaling that at least one Oracle cluster of this size was operational by then. I mark the first operational date as April 2024 to be slightly conservative. 
Also, it's very likely that Oracle has several of these 16k H100 clusters",Private,23.378001920000003,584846286.9215268,,"Cloud,Cohere,xAI",H100,"""With high-performance local storage, high-performance computing (HPC) storage, cluster networking and memory, bare metal instances can be part of an OCI Supercluster that can scale to tens of thousands of NVIDIA H100 GPUs.""
""An OCI Supercluster with H100 GPUs can scale up to 16,384 GPUs""",2024-04,,11.840984455358647,,,,,,,16384,NVIDIA,,True,,3,19.2097918453239,32423936000000000000,32423936000000000000,16210329600000000000,8105164800000000000,23.378001920000003,,693400986768.3336,584846286.9215268,,,,,,Meta GenAI 2024a,0.6666666666666665,https://web.archive.org/web/20231023100410/https://blogs.oracle.com/cloud-infrastructure/post/general-availability-oci-compute-nvidia-h100,https://archive.ph/B3HJm,https://archive.ph/iLAnE,https://archive.ph/cgqbk,,39.381266,-97.922211
Paper on Llama 3.1,Existing,Confirmed,Yes,19.510865733502133,16384.00000024128,NVIDIA H100 SXM5 80GB,16384,,Meta AI,2024-07-23,,Private,23.378001920000003,584357946.473871,,Meta,H100,,2024-07-23,,11.840984455358647,,,True,"Meta GenAI 2024a,Meta GenAI 2024b",,,16384,NVIDIA,,,,3,19.2097918453239,32423936000000000000,32423936000000000000,16210329600000000000,8105164800000000000,23.378001920000003,,693400986768.3336,584357946.473871,,,,,,Meta GenAI 2024a,0.6666666666666665,https://web.archive.org/web/20250213040824/https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/468347782_9231729823505907_4580471254289036098_n.pdf?_nc_sid=3c67a6&_nc_zt=14&oe=67B094C0&_nc_gid=Amq2wPDHtohnYaTcmwzmEM_&_nc_cat=110&_nc_ohc=qCynUx1yT0sQ7kNvgGyJpa7&ccb=1-7&oh=00_AYCdVmbLn7nRo_-1yU_lxmEbGI1Bby-xGC_1TZe4QDWaPA&_nc_ht=scontent-sjc3-1.xx,,,,,,
Project Ceiba Phase 1,Planned,Unlikely,Yes,19.510865733502133,16384.00000024128,NVIDIA GH200,16384,United States of America,"Amazon,NVIDIA",,This plan has been updated to use Blackwell chips. Unclear if this supercomputer was ever completed,Private,22.960537600000002,751209101.1855801,,"Cloud,NVIDIA",GH200,"The system will feature 16,384 GH200 NVL32 Grace Hopper Superchips and is expected to be the world’s largest cloud AI supercomputer.",Planned,"This was officially planned, but it appears they have since updated their plan (see Project Ceiba Phase 2)",11.848853685384858,,,,,,,16384,NVIDIA,,,,,19.209835737838148,32423936000000000000,32423936000000000000,16211968000000000000,8101888000000000000,22.960537600000002,,706079634651.0632,751209101.1855801,,,,,,,,https://web.archive.org/web/20240228165625/https://www.datacenterdynamics.com/en/news/project-ceiba-aws-and-nvidia-plan-to-build-worlds-largest-cloud-ai-supercomputer/,,,,,39.381266,-97.922211
Yotta Shakti Cloud D1,Planned,Likely,Yes,19.510865733502133,16384.00000024128,NVIDIA H100 SXM5 80GB,16384,India,Yotta Data Services,,,Private,22.960537600000002,770056053.2396562,"Yotta Data Center Park, Greater Noida
Plot No 7, Tusiana Village, Knowledge Park V, Greater Noida, Tusyana, Uttar Pradesh 201301, India",Cloud,H100,"Yotta will deploy the first cluster of 16,384 GPUs at NM1... Next, Yotta will deploy a similar-sized cluster at D1",Planned 2025,"Size is officially stated to be ""similar"" as NM1 cluster",11.848809792870606,,,,,,,16384,NVIDIA,,,,,19.2097918453239,32423936000000000000,32423936000000000000,16210329600000000000,8105164800000000000,22.960537600000002,,706008277436.8489,770056053.2396562,,,,,,,,https://web.archive.org/web/20240521205830/https://yotta.com/media/press-release-yotta-data-services-collaborates-with-nvidia-to-catalyze-indias-ai-transformation/,,,,,,
Yotta Shakti Cloud NM1 Phase 2,Planned,Confirmed,Yes,19.510865733502133,16384.00000024128,NVIDIA H100 SXM5 80GB,16384,India,Yotta Data Services,,,Private,22.960537600000002,770056053.2396562,"Yotta Datacenter Park - Panvel Hiranandani Fortune City. Survey No. 30, MH SH 76, Panvel, Navi Mumbai, Maharashtra 410206, India",Cloud,H100,"Yotta will deploy the first cluster of 16,384 GPUs at NM1",Planned 2025,,11.848809792870606,Yotta Shakti Cloud NM1 Phase 1,,,,,,16384,NVIDIA,,,,,19.2097918453239,32423936000000000000,32423936000000000000,16210329600000000000,8105164800000000000,22.960537600000002,,706008277436.8489,770056053.2396562,,,,,,,,https://web.archive.org/web/20240527112515/https://shakticloud.ai/,https://web.archive.org/web/20240423043934/https://yotta.com/media/press-release-yotta-receives-indias-first-ever-consignment-of-nvidia-h100-the-fastest-gpus-in-the-world-at-its-nm1-data-center-park/,https://web.archive.org/web/20240521205830/https://yotta.com/media/press-release-yotta-data-services-collaborates-with-nvidia-to-catalyze-indias-ai-transformation/,,,19.030146,73.02022
Anonymized Chinese System,Planned,Likely,Yes,19.477121254719663,20000,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.477121254719663,30000000000000000000,,30000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Sakura's B200s Phase 2,Planned,Confirmed,Yes,19.475293376659103,15095.502779403707,NVIDIA B200,4000,Japan,Sakura Internet,,"This builds on their previous 2k H100s
""Japan's Sakura Internet has secured around 20 billion yen worth ($130 million) of Nvidia's next-generation B200 chips"" ~= 4k B200s

10000 GPUs
at least 2k of these are H100s
Unclear amount of B200s
I estimate 4k B200s, and 6k H100s, based on a statement that they acquired $130 million worth of B200s
",Public/Private,16.416400000000003,482340860.9113168,"Ishikari City, Hokkaido Japan",Cloud,"B200,H100","The company plans to expand the number of GPUs deployed in “Koukaryoku” to fivefold from the initially planned quantity, aiming to incorporate around 10,000 GPUs, including NVIDIA’s latest “NVIDIA HGX B200 system” introduced in March",Planned Q1 2028,Unclear how many are H100s vs B200s,11.958968010069242,Sakura's H100s Phase 1,,,,NVIDIA H100 SXM5 80GB,6000,10000,NVIDIA,NVIDIA,,,,19.174245935596257,29874000000000000000,29874000000000000000,14936400000000000000,7468200000000000000,16.416400000000003,,909846251309.6658,482340860.9113168,,,,,,,,https://web.archive.org/web/20240603205526/https://www.trendforce.com/news/2024/04/22/news-japans-sakura-ai-gpu-procurement-reportedly-increases-fivefold-including-purchase-of-nvidia-b200/,https://web.archive.org/web/20240913153045/https://www.smbom.com/news/13481,,,,43.172542,141.315389
G42 Microsoft 30Mw UAE Cluster A,Planned,Likely,Yes,19.472537053262077,15000.000000220876,NVIDIA H100 SXM5 80GB,15000,United Arab Emirates,"G42,Microsoft",,At least one of the 30MW data centers is in Abu Dhabi,Public/Private,30,705007372.9611111,"Abu Dhabi, United Arab Emirates","Microsoft,Azure",H100,"Private conversations with Microsoft and G42 executives revealed that G42 and Khazna are building three new data centers in the UAE for Microsoft. The centers will be 30, 30, and 100 megawatts each, the equivalent of nearly 100,000 H100 chips (approximately 15,000, 15,000, and 50,000–60,000 respectively).",Planned,,11.69434191036418,,,,,,,15000,NVIDIA,,,,,19.17146316508384,29685000000000000000,29685000000000000000,14841000000000000000,7420500000000000000,21.021,30,494700000000,705007372.9611111,,,,,,,,https://web.archive.org/web/20250221170709/https://www.csis.org/analysis/united-arab-emirates-ai-ambitions,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,,24.453524,54.377438
G42 Microsoft 30Mw UAE Cluster B,Planned,Likely,Yes,19.472537053262077,15000.000000220876,NVIDIA H100 SXM5 80GB,15000,United Arab Emirates,"G42,Microsoft",,,Public/Private,30,705007372.9611111,,"Microsoft,Azure",H100,"Private conversations with Microsoft and G42 executives revealed that G42 and Khazna are building three new data centers in the UAE for Microsoft. The centers will be 30, 30, and 100 megawatts each, the equivalent of nearly 100,000 H100 chips (approximately 15,000, 15,000, and 50,000–60,000 respectively).",Planned,,11.69434191036418,,,,,,,15000,NVIDIA,,,,,19.17146316508384,29685000000000000000,29685000000000000000,14841000000000000000,7420500000000000000,21.021,30,494700000000,705007372.9611111,,,,,,,,https://web.archive.org/web/20250221170709/https://www.csis.org/analysis/united-arab-emirates-ai-ambitions,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,,23.85206,54.256172
Microsoft NVIDIA partnership,Planned,Unlikely,Unclear,19.472537053262077,15000.000000220876,NVIDIA H100 SXM5 80GB,15000,United States of America,Microsoft,2023-05-15,"The plans in this article are very vague. This superocmputer probably ended up getting built, but we don't know when, and we likely capture it as another independent entry",Private,21.785399999999996,715407335.7359537,,,"H100,A100","It is the first public cloud to incorporate NVIDIA’s advanced AI stack, adding tens of thousands of NVIDIA A100 and H100 GPUs, NVIDIA Quantum-2 400Gb/s InfiniBand networking and the NVIDIA AI Enterprise software suite to its platform.",2023-05-15,"Possible duplicate, number of GPUs says ""thousands"" so this is a very rough estimate",11.833297626692357,,,,,,,15000,NVIDIA,,,,,19.17146316508384,29685000000000000000,29685000000000000000,14841000000000000000,7420500000000000000,21.785399999999996,,681236057175.9069,715407335.7359537,,,,,,,,https://web.archive.org/web/20240819113910/https://nvidianews.nvidia.com/news/nvidia-microsoft-accelerate-cloud-enterprise-ai,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Likely,Yes,19.477121254719663,10000,,,China,,2024-10-15,,Public/Private,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,True,,9,19.477121254719663,30000000000000000000,,30000000000000000000,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.1,,,,,,35.486703,101.901875
iGenius Colosseum,Planned,Likely,Yes,19.459392487759228,14552.804446904473,NVIDIA GB200 NVL2,5760,Italy,iGenius,,80 servers x 72 GPUs/server = 5760 GPUs,Private,13.837824000000001,,Southern Italy,iGenius,GB200,iGenius aims to develop colossal AI models using 80 GB200 NVL72 servers,Planned 2025,,12.017294689481112,,,,,,,5760,NVIDIA,,,,,19.158362492095247,28800000000000000000,28800000000000000000,14400000000000000000,7200000000000000000,13.837824000000001,,1040626040626.0405,,,,,,,,,https://web.archive.org/web/20250306213958/https://opentools.ai/news/igenius-and-nvidia-to-transform-ai-landscape-with-colosseum-project,,,,,42.504154,12.646361
Microsoft Azure Eagle,Existing,Confirmed,Yes,19.454808286301645,14400.000000212056,NVIDIA H100 SXM5 80GB,14400,United States of America,Microsoft,2023-11-15,Said to be part of a larger system,Private,20.913984,476613908.53963214,,Cloud,H100,"""With 14,400 NVIDIA H100 GPUs (14400/8 = 1800 nodes?), InfiniBand, and Intel Xeon Sapphire Rapids CPUs, the new""",2023-11-15,,11.833297626692357,,,,,,,14400,NVIDIA,,True,,1,19.15373439812341,28497600000000000000,28497600000000000000,14247360000000000000,7123680000000000000,20.913984,,681236057175.9069,476613908.53963214,,,,,,Microsoft Azure Eagle,1,https://web.archive.org/web/20240901103636/https://www.servethehome.com/microsoft-azure-eagle-is-a-paradigm-shifting-cloud-supercomputer-nvidia-intel/,,,,,39.381266,-97.922211
TensorWave MI300X Cluster 1 Phase 2,Planned,Likely,Yes,19.41745508506616,13213.239009795365,AMD Instinct MI300X,10000,United States of America,TensorWave,,Unclear how many would be in each datacenter. Assuming half in each,Private,15.015000000000002,271858907.89310706,,Cloud,MI300X,"""By the end of 2024, TensorWave aims to have 20,000 MI300X accelerators deployed across two facilities""",Planned H1 2025,,11.939883144094898,TensorWave MI300X Cluster 1 Phase 1,,,,,,10000,AMD,,,,,19.116408480629897,26149000000000000000,26149000000000000000,13074000000000000000,6537000000000000000,15.015000000000002,,870729270729.2706,271858907.89310706,,,,,,,,https://web.archive.org/web/20241009165502/https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/,,,,,39.381266,-97.922211
TensorWave MI300X Cluster 2,Planned,Likely,Yes,19.41745508506616,13213.239009795365,AMD Instinct MI300X,10000,United States of America,TensorWave,,Unclear how many would be in each datacenter. Assuming half in each,Private,15.015000000000002,271858907.89310706,,Cloud,MI300X,"By the end of 2024, TensorWave aims to have 20,000 MI300X accelerators deployed across two facilities",Planned H1 2025,,11.939883144094898,,,,,,,10000,AMD,,,,,19.116408480629897,26149000000000000000,26149000000000000000,13074000000000000000,6537000000000000000,15.015000000000002,,870729270729.2706,271858907.89310706,,,,,,,,https://web.archive.org/web/20241009165502/https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/,,,,,39.381266,-97.922211
Saudi Data & AI Authority Sovereign AI factory,Planned,Likely,Yes,19.397940008672034,12632.642749049,NVIDIA GB200 NVL2,5000,Saudi Arabia,Saudi Data and Artificial Intelligence Authority,,,Public,12.012000000000002,,,,"Blackwell,Uncertain","NVIDIA and the Saudi Data & AI Authority (SDAIA) will deploy up to 5,000 Blackwell GPUs for a sovereign AI factory and enable smart city solutions",Planned,,12.017294689481112,,,,,,,5000,NVIDIA,,,,,19.096910013008056,25000000000000000000,25000000000000000000,12500000000000000000,6250000000000000000,12.012000000000002,,1040626040626.0404,,,,,,,,,https://archive.ph/vvAxQ,,,,,23.384784,44.652426
xAI Fulton Georgia,Planned,Likely,Yes,19.383443939252025,12217.944416551742,NVIDIA H100 SXM5 80GB,12112,United States of America,xAI,,"The article indicates that the 12,448 GPUs are 97% Hoppers, 3% Amperes

Location isn't confirmed, but an article from a year earlier reports that xAI received a ""10-year tax break on a $700 million project to deploy IT equipment at QTS’ 1025 Jefferson Street site"", which matches the description that this site is in Fulton, GA",Private,20,,"1025 Jefferson St NW, Atlanta, GA 30318","xAI,X","H100,A100","""The Georgia facility will house an estimated 12,448 Nvidia GPUs. The vast majority of these are Hopper generation H100 GPUs""
""Roughly 3% of the chips are Nvidia's less-powerful A100 GPUs""",Planned 2025,,11.781340436030284,,,,,NVIDIA A100,336,12448,NVIDIA,NVIDIA,,,,19.08237043169426,24179312000000000000,24179312000000000000,12088444800000000000,6044222400000000000,17.2428256,20,604422240000,,700000000,,"Of the $700 million in accelerated computing hardware going into the facility, $442 million is allocated to X, and $258 million is allocated to xAI",,,,,https://archive.ph/Oeulg,https://web.archive.org/web/20250125054811/https://www.datacenterdynamics.com/en/news/elon-musks-twitterx-granted-10-million-tax-break-for-ai-hardware-in-georgia/,,,,33.776722,-84.419589
Anonymized Chinese System,Existing,Likely,Yes,19.301029995663978,10000,,,China,,2025-02-15,,Private,,,China,,,,,,,,,,,,,10000,Anonymized,Anonymized,True,,17,,20000000000000000000,20000000000000000000,,,,,,,,,,,,xAI Colossus Memphis Phase 2,0.06,,,,,,35.486703,101.901875
Foxconn Hon Hai Kaohsiung Supercomputer,Planned,Confirmed,Yes,19.36248247475117,11642.243557523583,NVIDIA GB200 NVL2,4608,Taiwan,Foxconn,,"There was a later announcement for a 10k Blackwell Foxconn supercomputer, so slightly unclear if this just got upgraded or if they are seperate supercomputers",Private,11.0702592,,"806, Taiwan, Kaohsiung City, Cianjhen District, Chenggong 2nd Rd, 25號2 樓",Foxconn,GB200,"will be built around NVIDIA’s groundbreaking Blackwell architecture and feature the GB200 NVL72 platform, which includes a total of 64 racks and 4,608 Tensor Core GPUs",Planned 2026,,12.017294689481112,,,,,,,4608,NVIDIA,,,,,19.06145247908719,23040000000000000000,23040000000000000000,11520000000000000000,5760000000000000000,11.0702592,,1040626040626.0405,,,,,,,,,https://web.archive.org/web/20241009234027/https://blogs.nvidia.com/blog/foxconn-taiwan-blackwell/,https://web.archive.org/web/20241008143947/https://www.datacenterdynamics.com/en/news/nvidia-and-foxconn-to-build-taiwans-fastest-ai-supercomputer/,,,,22.604827,120.301292
NVIDIA MLPerf v4.0 Submission 2024,Existing,Confirmed,Yes,19.36150239756241,11616.000000170978,NVIDIA H100 SXM5 80GB,11616,,NVIDIA,2024-06-12,It seems likely that this could be an expansion on the NVIDIA Eos-DFW supercomputer,Private,16.574638080000003,414709261.30290306,,NVIDIA,H100,"In this round of MLPerf Training, NVIDIA has more than tripled its submission scale to 11,616 H100 GPUs and more than tripled performance to 3.4 minutes to train,",2024-06-12,,11.840984455358647,,,True,NVIDIA CoreWeave Eos-DFW Phase 1,,,11616,NVIDIA,,,,6,19.06042850938418,22988064000000000000,22988064000000000000,11492870400000000000,5746435200000000000,16.574638080000003,,693400986768.3335,414709261.30290306,,,,,,Meta GenAI 2024a,0.47265624999999634,https://web.archive.org/web/20240915170526/https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/,,,,,,
Anonymized Chinese System,Existing,Confirmed,No,19.301029995663978,10000,,,China,,2024-06-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.301029995663978,20000000000000000000,,20000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Alps Supercomputer Phase 2,Existing,Confirmed,Yes,19.327935049916142,10752.000000158252,NVIDIA GH200,10752,Switzerland,ETH Domain,2024-09-17,"Total number of sockets = 10,752
There are other portions of this supercomputer, but it is unclear which are in which location, as they are distributed. The 10k Grace Hopper GPUs is by far the largest and most significant portion, though",Public,11.9088,119153004.50529517,"Swiss National Supercomputing Centre
Via Trevano 131, 6900 Lugano, Switzerland",Swiss Universities,GH200,"has 10,752 of the world’s coveted NVIDIA Grace Hopper superchips",2024-09-17,Specs confirmed by the organization building this,11.951037052604368,Alps Supercomputer Phase 1,,,,,,10752,NVIDIA,,True,,10,19.026905054252165,21278208000000000000,21278208000000000000,10639104000000000000,5316864000000000000,15.34181376,11.9088,893381700927.0455,492647200.836752,118000000,119153004.50529517,"""the CHF100 million ($118 million) supercomputer""",,,xAI Colossus Memphis Phase 1,0.1075200000000002,https://web.archive.org/web/20240519135849/https://www.cscs.ch/computers/alps,https://web.archive.org/web/20240415160748/https://www.hpcwire.com/2023/04/05/into-the-alps-what-exactly-is-the-new-swiss-supercomputer-infrastructure/,https://web.archive.org/web/20241004224121/https://www.hpcwire.com/off-the-wire/cscs-debuts-alps-supercomputer-enhancing-switzerlands-research-infrastructure/,https://web.archive.org/web/20250215163401/https://www.top500.org/system/180259/,https://www.swissinfo.ch/eng/science/how-switzerlands-alps-supercomputer-aims-to-advance-ai/87659724,46.024451,8.960052
Microsoft Azure MLPerf 3.1 Submission,Existing,Confirmed,Yes,19.327935049916142,10752.000000158252,NVIDIA H100 SXM5 80GB,10752,United States of America,Microsoft,2023-11-08,Likely is CoreWeave Eos-DFW Phase 1,Private,15.61577472,355871718.3762587,,Microsoft,H100,"The GPT-3 LLM model and its 175 billion parameters were trained to completion in four minutes on 1,344 ND H100 v5 virtual machines (VMs), which represent 10,752 NVIDIA H100 Tensor Core GPUs",2023-11-08,,11.833297626692357,,,True,Microsoft Azure Eagle,,,10752,NVIDIA,,,,1,19.02686116173791,21278208000000000000,21278208000000000000,10638028800000000000,5319014400000000000,15.61577472,,681236057175.9069,355871718.3762587,,,,,,NVIDIA CoreWeave Eos-DFW Phase 1,1,https://web.archive.org/web/20240415030521/https://azure.microsoft.com/en-us/blog/azure-sets-a-scale-record-in-large-language-model-training/,,,,,39.381266,-97.922211
NVIDIA CoreWeave Eos-DFW Phase 1,Existing,Confirmed,Yes,19.327935049916142,10752.000000158252,NVIDIA H100 SXM5 80GB,10752,United States of America,"NVIDIA,CoreWeave",2023-11-08,There's also a 4.6k cluster with the same name,Private,15.61577472,355871718.3762587,"1000 Coit Rd, Plano, TX 75075","NVIDIA,CoreWeave",H100,"NVIDIA’s AI platform raised the bar for AI training and high performance computing in the latest MLPerf industry benchmarks. Among many new records and milestones, one in generative AI stands out: NVIDIA Eos — an AI supercomputer powered by a whopping 10,752 NVIDIA H100 Tensor Core GPUs and NVIDIA Quantum-2 InfiniBand networking — completed a training benchmark based on a GPT-3 model with 175 billion parameters trained on one billion tokens in just 3.9 minutes.",2023-11-08,Stated in NVIDIA blog post,11.833297626692357,,,,,,,10752,NVIDIA,,True,,1,19.02686116173791,21278208000000000000,21278208000000000000,10638028800000000000,5319014400000000000,15.61577472,,681236057175.9069,355871718.3762587,,,,,,NVIDIA CoreWeave Eos-DFW Phase 1,1,https://web.archive.org/web/20240430123932/https://blogs.nvidia.com/blog/scaling-ai-training-mlperf/,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,,,,33.014978,-96.764937
Oracle OCI Supercluster A100s,Existing,Likely,Yes,19.31063452464214,10332.103082516944,NVIDIA A100,32768,United States of America,Oracle,2023-11-15,"Unclear exactly when this first became operational, if it was ever built. The announcement March 2023 states Oracle OCI can now scale to 32k A100s, but it's unclear whether this implies that a 32k A100 cluster has already been built, or just that they have the capacity to build one. It seems likely that a cluster of this size was built, but there's little direct evidence for this. 
I estimate Q4 2023 to be slightly conservative on the timing for when it was first operational. ",Private,27.194818559999998,646451863.8502183,,Cloud,A100,"OCI Supercluster networking can now scale up to 4,096 OCI Compute Bare Metal instances with 32,768 A100 GPUs",Q4 2023,,11.575118363368931,,,,,,,32768,NVIDIA,,True,,4,19.00960452897816,20447232000000000000,20447232000000000000,10223616000000000000,5111808000000000000,27.194818559999998,,375939849624.0602,646451863.8502183,,,,,,Microsoft Azure Eagle,0.7175071584975551,https://web.archive.org/web/20240910231627/https://www.oracle.com/news/announcement/nvidia-chooses-oracle-cloud-infrastructure-for-ai-services-2023-03-21/,https://archive.ph/B3HJm,,,,39.381266,-97.922211
Google TPUv5e,Existing,Confirmed,Unclear,19.30148559209698,10116.721576702757,Google TPU v5e,50944,,Google,2023-11-08,,Private,14.797805568,158260458.05800846,,Google,TPUv5e,"we used Multislice Training to run what we believe to be the world’s largest publicly disclosed LLM distributed training job (in terms of the number of chips used for training) on a compute cluster of 50,944 Cloud TPU v5e chips (spanning 199 Cloud TPU v5e pods) that is capable of achieving 10 exa-FLOPs (16-bit), or 20 exa-OPs (8-bit), of total peak performance",2023-11-08,Unclear whether it is a single cluster,11.829802683278658,,,,,,,50944,Google,,,,,19,20020992000000000000,20020992000000000000,10000000000000000000,,14.797805568,,675775874608.383,158260458.05800846,,,,,,,,https://web.archive.org/web/20240822230040/https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e,,,,,,
Imbue 10k Cluster,Existing,Likely,Yes,19.296445794206395,10000.000000147247,NVIDIA H100 SXM5 80GB,10000,United States of America,Imbue,2023-09-07,,Private,14.523599999999998,332254920.4479826,,Imbue,H100,"Dell PowerEdge server cluster with nearly 10,000 NVIDIA H100 Tensor Core GPUs",2023-09-07,,11.833297626692357,,,,,,,10000,NVIDIA,,True,,1,18.99537190602816,19790000000000000000,19790000000000000000,9894000000000000000,4947000000000000000,14.523599999999998,,681236057175.9069,332254920.4479826,,,,,,Tesla 10k H100 Cluster,1,https://web.archive.org/web/20240423234157/https://investors.delltechnologies.com/news-releases/news-release-details/imbue-develop-next-generation-ai-models-150-million-dell-high,https://web.archive.org/web/20240830093115/https://imbue.com/company/introducing-imbue/,,,,39.381266,-97.922211
Poolside 10k Cluster,Planned,Likely,Yes,19.296445794206395,10000.000000147247,NVIDIA H100 SXM5 80GB,10000,,Poolside,,Unclear if they own this or if they're just renting it.,Private,14.014000000000001,470004915.3074074,,Poolside,H100,"With this new funding round we have been able to scale our training cluster to 10,000 GPUs and start the work of scaling up both RLCEF and our model training.",Planned,"Unclear if they own this or if they're just renting it.
Type of GPU unspecified, I'm assuming they're H100s",11.848809792870606,,,,,,,10000,NVIDIA,,,,,18.99537190602816,19790000000000000000,19790000000000000000,9894000000000000000,4947000000000000000,14.014000000000001,,706008277436.8488,470004915.3074074,,,,,,,,https://web.archive.org/web/20241007114224/https://poolside.ai/checkpoint/announcing-our-500-million-fundraise-to-make-progress-towards-agi,,,,,,
Tesla 10k H100 Cluster,Existing,Confirmed,Yes,19.296445794206395,10000.000000147247,NVIDIA H100 SXM5 80GB,10000,United States of America,Tesla,2023-08-28,"There are claims of the cluster costing $300 million, however, that was not an official anouncement and might just be a BOTEC ""30k per H100 times 10,000",Private,14.523599999999998,360340094.31769186,"""on prem"" for Tesla",Tesla,H100,"Tomorrow, @Tesla will turn on a massive and very expensive 10,000 unit NVIDIA H100 GPU cluster to help it train FSD",2023-08-28,"Said it was ""going live tomorrow"" in Aug 2023",11.833297626692357,,,,,,,10000,NVIDIA,,True,,1,18.99537190602816,19790000000000000000,19790000000000000000,9894000000000000000,4947000000000000000,14.523599999999998,,681236057175.9069,360340094.31769186,,,,,,Tesla 10k H100 Cluster,1,https://web.archive.org/web/20240802004227/https://www.theregister.com/2023/08/30/tesla_nvidia_supercomputer/,https://archive.ph/PwU9T,https://web.archive.org/web/20240524114346/https://www.tomshardware.com/news/teslas-dollar300-million-ai-cluster-is-going-live-today,,,42.410212,-82.981595
SoftBank Planned B200 Superpod,Planned,Likely,Yes,19.255272505103306,9095.502779315366,NVIDIA B200,4000,Japan,Softbank,,"They had 6k GPUs before (2k A100s, 4k H100s), so bringing the total to 10k GPUs would be adding 4k Blackwell GPUs",Private,8.008000000000001,200337911.7268724,Japan,Softbank,B200,"The company has further plans to expand this with an Nvidia DGX SuperPOD featuring GGX B200 systems within the fiscal year (FY) 2025, ultimately growing the number of GPUs installed to around 10,000 with a compute capability of 25.7 exaflops.",Planned,,12.05071844496806,,,,,,,4000,NVIDIA,,,,,18.954242509439325,18000000000000000000,18000000000000000000,9000000000000000000,4500000000000000000,8.008000000000001,,1123876123876.1238,200337911.7268724,,,,,,,,https://web.archive.org/web/20241116120654/https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse,https://web.archive.org/web/20241221053240/https://www.datacenterdynamics.com/en/news/softbank-installs-4000-nvidia-h100s-for-japanese-ai-computing-platform/,,,,36.386493,138.59223
S. Korea 6th national supercomputer,Planned,Likely,Yes,19.24092846635656,8800.000000129516,NVIDIA H100 SXM5 80GB,8800,Korea (Republic of),Ministry of Science and ICT,,,Public,12.332320000000001,413604325.4705186,Korea (Republic of),Researchers,H100,"A total of 8,800 GPUs will be used for No. 6",Planned 2026,Uncertain what type of GPUs will be used,11.848809792870606,,,,,,,8800,NVIDIA,,,,,18.93985457817833,17415200000000000000,17415200000000000000,8706720000000000000,4353360000000000000,12.332320000000001,,706008277436.8488,413604325.4705186,,,,,,,,https://web.archive.org/web/20250110174004/https://biz.chosun.com/en/en-science/2025/01/10/A4AEH4EX2JEA7CGFXUOADNCGD4/,,,,,,
Sesterce Nordics,Existing,Likely,Yes,19.209835737838148,8192.000000120572,NVIDIA H100 SXM5 80GB,8192,,Sesterce,2024-06-29,"This cluster was originally planned for Spain, but was later moved to the Nordics",Private,11.689000960000001,291932774.8579785,The Nordics,Cloud,H100,"8192 H100s available from 06/29/2024 to 06/29/2026 (Spain)
Sesterce 1024 Servers Cluster available in Spain Europe End of June.",2024-06-29,,11.840984455358647,,,,,,,8192,NVIDIA,,True,,12,18.908761849659918,16211968000000000000,16211968000000000000,8105164800000000000,4052582400000000000,11.689000960000001,,693400986768.3336,291932774.8579785,,,,,,Meta GenAI 2024a,0.3333333333333305,https://web.archive.org/web/20240523122734/https://gpulist.ai/detail/f6be42a,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,19.301029995663978,8000,,30000.000000000004,China,,,,Public,20,,China,,,,,,11.999999999999998,,,,,,,30000.000000000004,Anonymized,Anonymized,,,,19.301029995663978,20000000000000000000,,20000000000000000000,,20,,1000000000000,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,19.301029995663978,8000,,,China,,2024-03-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,7,19.301029995663978,20000000000000000000,,20000000000000000000,,,,,,,,,,,Microsoft Azure Eagle,0.6,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,No,19.301029995663978,8000,,,China,,2023-04-15,,,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,20000000000000000000,20000000000000000000,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,Yes,19.301029995663978,8000,,8000,China,,2024-08-15,,Private,10,300000000,China,,,,,,11.903089986991942,,,,,,,8000,Anonymized,Anonymized,True,,15,18.90308998699194,20000000000000000000,20000000000000000000,7999999999999999000,3999999999999999500,10,,799999999999.9999,300000000,,,,,,Meta GenAI 2024a,0.3,,,,,,35.486703,101.901875
Magic G4 Google Cloud Rental,Existing,Confirmed,Yes,19.19953578119834,8000.000000117801,NVIDIA H100 SXM5 80GB,8000,United States of America,Google,,"Rented from Google Cloud. Not marked ""potential duplicate"" because we don't have other operational Google cloud clusters of this size",Private,11.2112,376003932.2459259,,Magic,H100,"Magic-G4 supercomputer, which will be based on Nvidia H100 Tensor Core graphics processor unit (GPU) chips. Magic says it has 8,000 of the hugely powerful chips",Planned,,11.848809792870606,,,,,,,8000,NVIDIA,,True,,,18.898461893020105,15832000000000000000,15832000000000000000,7915200000000000000,3957600000000000000,11.2112,,706008277436.8489,376003932.2459259,,,,,,,,https://archive.ph/DLF9r,https://archive.ph/PZl4u,,,,39.381266,-97.922211
Nebius 8k Finland Phase 1,Existing,Likely,Yes,19.19953578119834,8000.000000117801,NVIDIA H100 SXM5 80GB,8000,Finland,Nebius AI,2024-05-10,,Private,11.415040000000001,284577774.9765411,"Moreenikatu 6, 04600 Mäntsälä, Finland",Cloud,H100,"we also constructed an 8,000-GPU supercluster",2024-05-10,,11.840984455358647,,,,,,,8000,NVIDIA,,True,,12,18.898461893020105,15832000000000000000,15832000000000000000,7915200000000000000,3957600000000000000,11.415040000000001,,693400986768.3336,284577774.9765411,,,,,,Meta GenAI 2024a,0.32552083333333276,https://web.archive.org/web/20241008144121/https://www.datacenterdynamics.com/en/news/nebius-deploys-ai-cluster-at-equinix-data-center-in-paris/,https://archive.ph/x0bXS,,,,60.629522,25.26336
Tesla Dojo 1 Planned Phase 2,Planned,Likely,Yes,19.19953578119834,8000.000000117801,Tesla D1 Dojo,,United States of America,Tesla,,"""roughly 8k H100-equivalent"" I interpreted this as ",Private,,,"San Jose, California",Tesla,Dojo,And Dojo 1 will have roughly 8k H100-equivalent of training online by end of year,Planned,"Source is one Musk Tweet. Uncertain on the exact number of chips, because Musk only specifies H100 equivalents",,Tesla Dojo 1 Phase 1,,,,,,,Tesla,,,,,18.904823699800943,15832000000000000000,15832000000000000000,8032000000000000000,,,,,,,,,,,,,https://archive.ph/pM3CE,https://web.archive.org/web/20240903213050/https://techcrunch.com/2024/08/10/teslas-dojo-a-timeline/,,,,37.336157,-121.890608
Microsoft GPT-4 cluster,Existing,Likely,Yes,19.193124598354462,7882.769075406651,NVIDIA A100,25000,United States of America,"Microsoft,OpenAI",2022-04-30,"Training finished August 2022. Training lasted ~3 months, meaning that it was operational late May at the latest",Private,21.294,562523106.0274715,"Des Moines, Iowa",OpenAI,A100,"OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU....",2022-04-30,"Uncertain, but seems likely located in Des Moines, Iowa",11.563837352959242,,,,,,,25000,NVIDIA,,True,,1,18.892094602690477,15600000000000000000,15600000000000000000,7800000000000000000,3900000000000000000,21.294,,366300366300.3663,562523106.0274715,,,,True,,Microsoft GPT-4 cluster,1,https://archive.ph/qTvzU,https://archive.ph/vqdRc#selection-4109.11-4109.239,https://web.archive.org/web/20241212034522/https://www.glennklockwood.com/garden/microsoft-supercomputers,https://web.archive.org/web/20250201064042/https://semianalysis.com/2023/07/10/gpt-4-architecture-infrastructure/,,41.584997,-93.624273
Aramco Groq Inference Cluster,Existing,Likely,Yes,19.170078275273156,7475.366346749768,GroqChip LPU v1,19725,Saudi Arabia,Saudi Aramco,2025-01-15,"This cluster is designed for only inference, not training",Private,8.49023175,674658904.266,"Dammam, Saudi Arabia","Saudi Aramco,Cloud",GroqChip,"""19,725 LPUs deployed Dammam, January 2025""",2025-01,,11.640255316204774,,,,,,,19725,Unknown,,True,,28,18.569174861145136,14793750000000000000,14793750000000000000,3708300000000000000,,8.49023175,,436772529795.7856,674658904.266,,,,,,xAI Colossus Memphis Phase 1,0.07475366346639756,https://web.archive.org/web/20250212161800/https://www.linkedin.com/posts/sundeepm_groq-aramco-digital-build-the-activity-7294312309013848064-T1fY/,,,,,26.4344,50.0982
Oak Ridge NL Frontier,Existing,Confirmed,Yes,19.158756074028645,7282.99949480149,AMD Radeon Instinct MI250X,37632,United States of America,US Department of Energy,2022-05-30,"(9408 nodes)x(4 GPUs/node) = 37,632 GPUs",Public,40,620445966.1887438,"Oak Ridge National Laboratory
5200, 1 Bethel Valley Rd, Oak Ridge, TN 37830","US Government,Academia,Industry",AMD MI250X,"The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes....
Each Frontier compute node consists of [1x] 64-core AMD “Optimized 3rd Gen EPYC” CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X",2022-05-30,Details released on official government website,11.556696082700684,,,,,,,37632,AMD,,True,,2,19.158756074028645,14413056000000000000,14413056000000000000,14413056000000000000,3601382400000000000,40.066790399999995,40,360326400000,967488702.6423289,600000000,620445966.1887438,$600M,True,,Microsoft GPT-4 cluster,0.9239138461538378,https://web.archive.org/web/20240720224959/https://docs.olcf.ornl.gov/systems/frontier_user_guide.html,https://web.archive.org/web/20240823003004/https://www.ornl.gov/news/frontier-supercomputer-debuts-worlds-fastest-breaking-exascale-barrier,,,,36.010294,-84.269567
Microsoft Sweden Gävle,Planned,Likely,Yes,19.120573931205847,6670.035371497883,,6666,Sweden,Microsoft,,"The ""Reported Manual Flop/s"" column is the number of INT8 FLOPs that this cluster could do assuming it was using (20k/3) GPUs, and they were all H100s. I assume that the true FLOP/s number will be higher as I assume they will likely use some Blackwell chips. Unclear how many GPUs will be in each of the three stated data centers, so this (20k/3) is a lower bound on how large the largest cluster must be. Also, it is unclear what type of GPUs will be used (though presumable some H100s and Blackwells). ",Private,,,"805 91 Gävle, Sweden","Cloud,Microsoft",,"Microsoft said it will place more than 20,000 graphic processing units (GPUs) at its data centers in Sandviken, Gävle, and Staffanstorp",Planned 2027,"Unclear how many GPUs will be in which location, and which type. FLOP/s count is lower bound on FLOP/s assuming that they're all H100s (and there are 20k/3 GPUs in each cluster)",,,,,,,,6666,Unknown,,,,,18.81954393554187,13200000000000000000,13200000000000000000,6600000000000000000,,,,,,,,,,,,,https://web.archive.org/web/20240915193858/https://www.datacenterdynamics.com/en/news/microsoft-to-deploy-20000-gpus-in-sweden/,,,,,60.616103,17.113423
Microsoft Sweden Sandviken,Planned,Likely,Yes,19.120573931205847,6670.035371497883,,6666,Sweden,Microsoft,,"The ""Reported Manual Flop/s"" column is the number of INT8 FLOPs that this cluster could do assuming it was using (20k/3) GPUs, and they were all H100s. I assume that the true FLOP/s number will be higher as I assume they will likely use some Blackwell chips. Unclear how many GPUs will be in each of the three stated data centers, so this (20k/3) is a lower bound on how large the largest cluster must be. Also, it is unclear what type of GPUs will be used (though presumable some H100s and Blackwells). ",Private,,,"Gamla Tunavägen, 811 42 Sandviken, Sweden","Cloud,Microsoft",,"Microsoft said it will place more than 20,000 graphic processing units (GPUs) at its data centers in Sandviken, Gävle, and Staffanstorp",Planned 2027,"Unclear how many GPUs will be in which location, and which type. FLOP/s count is lower bound on FLOP/s assuming that they're all H100s (and there are 20k/3 GPUs in each cluster)",,,,,,,,6666,Unknown,,,,,18.81954393554187,13200000000000000000,13200000000000000000,6600000000000000000,,,,,,,,,,,,,https://web.archive.org/web/20240915193858/https://www.datacenterdynamics.com/en/news/microsoft-to-deploy-20000-gpus-in-sweden/,,,,,60.636369,16.731854
Microsoft Sweden Staffanstorp,Planned,Likely,Yes,19.120573931205847,6670.035371497883,,6666,Sweden,Microsoft,,"The ""Reported Manual Flop/s"" column is the number of INT8 FLOPs that this cluster could do assuming it was using (20k/3) GPUs, and they were all H100s. I assume that the true FLOP/s number will be higher as I assume they will likely use some Blackwell chips. Unclear how many GPUs will be in each of the three stated data centers, so this (20k/3) is a lower bound on how large the largest cluster must be. Also, it is unclear what type of GPUs will be used (though presumable some H100s and Blackwells). ",Private,,,"Västanvägen 86, 245 42 Staffanstorp, Sweden","Cloud,Microsoft",,"Microsoft said it will place more than 20,000 graphic processing units (GPUs) at its data centers in Sandviken, Gävle, and Staffanstorp",Planned 2027,"Unclear how many GPUs will be in which location, and which type. FLOP/s count is lower bound on FLOP/s assuming that they're all H100s (and there are 20k/3 GPUs in each cluster)",,,,,,,,6666,Unknown,,,,,18.81954393554187,13200000000000000000,13200000000000000000,6600000000000000000,,,,,,,,,,,,,https://web.archive.org/web/20240915193858/https://www.datacenterdynamics.com/en/news/microsoft-to-deploy-20000-gpus-in-sweden/,,,,,55.637921,13.18615
Alps Supercomputer Phase 1,Existing,Confirmed,Yes,19.102625768190283,6400.000000094244,NVIDIA GH200,6400,Switzerland,ETH Domain,2024-06-01,,Public,8.6825,293225224.7466417,"Swiss National Supercomputing Centre
Via Trevano 131, 6900 Lugano, Switzerland",Swiss Universities,GH200,Calculated from Top500,2024-06-01,,11.862950980543502,,Alps Supercomputer Phase 2,,,,,6400,NVIDIA,,True,,15,18.8015957725263,12665600000000000000,12665600000000000000,6332800000000000000,3164800000000000000,9.132032,8.6825,729375179959.6891,293225224.7466417,,,,,,Meta GenAI 2024a,0.2604166666666663,https://web.archive.org/web/20250215163401/https://www.top500.org/system/180259/,,,,,46.024451,8.960052
AWS EC2 Trn1,Existing,Likely,Unclear,19.10037054511756,6366.851945520705,Amazon Trainium1,30000,United States of America,Amazon,2022-10-11,,Private,,,,Cloud,AWS Trainium,"customers will be able to scale the training of machine learning models with up to 30,000 Trainium accelerators interconnected with EFA petabit-scale networking",2022-10-11,,,,,,,,,30000,Amazon AWS,,,,,18.75587485567249,12600000000000000000,12600000000000000000,5700000000000000000,5700000000000000000,,,,,,,,,,,,https://web.archive.org/web/20240531005626/https://press.aboutamazon.com/2022/10/aws-announces-general-availability-of-amazon-ec2-trn1-instances-powered-by-aws-designed-trainium-chips,,,,,39.381266,-97.922211
Anonymized Chinese System,Planned,Likely,Yes,19,6000,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.77815125038364,10000000000000000000,10000000000000000000,6000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Paper on Movie Gen,Existing,Confirmed,Yes,19.084897001229848,6144.00000009043,NVIDIA H100 SXM5 80GB,6144,,Meta AI,2024-10-04,,Private,8.766750720000001,233622579.04443178,,Meta,H100,"""We trained the media generation models using up to 6,144 H100 GPUs, each running at 700W TDP and with 80GB HBM3,""",2024-10-04,,11.840984455358647,,,True,Meta GenAI 2024a,,,6144,NVIDIA,,,,24,18.783823113051618,12158976000000000000,12158976000000000000,6078873600000000000,3039436800000000000,8.766750720000001,,693400986768.3336,233622579.04443178,,,,,,xAI Colossus Memphis Phase 1,0.06144000000000011,https://web.archive.org/web/20250211214423/https://ai.meta.com/static-resource/movie-gen-research-paper,,,,,,
AIST ABCI 3.0,Existing,Confirmed,Yes,19.083764550830942,6128.0000000902255,NVIDIA H200 SXM,6128,Japan,National Institute of Advanced Industrial Science and Technology (AIST),2025-01-20,,Public,8.587779200000002,235596799.71093044,"Kashiwa Campus, The University of Tokyo
Japan, 〒277-0882 Chiba, Kashiwa, Kashiwanoha, 5 Chome−１−5","industry,academia,and government in Japan",H200,ABCI 3.0 consists of computing servers equipped with 6128 of the NVIDIA H200 GPUs,2025-01-20,,11.848853685384858,AIST ABCI 2.0,,,,,,6128,NVIDIA,,True,,32,18.78273455516696,12127312000000000000,12127312000000000000,6063656000000000000,3030296000000000000,8.587779200000002,,706079634651.0631,218948448.5368963,233159040,235596799.71093044,36B yen,,,xAI Colossus Memphis Phase 1,0.061280000000000424,https://web.archive.org/web/20241115024655/https://arxiv.org/html/2411.09134v1,https://web.archive.org/web/20250211075424/https://www.asahi.com/ajw/articles/15593572,,,,35.68882,139.692526
Anonymized Chinese System,Existing,Likely,Yes,19,6000,,,China,,2022-08-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,3,19,10000000000000000000,,10000000000000000000,,,,,,,,,,,Microsoft GPT-4 cluster,0.8,,,,,,35.486703,101.901875
University of Bristol Isambard-AI,Planned,Confirmed,Yes,19.032682893111126,5448.000000080245,NVIDIA GH200,5448,United Kingdom of Great Britain and Northern Ireland,UK Research and Innovation,,,Public,7.6348272,275854310.95051694,"National Composites Centre
Bristol & Bath Science Park, Emersons Green, Bristol BS16 7FS, United Kingdom",U.K. government’s Frontier AI Taskforce,GH200,"When fully installed next year, it will pack 5,448 NVIDIA GH200 Grace Hopper Superchips",Planned Q2 2025,"Phase 1 (~168 GPUs) already confirmed operational, phase 2 to expected around summer 2025",11.848853685384858,,,,,,,5448,NVIDIA,,,,,18.73165289744714,10781592000000000000,10781592000000000000,5390796000000000000,2694036000000000000,7.6348272,,706079634651.0632,249791698.19696295,273000000,275854310.95051694,The UK government says it will cough up £225 million ($273 million),,,,,https://web.archive.org/web/20240526090503/https://blogs.nvidia.com/blog/uk-largest-ai-supercomputer/,https://web.archive.org/web/20240525234841/https://www.hpcwire.com/off-the-wire/uks-isambard-ai-supercomputer-goes-online-ranking-2nd-greenest-globally/,https://www.theregister.com/2023/11/01/uk_isambard_supercomputer/,,,51.453802,-2.597298
Anonymized Chinese System,Existing,Likely,Yes,19,5000,,,China,,2025-02-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,36,19,10000000000000000000,,10000000000000000000,,,,,,,,,,,xAI Colossus Memphis Phase 2,0.03,,,,,,35.486703,101.901875
KDDI Sharp Sakai,Planned,Likely,Yes,19,5053.057099619643,NVIDIA GB200 NVL2,2000,Japan,KDDI,,"Unclear whether ""1,000 units"" means 1k GPUs, 1k chips, or 1k racks of 72 GPUs",Private,4.8048,,"１番地 Takumicho, Sakai Ward, Sakai, Osaka 590-0908, Japan",Cloud,GB200,"It is assumed that NVIDIA shall supply an estimated quantity of 1,000 units of GB200 NVL72",Planned Q1 2026,Very uncertain about GPU number,12.017294689481112,,,,,,,2000,NVIDIA,,,,,18.69897000433602,10000000000000000000,10000000000000000000,5000000000000000000,2500000000000000000,4.8048,,1040626040626.0406,,,,,,,,,https://web.archive.org/web/20240917144105/https://newsroom.kddi.com/english/news/detail/kddi_nr_s-9_3387.html,,,,,34.580808,135.482392
Meta Research SuperCluster (RSC-1) Phase 2,Existing,Confirmed,Yes,18.999304572338346,5044.972208260219,NVIDIA A100,16000,United States of America,Meta AI,2023-05-18,"The location is found from triangulating from the promo video. I'm fairly confident in it, but it isn't guaranteed to be accurate",Private,13.278719999999998,357306992.66499525,"6200 Technology Blvd, 23150 Sandston, Virginia, USA",Meta,A100,"The full system includes 2,000 DGX A100 systems, totaling a staggering 16,000 A100 GPUs",2023-05-18,,11.575118363368931,Meta Research SuperCluster (RSC-1) Phase 1,,,,,,16000,NVIDIA,,True,,4,18.69827457667437,9984000000000000000,9984000000000000000,4992000000000000000,2496000000000000000,13.278719999999998,,375939849624.0602,357306992.66499525,,,,,,Microsoft GPT-4 cluster,0.6399999999999952,https://web.archive.org/web/20231215015022/http://www.hpcwire.com/2023/05/18/meta-completes-research-supercluster-announces-next-gen-datacenter/,https://web.archive.org/web/20240828114058/https://ai.meta.com/blog/ai-rsc/,,,,37.484236,-77.237219
Inflection-2 training cluster,Existing,Likely,Yes,18.995415798542414,5000.000000073624,NVIDIA H100 SXM5 80GB,5000,United States of America,"Inflection AI,CoreWeave",2023-07-30,"Inflection lent 3,500 H100 GPUs to Corweave (who appears to be the party that built that cluster) for the June MLPerf submission. Given that this is just slightly larger and operational just after, it appears very likely that this cluster is an extension of the MLPerf cluster (or the MLPerf cluster was just a subset of this cluster).
It's possible that both of these subsets of the Eos-DFW cluster, but there isn't much evidence for this besides that Eos-DFW was submitted to MLPerf by CoreWeave later that year (I give it a ~50% chance this is Eos-DFW)",Private,7.261799999999999,179900873.3456286,,Inflection,H100,"Inflection-2 was trained on 5,000 NVIDIA H100 GPUs",2023-07-30,,11.833297626692357,NVIDIA Coreweave MLPerf v3.0 Submission 2023,,True,NVIDIA CoreWeave Eos-DFW Phase 1,,,5000,NVIDIA,,,,5,18.69434191036418,9895000000000000000,9895000000000000000,4947000000000000000,2473500000000000000,7.261799999999999,,681236057175.9069,179900873.3456286,,,,,,Microsoft GPT-4 cluster,0.6342948717948694,https://web.archive.org/web/20241203070436/https://inflection.ai/blog/inflection-2,https://archive.ph/fB8Io,,,,39.381266,-97.922211
Sustainable Metal Cloud Singapore Phase 2,Planned,Likely,Unclear,18.995415798542414,5000.000000073624,NVIDIA H100 SXM5 80GB,5000,Singapore,SMC - Sustainable Metal Cloud,,,Private,7.007000000000001,235002457.6537037,Singapore,Cloud,H100,"Their ambitious projection is to have 5,000 GPUs operational in Singapore by the end of the year",Planned H2 2024,,11.848809792870606,Sustainable Metal Cloud Singapore Phase 1,,,,,,5000,NVIDIA,,,,,18.69434191036418,9895000000000000000,9895000000000000000,4947000000000000000,2473500000000000000,7.007000000000001,,706008277436.8488,235002457.6537037,,,,,,,,https://web.archive.org/web/20250116210511/https://itbrief.asia/story/firmus-sustainable-metal-cloud-powerfully-cuts-ai-energy-usage,,,,,1.351616,103.808053
Anonymized Chinese System,Existing,Confirmed,Yes,18.954242509439325,5000,,40000,China,,2024-06-15,,Public/Private,,,China,,,,,,,,,,,,,40000,Anonymized,Anonymized,True,,20,18.954242509439325,8999999999999999000,,8999999999999999000,,,,,,,,,,,Meta GenAI 2024a,0.2,,,,,,35.486703,101.901875
NVIDIA Eos Phase 2,Existing,Confirmed,Yes,18.95995826462155,4608.000000067861,NVIDIA H100 SXM5 80GB,4608,United States of America,NVIDIA,2023-11-01,There's also a 10.7k cluster with the same name,Private,6.69247488,152516450.7326823,United States of America,NVIDIA,H100,"Its “Eos” supercomputer is expected to be the world’s fastest AI system after it begins operations later this year, featuring a total of 576 DGX H100 systems with 4,608 DGX H100 GPUs.",2023-11-01,Stated in NVIDIA blog post,11.833297626692357,NVIDIA Eos Phase 1,,,,,,4608,NVIDIA,,True,,8,18.658884376443318,9119232000000000000,9119232000000000000,4559155200000000000,2279577600000000000,6.69247488,,681236057175.9069,152516450.7326823,,,,,,Tesla 10k H100 Cluster,0.46080000000000093,https://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,,,39.381266,-97.922211
Lawrence Livermore NL Tuolumne,Existing,Confirmed,Yes,18.956034354989296,4566.553613003071,AMD Instinct MI300A,4608,United States of America,US Department of Energy,2024-11-18,,Public,5.6612,188962954.36889708,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","Lawrence Livermore NL,Scientific Research",AMD MI300A,Calculated from Top500,2024-11-18,,11.90209586132783,,,,,,,4608,AMD,,True,,35,18.655004359325314,9037209600000000000,9037209600000000000,4518604800000000000,2259302400000000000,7.1386398720000015,5.6612,798170847170.2113,188962954.36889708,,,,,,xAI Colossus Memphis Phase 1,0.045665536129358676,https://web.archive.org/web/20241109063437/https://www.top500.org/system/180282/,,,,,37.682018,-121.768374
Google Oklahoma TPU v4 Pods,Existing,Confirmed,Yes,18.954782628789978,4553.410813609226,Google TPU v4,32768,United States of America,Google,2022-05-12,"8 pods x (4096 chips/pod) = 32,768 chips",Private,11.861950464,299820914.1164615,"4581 Webb St, Pryor, OK 74361",Google,TPUv4,"Google’s fourth iteration of its Tensor Processing Units launched at last year’s I/O and a single TPU pod consists of 4,096 of these chips. Each chip has a peak performance of 275 teraflops and each pod promises a combined compute power of up to 1.1 exaflops of compute power. Google now operates a full cluster of eight of these pods in its Oklahoma data center with up to 9 exaflops of peak aggregate performance",2022-05-12,,11.88062652272075,,,,,,,32768,Google,,True,,2,18.954782628789978,9011200000000000000,9011200000000000000,9011200000000000000,,11.861950464,,759672705374.0629,299820914.1164615,,,,,,Microsoft GPT-4 cluster,0.5776410256410217,https://web.archive.org/web/20240516184309/https://techcrunch.com/2022/05/11/google-launches-a-9-exaflop-cluster-of-cloud-tpu-v4-pods-into-public-preview/,,,,,36.241172,-95.330047
Anonymized Chinese System,Existing,Likely,Unclear,18.954242509439325,4000,,30000.000000000004,China,,2024-11-15,,Private,20,600000000,China,,,,,,11.30102999566398,,,,,,,30000.000000000004,Anonymized,Anonymized,,,,18.602059991327963,8999999999999999000,8999999999999999000,3999999999999999500,1999999999999999700,20,,199999999999.99997,600000000,,,,,,,,,,,,,35.486703,101.901875
Mare Nostrum 5,Existing,Confirmed,Yes,18.94772380820454,4480.000000065971,NVIDIA H100 SXM5 80GB,4480,Spain,"Barcelona Supercomputing Center,EuroHPC JU",2023-11-01,,Public,6,162157070.19671246,"Barcelona Supercomputing Center
Plaça d'Eusebi Güell, 1-3, Les Corts, 08034 Barcelona, Spain",European science and industry communities,H100,1120 nodes with 4 Hopper GPUs each,2023-11-01,Specs on their official website,11.868498669642662,,,,,,,4480,NVIDIA,,True,,10,18.646649920026306,8865920000000000000,8865920000000000000,4432512000000000000,2216256000000000000,6.5065728,6,738752000000,148279882.65677443,159400000,162157070.19671246,"The budget is set for €151.41 million, which is $159.4 million with current US dollar exchange rates to the euro.",,,Tesla 10k H100 Cluster,0.44800000000000045,https://web.archive.org/web/20240710184832/https://www.bsc.es/es/marenostrum/marenostrum-5,https://web.archive.org/web/20240421214717/https://www.hpcwire.com/off-the-wire/bsc-inaugurates-marenostrum-5-marking-a-historic-investment-in-spanish-and-european-science/,https://www.nextplatform.com/2022/06/16/atos-wins-marenostrum-5-deal-at-barcelona-supercomputing-center/,,,41.389806,2.115513
Together AI H100 Cluster,Existing,Likely,Yes,18.942260912503038,4424.000000065149,NVIDIA H100 SXM5 80GB,4424,United States of America,Together,2023-10-16,Unclear if all the GPUs are networked together,Private,6.425240639999999,146721675.82831895,,Cloud,H100,Our first large H100 cluster (4424 GPUs) starts coming online today!,2023-10-16,,11.833297626692357,,,,,,,4424,NVIDIA,,True,,9,18.641187024324804,8755096000000000000,8755096000000000000,4377105600000000000,2188552800000000000,6.425240639999999,,681236057175.9069,146721675.82831895,,,,,,Tesla 10k H100 Cluster,0.4424000000000007,https://archive.ph/58l2A#selection-489.0-489.68,,,,,39.381266,-97.922211
EuroHPC Leonardo,Existing,Confirmed,Yes,18.93581831481724,4358.855987936838,NVIDIA A100,13824,Italy,EuroHPC JU,2022-11-22,,Public,12.5985,280971647.2256874,"Via Piero Gobetti, 101, 40129 Bologna BO, Italy","European researchers,academia,and industry",A100,"This result is achieved with 3456 computing nodes, each equipped with four NVIDIA  A100 SXM6 64GB  GPUs driven by a single 32-cores Intel Ice Lake CPU.",2022-11-22,,11.53446947883746,,,,,,,13824,NVIDIA,,True,,5,18.63478831915326,8626176000000000000,8626176000000000000,4313088000000000000,2156544000000000000,11.774730239999998,12.5985,342349327300.86914,311419540.35140723,272208000,280971647.2256874,Cost 240M Euros in 2022,,,Microsoft GPT-4 cluster,0.5529599999999969,https://web.archive.org/web/20240830104624/https://www.theregister.com/2022/11/22/leonardo_supercomputer_goes_live/,https://web.archive.org/web/20230720164245/https://en.wikipedia.org/wiki/Leonardo_(supercomputer),https://web.archive.org/web/20250115051101/https://leonardo-supercomputer.cineca.eu/hpc-system/,,,44.522194,11.338427
Amazon Titan training cluster,Existing,Likely,Yes,18.933803023581916,4338.676099103813,NVIDIA A100,13760,United States of America,Amazon,2023-01-15,"Model was released to large clients April 2023, took 48 days to train, so this cluster was operational at least three months beforehand",Private,11.419699199999998,309575729.94726515,,Amazon,A100,"Specifically, they trained a 200B dense model on 4T tokens of data across 13,760 NVIDIA A100 chips (using 1,720 P4d nodes)",2023-01,,11.575118363368931,,,,,,,13760,NVIDIA,,True,,6,18.632773027917935,8586240000000000000,8586240000000000000,4293120000000000000,2146560000000000000,11.419699199999998,,375939849624.0602,309575729.94726515,,,,,,Microsoft GPT-4 cluster,0.550399999999999,https://web.archive.org/web/20240824020111/https://lifearchitect.ai/titan/,,,,,39.381266,-97.922211
Google Hypercomputer TPU v5p pod,Existing,Confirmed,Yes,18.915150690863367,4156.28095003594,Google TPU v5p,8960,,Google,2023-12-06,"The ""Hypercomputer"" is a cloud network, a TPU v5p pod seems to just be one component. Versions of this pod likely exist in multiple locations, possibly around the world",Private,,57598146.17527369,,"Google,Cloud",TPUv5p,"Each TPU v5p pod composes together 8,960 chips over our highest-bandwidth inter-chip interconnect (ICI) at 4,800 Gbps/chip in a 3D torus topology.",2023-12-06,,,,,,,,,8960,Google,,True,,18,18.614120695199386,8225280000000000000,8225280000000000000,4112640000000000000,,,,,57598146.17527369,,,,,,Microsoft Azure Eagle,0.28863062152602326,https://web.archive.org/web/20240917131707/https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer,https://web.archive.org/web/20240528131246/https://www.hpcwire.com/2023/12/28/google-addresses-the-mysteries-of-its-hypercomputer/,,,,,
Sesterce H100s Phase 2,Existing,Likely,Yes,18.90880574217417,4096.0000000603195,NVIDIA H100 SXM5 80GB,4096,France,Sesterce,2024-07-06,,Private,5.844500480000001,145966387.42898926,"Paris, France",Cloud,H100,From a private correspondence,2024-07-06,,11.840984455358647,Sesterce H100s Phase 1,,,,,,4096,NVIDIA,,True,,29,18.607731853995933,8105984000000000000,8105984000000000000,4052582400000000000,2026291200000000000,5.844500480000001,,693400986768.3336,145966387.42898926,,,,,,Meta GenAI 2024a,0.16666666666666657,,,,,,48.853495,2.348392
Sesterce Synapse Phase 2,Existing,Likely,Yes,18.90880574217417,4096.0000000603195,NVIDIA H200 SXM,4096,France,Sesterce,2025-01-15,,Private,5.7401344000000005,144832469.2992,"Paris, France",Cloud,H200,Private Correspondence,2025-01-15,,11.848853685384858,Sesterce Synapse Phase 1,,,,,,4096,NVIDIA,,True,,44,18.60777574651019,8105984000000000000,8105984000000000000,4052992000000000000,2025472000000000000,5.7401344000000005,,706079634651.0632,144832469.2992,,,,,,xAI Colossus Memphis Phase 1,0.0409600000000004,https://web.archive.org/web/20240823055517/https://www.sesterce.com/reserved-cloud,https://archive.ph/ZDjLF,,,,48.853495,2.348392
Yotta Shakti Cloud NM1 Phase 1,Existing,Confirmed,Yes,18.90880574217417,4096.0000000603195,NVIDIA H100 SXM5 80GB,4096,India,Yotta Data Services,2024-05-15,There is evidence on GPU listings of this portion being operational. The date comes from an article a month earlier saying when they expect to officially go live,Private,5.844500480000001,145703820.78798905,"Yotta Datacenter Park - Panvel Hiranandani Fortune City. Survey No. 30, MH SH 76, Panvel, Navi Mumbai, Maharashtra 410206, India",Cloud,H100,"""Yotta’s first slot of more than 4,000 GPUs was already booked by enterprises and is expected to go live by May 15, 2024""
""Yotta will deploy the first cluster of 16,384 GPUs at NM1""",2024-05-15,,11.840984455358647,,,,,,,4096,NVIDIA,,True,,25,18.607731853995933,8105984000000000000,8105984000000000000,4052582400000000000,2026291200000000000,5.844500480000001,,693400986768.3336,145703820.78798905,,,,,,Meta GenAI 2024a,0.16666666666666657,https://web.archive.org/web/20240527112515/https://shakticloud.ai/,https://web.archive.org/web/20240423043934/https://yotta.com/media/press-release-yotta-receives-indias-first-ever-consignment-of-nvidia-h100-the-fastest-gpus-in-the-world-at-its-nm1-data-center-park/,https://web.archive.org/web/20240521205830/https://yotta.com/media/press-release-yotta-data-services-collaborates-with-nvidia-to-catalyze-indias-ai-transformation/,https://web.archive.org/web/20240407130017/https://www.business-standard.com/companies/news/data-centre-operator-yotta-looks-to-expand-its-gpu-capacity-march-2025-124040700389_1.html,,19.030146,73.02022
PanaAI AUS AISF,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H200 SXM,4088,Australia,PanaAI,,,Private,5.7289232000000005,146060910.18584076,Australia,Cloud,H200,It will host up to 4088 NVIDIA H200 Tensor Core GPUs,Planned Q1 2025,,11.848853685384858,,,,,,,4088,NVIDIA,,,,,18.60692668566907,8090152000000000000,8090152000000000000,4045076000000000000,2021516000000000000,5.7289232000000005,,706079634651.0632,146060910.18584076,,,,,,,,https://web.archive.org/web/20250116143118/https://www.technologydecisions.com.au/content/it-management/news/panaai-to-build-australia-s-fastest-ai-supercomputer-731317630,,,,,-25.734968,134.489563
Voltage Park Location 5,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232000000005,192138009.3776681,,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.848809792870606,,,,,,,4088,NVIDIA,,,,,18.606882793154817,8090152000000000000,8090152000000000000,4044667200000000000,2022333600000000000,5.7289232000000005,,706008277436.8489,192138009.3776681,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,,39.381266,-97.922211
Voltage Park Location 6,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232000000005,192138009.3776681,,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.848809792870606,,,,,,,4088,NVIDIA,,,,,18.606882793154817,8090152000000000000,8090152000000000000,4044667200000000000,2022333600000000000,5.7289232000000005,,706008277436.8489,192138009.3776681,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,,39.381266,-97.922211
Voltage Park Texas Phase 2,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232000000005,192138009.3776681,Texas,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.848809792870606,Voltage Park Texas Phase 1,,,,,,4088,NVIDIA,,,,,18.606882793154817,8090152000000000000,8090152000000000000,4044667200000000000,2022333600000000000,5.7289232000000005,,706008277436.8489,192138009.3776681,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,,31.803973,-98.822319
Voltage Park Utah,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232000000005,192138009.3776681,Utah,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.848809792870606,,,,,,,4088,NVIDIA,,,,,18.606882793154817,8090152000000000000,8090152000000000000,4044667200000000000,2022333600000000000,5.7289232000000005,,706008277436.8489,192138009.3776681,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,,39.254817,-111.6025
Voltage Park Virginia,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232000000005,192138009.3776681,Virginia,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.848809792870606,,,,,,,4088,NVIDIA,,,,,18.606882793154817,8090152000000000000,8090152000000000000,4044667200000000000,2022333600000000000,5.7289232000000005,,706008277436.8489,192138009.3776681,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,,37.677592,-78.619053
Voltage Park Washington,Planned,Likely,Yes,18.90795668133305,4088.000000060189,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232000000005,192138009.3776681,"1023 39th Ave SE, Puyallup, WA 98374",Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.848809792870606,,,,,,,4088,NVIDIA,,,,,18.606882793154817,8090152000000000000,8090152000000000000,4044667200000000000,2022333600000000000,5.7289232000000005,,706008277436.8489,192138009.3776681,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,https://web.archive.org/web/20240226140823/https://choosetacomapierce.org/news-events/voltage-park-selects-centeris-data-center/,,,47.161024,-122.278602
Anonymized Chinese System,Existing,Confirmed,Yes,18.90308998699194,4000,,,China,,2024-05-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,26,18.90308998699194,7999999999999999000,,7999999999999999000,,,,,,,,,,,Meta GenAI 2024a,0.2,,,,,,35.486703,101.901875
CoreWeave LiquidLab,Existing,Likely,Yes,18.898505785534358,4000.0000000589002,NVIDIA H100 SXM5 80GB,4000,United States of America,CoreWeave,,Unclear exactly what type of GPU they use. Likely operational by now,Private,5.6056,188001966.12296295,,CoreWeave,H100,"CoreWeave has a Liquid Lab where we conduct extensive liquid cooling testing, and we’re getting ready to deploy our first “small” liquid deployment of 4,000 GPUs",Planned,,11.848809792870606,,,,,,,4000,NVIDIA,,True,,,18.597431897356124,7916000000000000000,7916000000000000000,3957600000000000000,1978800000000000000,5.6056,,706008277436.8489,188001966.12296295,,,,,,,,https://web.archive.org/web/20241006003335/https://www.coreweave.com/blog/building-ai-clusters-for-enterprises-2025,,,,,39.381266,-97.922211
Microsoft Azure ND H100 v5 VM,Existing,Confirmed,Yes,18.898505785534358,4000.0000000589002,NVIDIA H100 SXM5 80GB,4000,United States of America,Microsoft,2023-03-13,Planned to scale up,Private,5.8094399999999995,190859527.04343322,,"Cloud,Azure",H100,"Today marks the general availability of our Azure ND H100 v5 Virtual Machine (VM) series, featuring the latest NVIDIA H100 Tensor Core GPUs and NVIDIA Quantum-2 InfiniBand networking... we are leveraging an AI optimized 4K GPU cluster and will be ramping to hundreds of thousands of the latest GPUs in the next year.",2023-03-13,,11.833297626692357,,,,,,,4000,NVIDIA,,True,,7,18.597431897356124,7916000000000000000,7916000000000000000,3957600000000000000,1978800000000000000,5.8094399999999995,,681236057175.9069,190859527.04343322,,,,,,Microsoft GPT-4 cluster,0.5074358974358958,https://web.archive.org/web/20240817024910/https://azure.microsoft.com/en-us/blog/scale-generative-ai-with-new-azure-ai-infrastructure-advancements-and-availability/,https://web.archive.org/web/20240819113927/https://azure.microsoft.com/en-us/blog/azure-previews-powerful-and-scalable-virtual-machine-series-to-accelerate-generative-ai/,,,,39.381266,-97.922211
Mistral Large Model Implied Cluster,Existing,Unlikely,Yes,18.898505785534358,4000.0000000589002,NVIDIA H100 SXM5 80GB,4000,,,2023-11-26,"Taking H100 price as $2/hr, and assuming they trained for 3 months max, this implies that the cluster they used was >4k GPUs",Private,5.8094399999999995,132084652.72911642,,"Cloud,Mistral",,"""Mensch said his new model costs less than 20 million [Euros] to train""",2023-11-26,GPU count implied from stated training cost,11.833297626692357,,,,,,,4000,NVIDIA,,,,,18.597431897356124,7916000000000000000,7916000000000000000,3957600000000000000,1978800000000000000,5.8094399999999995,,681236057175.9069,132084652.72911642,,,,,,,,https://archive.ph/aAkAp,,,,,,
Nebius Kansas City Phase 1,Existing,Likely,Yes,18.898505785534358,4000.0000000589002,NVIDIA H200 SXM,4000,United States of America,Nebius AI,2025-04-15,"Current power is 5MW our of potential expansion to 40MW (which would host 35k GPUs), so extrapolating, that would put it at ~4k GPUs
They claimed that it was on track to go live in Q1 2025 in March 2025, implying that it likely came online soon after March",Private,5,142916741.8648148,"Kansas City, USA",Cloud,H200,"will house thousands of state-of-the-art NVIDIA GPUs, primarily NVIDIA Hopper GPUs in the initial phase, with the energy-efficient NVIDIA Blackwell platform expected to arrive in 2025. The colocation can be expanded from an initial 5 MW up to 40 MW, or about 35 thousand GPUs, at full potential capacity",2025-04,,11.898505785534358,,,,,,,4000,NVIDIA,,True,,52,18.597475789870373,7916000000000000000,7916000000000000000,3958000000000000000,1978000000000000000,5.6056,5,791600000000,142916741.8648148,,,,,,xAI Colossus Memphis Phase 2,0.020000000000000004,https://web.archive.org/web/20250109005023/https://group.nebius.com/newsroom/nebius-expands-in-us-with-first-gpu-cluster-in-kansas-city-offices-in-san-francisco-dallas-and-new-york,,,,,39.098485,-94.578631
OneAsia OBON Clusters,Planned,Likely,Yes,18.898505785534358,4000.0000000589002,NVIDIA H100 SXM5 80GB,4000,Thailand,OneAsia,,,Private,5.6056,188001966.12296295,"Bangkok, Thailand","Cloud,OBON,SIAM AI",H100,OBON will deploy multiple 4K supercomputing clusters at OneAsia's state-of-the-art data centre,Planned,,11.848809792870606,,,,,,,4000,NVIDIA,,,,,18.597431897356124,7916000000000000000,7916000000000000000,3957600000000000000,1978800000000000000,5.6056,,706008277436.8489,188001966.12296295,,,,,,,,https://web.archive.org/web/20241211224237/https://www.prnewswire.com/apac/news-releases/oneasia-and-obon-unveil-thailands-first-4k-supercomputing-clusters-revolutionizing-ai-roadmap-in-thailand-302135527.html?tc=eml_cleartime,,,,,13.752494,100.493509
Sweden 4k H100 Cluster,Existing,Likely,Yes,18.898505785534358,4000.0000000589002,NVIDIA H100 SXM5 80GB,4000,Sweden,,2025-01-14,"Operational by 1/14/25 at the latest, but likely meaningfully earlier",Private,5.6056,150626230.5,Sweden,Cloud,H100,From a private correspondence,2025-01-14,From a private correspondence about GPU cluster availability,11.848809792870606,,,,,,,4000,NVIDIA,,True,,46,18.597431897356124,7916000000000000000,7916000000000000000,3957600000000000000,1978800000000000000,5.6056,,706008277436.8489,150626230.5,,,,,,xAI Colossus Memphis Phase 1,0.04000000000000034,,,,,,64.964875,17.675409
Telangana Yotta H1 Hyderbad AI City Cluster Phase 1,Planned,Likely,Yes,18.898505785534358,4000.0000000589002,NVIDIA H100 SXM5 80GB,4000,India,Government of Telangana,,,Public,5.6056,188001966.12296295,"Hyderabad, India",Cloud,H100,"The first phase of the Supercomputer consisting of 4,000 high performance GPUs and the first Data Center within the campus, Yotta H1 will be operational within the next 24 months",Planned 2026,,11.848809792870606,,,,,,,4000,NVIDIA,,,,,18.597431897356124,7916000000000000000,7916000000000000000,3957600000000000000,1978800000000000000,5.6056,,706008277436.8489,188001966.12296295,,,,,,,,https://web.archive.org/web/20240906230223/https://www.crn.in/news/government-of-telangana-partners-with-yotta-to-launch-indias-largest-ai-supercomputer-of-25000-high-performance-gpus-in-a-purpose-built-50-mw-ai-cloud-data-centre-campus-in-hyderabad/,,,,,17.361362,78.474525
Gemini 1.0 Ultra training cluster A,Existing,Likely,Yes,18.896790681812295,3984.234461908105,Google TPU v4,28672,United States of America,Google,2023-08-20,Believed to be trained on two sets of 7 pods of 4096 TPUs,Private,10.113073152,259857295.64948127,,Google,TPUv4,"Gemini Ultra, the most compute-intensive model known to date, was trained across multiple data centers totaling 55,000 TPU v4s",2023-08-20,,11.89190753313044,,,,,,,28672,Google,,True,,10,18.896790681812295,7884800000000000000,7884800000000000000,7884800000000000000,,10.113073152,,779664092357.5907,259857295.64948127,,,,,,Microsoft GPT-4 cluster,0.5054358974358981,https://arxiv.org/abs/2312.11805,https://web.archive.org/web/20250320051300/https://ifp.org/web/20250320051300/https://ifp.org/future-of-ai-compute/,,,,39.381266,-97.922211
Gemini 1.0 Ultra training cluster B,Existing,Likely,Yes,18.896790681812295,3984.234461908105,Google TPU v4,28672,United States of America,Google,2023-08-20,Believed to be trained on two sets of 7 pods of 4096 TPUs,Private,10.113073152,259857295.64948127,,Google,TPUv4,"Gemini Ultra, the most compute-intensive model known to date, was trained across multiple data centers totaling 55,000 TPU v4s",2023-08-20,,11.89190753313044,,,True,Google Oklahoma TPU v4 Pods,,,28672,Google,,,,10,18.896790681812295,7884800000000000000,7884800000000000000,7884800000000000000,,10.113073152,,779664092357.5907,259857295.64948127,,,,,,Microsoft GPT-4 cluster,0.5054358974358981,https://arxiv.org/abs/2312.11805,https://web.archive.org/web/20250320051300/https://ifp.org/web/20250320051300/https://ifp.org/future-of-ai-compute/,,,,39.381266,-97.922211
Vultr Chicago Cluster,Existing,Likely,Yes,18.89457633978582,3963.971702938611,AMD Instinct MI300X,3000,United States of America,Vultr,2024-12-11,"Not totally certain that the ""thousands"" of MI300X's are all in this data center, but I'm pretty sure. I'm also estimating ""thousands"" as 3k",Private,4.5864,81699126.74275017,"4513 Western Ave, Lisle, IL 60532",Cloud,MI300X,"With thousands of MI300X GPUs available, clusters of any size can be deployed",2024-12-11,Uncertain about exact chip count,11.932057806582943,,,,,,,3000,AMD,,True,,49,18.59352973534956,7844700000000000000,7844700000000000000,3922200000000000000,1961100000000000000,4.5864,,855180533751.9623,81699126.74275017,,,,,,xAI Colossus Memphis Phase 1,0.03963971702880275,https://web.archive.org/web/20241222233226/https://blogs.vultr.com/Lisle-data-center,,,,,41.802886,-88.094986
Anonymized Chinese System,Planned,Likely,Yes,18.90308998699194,4000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.90308998699194,7999999999999999000,,7999999999999999000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,Yes,18.90308998699194,4000,,10000,China,,2023-09-15,,Private,10,300000000,China,,,,,,11.60205999132796,,,,,,,10000,Anonymized,Anonymized,True,,14,18.602059991327963,7999999999999999000,7999999999999999000,3999999999999999500,1999999999999999700,10,,399999999999.99994,300000000,,,,,,Tesla 10k H100 Cluster,0.4,,,,,,35.486703,101.901875
NFDG Andromeda Phase 2,Existing,Confirmed,Yes,18.88462319746458,3874.1586660499647,NVIDIA H100 SXM5 80GB,3632,United States of America,Nat Friedman and Daniel Gross,2024-07-15,,Private,5.808624640000001,146020983.3802864,"Ashburn, Virginia",Startups they invest in,"H100,A100","3,200 H100s on 400 nodes interlinked with 3.2Tbps infiniband
432 H100s on 54 nodes interlinked with 3.2Tbps infiniband
768 A100s for training and inference with 1.6Tbps infiniband",2024-07-15,Listed as available on their official website,11.819478740193112,NFDG Andromeda Phase 1,,,,NVIDIA A100 SXM4 40 GB,768,4400,NVIDIA,NVIDIA,True,,36,18.583552052967537,7666960000000000000,7666960000000000000,3833116800000000000,1916558400000000000,5.808624640000001,,659900929663.1017,146020983.3802864,,,,,,Meta GenAI 2024a,0.1576399196774459,https://web.archive.org/web/20240814194924/https://andromeda.ai/,https://web.archive.org/web/20240706081837/https://www.businessinsider.com/nvidia-gpu-venture-capitalists-buying-for-startups-2023-6,,,,39.043555,-77.487449
NVIDIA Coreweave MLPerf v3.0 Submission 2023,Existing,Confirmed,Yes,18.850813795196483,3584.000000052779,NVIDIA H100 SXM5 80GB,3584,United States of America,"NVIDIA,CoreWeave,Inflection AI",2023-06-01,"Inflection lent 3,500 H100 GPUs to Corweave (who appears to be the party that built that cluster) for the June MLPerf submission. Given that this is just slightly larger and operational just after, it appears very likely that this cluster is an extension of the MLPerf cluster (or the MLPerf cluster was just a subset of this cluster).
It's possible that both of these subsets of the Eos-DFW cluster, but there isn't evidence for this besides that Eos-DFW was submitted to MLPerf later that year",Private,5.205258239999999,129400759.97831686,,NVIDIA,H100,"The 10,752 H100 GPUs far surpassed the scaling in AI training in June, when NVIDIA used 3,584 Hopper GPUs.",2023-06-01,,11.833297626692357,,Inflection-2 training cluster,True,NVIDIA CoreWeave Eos-DFW Phase 1,,,3584,NVIDIA,,,,9,18.54973990701825,7092736000000000000,7092736000000000000,3546009600000000000,1773004800000000000,5.205258239999999,,681236057175.9069,129400759.97831686,,,,,,Microsoft GPT-4 cluster,0.4546625641025631,https://web.archive.org/web/20240915170526/https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/,https://archive.ph/fB8Io,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,18.845098040014257,4000,,,China,,2024-08-15,,Public/Private,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,True,,41,18.845098040014257,7000000000000000000,,7000000000000000000,,,,,,,,,,,CoreWeave H200s,0.08,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,No,18.845098040014257,3000,,,China,,2024-06-15,,Public/Private,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,,,,18.845098040014257,7000000000000000000,,7000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.77815125038364,3000,,10000,China,,2021-07-15,,Private,6,200000000,China,,,,,,11.698970004336019,,,,,,,10000,Anonymized,Anonymized,True,,1,18.477121254719663,6000000000000000000,6000000000000000000,3000000000000000000,1999999999999999700,6,,500000000000,300000000,100000000,200000000,,,,DeepSeek Fire-Flyer 2,1,,,,,,35.486703,101.901875
XTX Markets Cluster,Existing,Likely,Yes,18.79518458968242,3153.1076301626354,NVIDIA A100,10000,,XTX Markets,2023-03-27,"Unclear how many of the GPUs are A100s vs V100s
The State of AI report indicates that they have 10k A100s, but it's unclear if they're just estimating this or have an actual source
Also, the earliest point in the waybackmaching that the State of AI report mentions this cluster is March 2023, which is where the date comes from",Private,14.523599999999998,409025143.8191224,,XTX Markets,"A100,V100,20,000 GPUs Maybe 10k A100, 10k V100?",Our dedicated research cluster contains a hundred thousand cores and twenty thousand A/V100 GPUs,2023-03-27,"Unclear how many of the GPUs are A100s vs V100s
The State of AI report indicates that they have 10k A100s, but it's unclear if they're just estimating this or have an actual source",11.478407157634617,,,,,NVIDIA V100,10000,20000,NVIDIA,NVIDIA,True,,8,18.64048143697042,6240000000000000000,6240000000000000000,4370000000000000000,1716700000000000000,14.523599999999998,,300889586603.8724,409025143.8191224,,,,,,Microsoft GPT-4 cluster,0.39999999999999686,https://web.archive.org/web/20240222140115/https://www.xtxmarkets.com/career/xty-labs-ai-residency/,https://web.archive.org/web/20230331093712/https://www.stateof.ai/compute,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.77815125038364,3000,,,China,,,,Public,30,,China,,,,,,11.30102999566398,,,,,,,10000,Anonymized,Anonymized,True,,,18.77815125038364,6000000000000000000,,6000000000000000000,,,30,200000000000,,,,,,,,,,,,,,35.486703,101.901875
Novo Nordisk Gefion,Existing,Confirmed,Yes,18.77815125038364,3031.8342597717624,NVIDIA H100 SXM5 80GB,1528,Denmark,"Novo Nordisk Foundation,Danish Centre for AI Innovation,Export and Investment Fund of Denmark",2024-10-23,The Export and Investment Fund of Denmark (which is government-affiliated) owns 15% of the company created by Novo Nordisk that owns this supercomputer,Private,2.9307,99155664.2489824,Denmark,Public and private research; cloud,H100,"The supercomputer will feature 191 DGX H100 systems with a total of 1,528 Nvidia H100 Tensor Core GPUs and 382 Intel Xeon Platinum CPUs connected by Quantum 2 InfiniBand",2024-10-23,,11.71252389594499,,,,,,,1528,NVIDIA,,True,,49,18.17949526026783,6000000000000000000,6000000000000000000,1511803200000000000,755901600000000000,2.18027264,2.9307,515850547650.73193,58217750.86250277,98000000,99155664.2489824,"In terms of price, Novo Nordisk Foundation is putting up 600 million Danish krone (about 14 cents to the US dollar) and the Export and Investment Fund of Denmark (EIFO) us putting up another 100 million Danish krone.",,,xAI Colossus Memphis Phase 1,0.030318342597271442,https://web.archive.org/web/20240531144416/https://www.datacenterdynamics.com/en/news/novo-nordisk-foundation-to-build-danish-nvidia-dgx-superpod-supercomputer/,https://web.archive.org/web/20250201135159/https://novonordiskfonden.dk/en/news/denmarks-first-ai-supercomputer-is-now-operational/,https://www.nextplatform.com/2024/10/23/the-great-danes-get-a-supercomputer-for-ai-and-maybe-hpc/,,,55.963398,10.046297
Anonymized Chinese System,Existing,Likely,Yes,18.77815125038364,3000,,100000.00000000001,China,,2021-03-15,,Public,,,China,,,,,,,,,,,,,100000.00000000001,Anonymized,Anonymized,True,,1,18.77815125038364,6000000000000000000,,6000000000000000000,3000000000000000000,,,,,,,,,,Sunway OceanLight,1,,,,,,35.486703,101.901875
FPT AI Factory Japan,Planned,Likely,Yes,18.773567048926058,3000.0000000441755,NVIDIA H100 SXM5 80GB,3000,Japan,FPT Corporation,,"Same investment size as the Vietnam cluster, which said it would have ""thousands"" of GPUs. Estimating ""thousands"" as 3k here. They were supposed to receive their first shipment of 1k H100s in December 2024. The facility is said to be $200 million, so assuming an H100 price of ~30k, this means there would be a max of 6k H100s here",Private,4.2042,141001474.59222218,Japan,Cloud,"H100,H200","The center, like a similar $200 million Vietnam facility announced this year, will run on Nvidia Corp",Planned Q1 2025,,11.848809792870606,,,,,NVIDIA H200 SXM,,3000,NVIDIA,NVIDIA,,,,18.472493160747824,5937000000000000000,5937000000000000000,2968200000000000000,1484100000000000000,4.2042,,706008277436.8489,141001474.59222218,,,,,,,,https://archive.ph/pqult,,,,,36.386493,138.59223
FPT AI Factory Vietnam,Planned,Likely,Yes,18.773567048926058,3000.0000000441755,NVIDIA H100 SXM5 80GB,3000,Vietnam,FPT Corporation,,"Estimating ""thousands"" as 3k here. They were supposed to receive their first shipment of 1k H100s in December 2024. The facility is said to be $200 million, so assuming an H100 price of ~30k, this means there would be a max of 6k H100s here",Private,4.2042,141001474.59222218,Vietnam,Cloud,"H100,H200","FPT's AI Factory, the first of its kind in Vietnam, is equipped with thousands of NVIDIA GPU H100 graphics chips and will be ready to provide services starting January 2025",Planned Q1 2025,,11.848809792870606,,,,,NVIDIA H200 SXM,,3000,NVIDIA,NVIDIA,,,,18.472493160747824,5937000000000000000,5937000000000000000,2968200000000000000,1484100000000000000,4.2042,,706008277436.8489,141001474.59222218,,,,,,,,https://web.archive.org/web/20250116004357/https://fpt.com/en/news/fpt-news/can-canh-sieu-chip-nvidia-tai-nha-may-ai-cua-fpt-tai-viet-nam,,,,,14.315424,108.339537
Paper on Gemma 2 27B,Existing,Confirmed,Yes,18.751293888224698,2850.0212228817895,Google TPU v5p,6144,,Google,2024-06-27,'Table 3 | Training infrastructure with sharding' in 27B row lists 6144 in #Chips column,Private,,39221061.61351344,,Google,TPUv5p,"we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips",2024-06-27,,,,,True,Google Hypercomputer TPU v5p pod,,,6144,Google,,,,38,18.450263892560713,5640192000000000000,5640192000000000000,2820096000000000000,,,,,39221061.61351344,,,,,,Meta GenAI 2024a,0.1159676604345629,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,,,
KAUST Shaheen-III,Planned,Likely,Yes,18.746078444676467,2816.000000041447,NVIDIA GH200,2816,Saudi Arabia,King Abdullah University of Science and Technology (KAUST),,"The CPU portion became operational in November 2023. It's unclear if the GPU portion is already operational, is still under construction, or got cancelled",Public,5.3,129114064.26627162,"King Abdullah University Of Science And Technology 23955, Saudi Arabia",,GH200,"Across these seven accelerated cabinets: 704 nodes, each with quadruple Nvidia “Grace Hopper” Superchips, each with a tightly coupled CPU and GPU. These 2,816 Superchips

The compute-only partition, spread across 18 liquid-cooled HPE Cray EX4000 cabinets and networked with HPE Slingshot, will comprise 4,608 nodes, each with two of AMD’s fourth-gen Epyc “Genoa” CPUs",Planned,,11.7207725794117,,,,,,,2816,NVIDIA,,,,,18.44504844901249,5572864000000000000,5572864000000000000,2786432000000000000,1392512000000000000,3.9463424000000003,5.3,525741886792.4528,129114064.26627162,,,,,,,,https://web.archive.org/web/20240707221559/https://www.kaust.edu.sa/en/news/kaust-s-shaheen-iii-confirmed-as-the-middle-east-s-most-powerful-supercomputer,https://web.archive.org/web/20221128092556/https://www.hpcwire.com/2022/09/27/hpe-to-build-100-petaflops-shaheen-iii-supercomputer/,,,,24.758507,46.735178
Eni HPC6,Existing,Confirmed,Yes,18.725838481801038,2687.7736230815135,AMD Radeon Instinct MI250X,13888,Italy,Eni,2024-11-18,,Private,14.1546496,349498499.42604226,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,AMD MI250X,"the system includes 3472 computing nodes housing a total of 13,888 GPU",2024-11-18,,11.574939358977346,,,,,,,13888,AMD,,True,,57,18.725838481801038,5319104000000000000,5319104000000000000,5319104000000000000,1329081600000000000,14.1546496,,375784929356.3579,349498499.42604226,,,,,,xAI Colossus Memphis Phase 1,0.026877736230419585,https://archive.ph/RIheG,,,,,45.101078,8.859647
Anonymized Chinese System,Existing,Likely,Yes,18.69897000433602,3000,,10000,China,,2025-01-15,,Private,,,China,,,,,,,,,,,,,10000,Anonymized,Anonymized,True,,63,,5000000000000000000,5000000000000000000,,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.03,,,,,,35.486703,101.901875
Los Alamos NL Venado,Existing,Confirmed,Yes,18.704685759518245,2560.000000037698,NVIDIA GH200,2560,United States of America,US Department of Energy,2024-04-16,,Public,2.7775,117272446.69013454,"Los Alamos, New Mexico, USA",Los Alamos National Laboratory,GH200,"The system consists of 2,560 direct, liquid-cooled Nvidia Grace Hopper Superchips; 920 Nvidia Grace CPU Superchips, each of which contains 144 Arm cores",2024-04-16,,11.960001696241358,,,,,,,2560,NVIDIA,,True,,31,18.403655763854264,5066240000000000000,5066240000000000000,2533120000000000000,1265920000000000000,3.6528128000000004,2.7775,912014401440.144,117272446.69013454,,,,,,Meta GenAI 2024a,0.10416666666666656,https://web.archive.org/web/20240921074409/https://www.datacenterdynamics.com/en/news/los-alamos-national-laboratory-unveils-new-venado-supercomputer-reportedly-capable-of-10-exaflops-of-peak-ai-performance/,,,,,35.881462,-106.301781
Anonymized Chinese System,Planned,Confirmed,Yes,18.69897000433602,3000,,3000,China,,,,Public/Private,8,100000000,China,,,,,,11.795880017344073,,,,,,,7000,Anonymized,Anonymized,,,,18.69897000433602,5000000000000000000,,5000000000000000000,1000000000000000100,2,8,625000000000,100000000,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.69897000433602,3000,,,China,,2024-01-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,27,18.69897000433602,5000000000000000000,,5000000000000000000,,,,,,,,,,,Microsoft Azure Eagle,0.2,,,,,,35.486703,101.901875
Meta Research SuperCluster 2 (RSC-2),Existing,Confirmed,Yes,18.69827457667437,2522.48610413013,NVIDIA A100,8000,United States of America,Meta AI,2023-10-31,,Private,6.639359999999999,157825162.07280722,,"Meta,Primarily vision applications at Meta",A100,"RSC-1 is a general ML cluster (e.g., training some of the prominent LLMs) of 16k GPU size, while RSC-2 focuses on vision applications and is of 8k GPU size.",2023-10-31,Reported in publication,11.575118363368931,,,,,,,8000,NVIDIA,,True,,20,18.397244581010384,4992000000000000000,4992000000000000000,2496000000000000000,1248000000000000000,6.639359999999999,,375939849624.0602,157825162.07280722,,,,,,Tesla 10k H100 Cluster,0.2522486104092987,https://web.archive.org/web/20241108001043/https://arxiv.org/abs/2410.21680,https://arxiv.org/pdf/2410.21680v1,,,,39.381266,-97.922211
NFDG Andromeda Phase 1,Existing,Confirmed,Yes,18.69646542927155,2512.000000036976,NVIDIA H100 SXM5 80GB,2512,United States of America,Nat Friedman and Daniel Gross,2023-06-13,They have since expanded this,Private,3.6483283199999996,90696068.37765959,"Ashburn, Virginia",Startups they invest in,H100,"2,512 H100s on 314 nodes interlinked with 3.2Tbps infiniband",2023-06-13,Listed as available on their official website,11.833297626692357,,NFDG Andromeda Phase 2,,,,,2512,NVIDIA,,True,,13,18.395391541093318,4971248000000000000,4971248000000000000,2485372800000000000,1242686400000000000,3.6483283199999996,,681236057175.9069,90696068.37765959,,,,,,Microsoft GPT-4 cluster,0.3186697435897409,https://web.archive.org/web/20230907083238/https://andromeda.ai/,https://web.archive.org/web/20240706081837/https://www.businessinsider.com/nvidia-gpu-venture-capitalists-buying-for-startups-2023-6,,,,39.043555,-77.487449
Tesla A100 Cluster Phase 2,Existing,Confirmed,Yes,18.662062404019924,2320.6872157997213,NVIDIA A100 SXM4 80 GB,7360,United States of America,Tesla,2022-08-16,,Private,6.2689536,166279435.48285377,,Tesla,A100,"now has a total of 7,360 A100 GPUs",2022-08-16,,11.563837352959242,Tesla A100 Cluster Phase 1,,,,,,7360,NVIDIA,,True,,6,18.36103240835594,4592640000000000000,4592640000000000000,2296320000000000000,1148160000000000000,6.2689536,,366300366300.36633,166279435.48285377,,,,,,Microsoft GPT-4 cluster,0.2944000000000004,https://web.archive.org/web/20230521065409/https://www.hpcwire.com/2022/08/16/tesla-gooses-its-gpu-powered-ai-super-is-dojo-next/,,,,,39.381266,-97.922211
EuroHPC LUMI,Existing,Confirmed,Yes,18.65918345871274,2305.354219336614,AMD Radeon Instinct MI250X,11912,Finland,EuroHPC JU,2022-09-15,"Came online in phases, so unclear exactly when it became operational. A portion of it first was innagurated as operational June 2022, and close to 10k GPUs were confirmed to operational by Nov 2022. Reports suggest that ""phase two"" began in August. Marking the operational date as September as an estimate",Public,12.0922,171001750.0814989,"Tehdaskatu 15, 87100 Kajaani, Finland","Researchers,Cloud",AMD MI250X,"The GPU partition consists of 2978 nodes, each node with one 64-core AMD Trento CPU and four AMD MI250X GPUs. A total of 11912 AMD GPUs.",2022-09,Confirmed by official sources,11.576678137095975,,,,,,,11912,AMD,,True,,8,18.65918345871274,4562296000000000000,4562296000000000000,4562296000000000000,1139978400000000000,12.682706399999999,12.0922,377292469525.81,306928476.0706662,165000000,171001750.0814989,"""The total budget of the product, which will be co-funded by the EuroHPC JU and the LUMI Consortium, will total €144.5 million (over $165 million). The total cost of ownership of the system from 2020 to 2026 will be €200 million ($237.1 million), which includes the cost of hardware.""",,,Microsoft GPT-4 cluster,0.29245487179486945,https://web.archive.org/web/20241003125301/https://www.lumi-supercomputer.eu/lumis-full-system-architecture-revealed/,https://web.archive.org/web/20240908234416/https://en.wikipedia.org/wiki/LUMI,https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20240502/01_Architecture/,https://www.tomshardware.com/news/amd-based-finnish-supercomputer-will-be-one-of-worlds-most-powerful-in-mid-2021,,64.233706,27.703313
Lawrence Berkeley NL NERSC Perlmutter,Existing,Confirmed,Yes,18.650582586336494,2260.147549300599,NVIDIA A100 SXM4 40 GB,6144,United States of America,US Department of Energy,2021-05-17,,Public,6.9,153237845.35186794,"Lawrence Berkeley National Laboratory, Building 59, 1 Cyclotron Rd, Berkeley, CA 94720",Scientific research,A100,"1536 nodes: 1x AMD EPYC 7763	4x NVIDIA A100 (40GB)
256 nodes: 1x AMD EPYC 7763	4x NVIDIA A100 (80GB)	",2021-05-17,,11.510703499935255,,,,,NVIDIA A100 SXM4 80 GB,1024,7168,NVIDIA,NVIDIA,True,,2,18.34955259067251,4472832000000000000,4472832000000000000,2236416000000000000,1118208000000000000,6.157598719999999,6.9,324118260869.56525,198102286.01127368,146000000,153237845.35186794,$146 million for machine plus multiple years of service,,,Sunway OceanLight,0.7523148148148198,https://web.archive.org/web/20240421214604/https://www.hpcwire.com/2021/05/27/nersc-debuts-perlmutter-worlds-fastest-ai-supercomputer/,https://web.archive.org/web/20240927184852/https://docs.nersc.gov/systems/perlmutter/architecture/,https://top500.org/news/crays-next-generation-supercomputer-headed-to-berkeley-lab-in-2020/,,,37.877776,-122.249533
AIST ABCI-Q,Planned,Confirmed,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,Japan,National Institute of Advanced Industrial Science and Technology (AIST),,"""more than 2,000"": estimated as 2048",Public,2.8700672000000003,96257006.65495703,"AIST Tsukuba
1 Chome Higashi, Tsukuba, Ibaraki 305-0046, Japan",,H100,"The supercomputer is powered by more than 2,000 NVIDIA H100 Tensor Core GPUs in 500+ nodes",Planned H1 2025,"""more than 2,000"": estimated as 2,200",11.848809792870606,,,,,,,2048,NVIDIA,,,,,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.8700672000000003,,706008277436.8489,96257006.65495703,,,,,,,,https://web.archive.org/web/20240729195553/https://nvidianews.nvidia.com/news/nvidia-powers-japans-abci-q-supercomputer-for-quantum-research,,,,,36.08322,140.077552
Horizon Compute Baobab Phase 2,Existing,Likely,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,United States of America,Horizon Compute,2025-03-01,"Unclear exactly when this was expanded to 2k H100s, but at the latest by March 1st, 2025",Private,2.8700672000000003,69027926.85064429,,Cloud,H100,"256 HGX H100 nodes

(2048 GPUs)",2025-03-01,,11.848809792870606,Horizon Compute Baobab Phase 1,,,,,,2048,NVIDIA,,True,,75,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.8700672000000003,,706008277436.8489,69027926.85064429,,,,,,xAI Colossus Memphis Phase 2,0.010240000000000013,https://web.archive.org/web/20241230203302/https://horizoncompute.com/,https://archive.ph/VaYyj#selection-320.0-330.0,,,,39.381266,-97.922211
Northern Data Group Taiga Cloud Island 3,Existing,Confirmed,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,,Northern Data Group,2024-03-30,,Private,2.9222502400000003,73105785.86519085,Europe,Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""the purchase of 20 NVIDIA H100 GPU Pods – each made up of 512 H100 GPUs""",2024-03-30,,11.840984455358647,,,,,,,2048,NVIDIA,,True,,36,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,73105785.86519085,,,,,,Meta GenAI 2024a,0.08333333333333327,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,,,,,,
Northern Data Group Taiga Cloud Island 4,Existing,Confirmed,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,,Northern Data Group,2024-03-30,,Private,2.9222502400000003,73105785.86519085,Europe,Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""the purchase of 20 NVIDIA H100 GPU Pods – each made up of 512 H100 GPUs""",2024-03-30,,11.840984455358647,,,,,,,2048,NVIDIA,,True,,36,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,73105785.86519085,,,,,,Meta GenAI 2024a,0.08333333333333327,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,,,,,,
Northern Data Group Taiga Cloud Island 5,Existing,Confirmed,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,,Northern Data Group,2024-03-30,,Private,2.9222502400000003,73105785.86519085,Europe,Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""the purchase of 20 NVIDIA H100 GPU Pods – each made up of 512 H100 GPUs""",2024-03-30,,11.840984455358647,,,,,,,2048,NVIDIA,,True,,36,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,73105785.86519085,,,,,,Meta GenAI 2024a,0.08333333333333327,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,,,,,,
Northern Data Group Taiga Cloud NO1 Island 1,Existing,Confirmed,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,Norway,Northern Data Group,2024-03-30,"There are two 2,048 H100 islands at this location",Private,2.9222502400000003,73105785.86519085,"Stølevegen 39, 4715 Øvrebø, Norway",Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""The partnership provides Taiga Cloud with additional access to 100% hydro-powered data centers, with Bulk housing 2 of Taiga Cloud’s islands of NVIDIA H100 GPUs and customer availability starting from Q1 2024.""",2024-03-30,,11.840984455358647,,,,,,,2048,NVIDIA,,True,,36,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,73105785.86519085,,,,,,Meta GenAI 2024a,0.08333333333333327,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,https://web.archive.org/web/20240621132432/https://bulkinfrastructure.com/newsroom/taiga-cloud-continues-rollout-of-nvidia-h100-gpus,,,,58.257573,7.89205
Northern Data Group Taiga Cloud NO1 Island 2,Existing,Confirmed,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,Norway,Northern Data Group,2024-03-30,"There are two 2,048 H100 islands at this location",Private,2.9222502400000003,73105785.86519085,"Stølevegen 39, 4715 Øvrebø, Norway",Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""The partnership provides Taiga Cloud with additional access to 100% hydro-powered data centers, with Bulk housing 2 of Taiga Cloud’s islands of NVIDIA H100 GPUs and customer availability starting from Q1 2024.""",2024-03-30,,11.840984455358647,,,,,,,2048,NVIDIA,,True,,36,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,73105785.86519085,,,,,,Meta GenAI 2024a,0.08333333333333327,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,https://web.archive.org/web/20240621132432/https://bulkinfrastructure.com/newsroom/taiga-cloud-continues-rollout-of-nvidia-h100-gpus,,,,58.257573,7.89205
NVIDIA Israel-1 Phase 2,Existing,Likely,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,Israel,NVIDIA,2024-11-20,"Not totally sure when this became operational, but it seems like it was at least operational by November 2024",Private,2.9222502400000003,78062110.53056912,Israel,NVIDIA,H100,"will feature 256 Nvidia HGX H100 systems, combining 2,048 Nvidia H100 80GB GPUs with more than 34 million CUDA cores and 1 million fourth-generation Tensor Cores, 2,560 BlueField-3 DPUs, and 80 Spectrum-4 switches",2024-11-20,"The first half of the GPUs were confirmed to be operational Nov 2023, with the second half expected to be operational in the first half of 2024",11.840984455358647,NVIDIA Israel-1 Phase 1,,,,,,2048,NVIDIA,,True,,65,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,78062110.53056912,,,,,,xAI Colossus Memphis Phase 1,0.0204800000000002,https://web.archive.org/web/20240628053616/https://www.datacenterdynamics.com/en/news/nvidias-israel-1-supercomputer-starts-operations/,,,,,31.847261,34.841999
Anonymized Chinese System,Existing,Confirmed,Yes,18.602059991327963,2000,,2000,China,,2024-09-15,,Private,3,70000000,China,,,,,,11.823908740944317,,,True,,,,2000,Anonymized,Anonymized,,,54,18.30102999566398,3999999999999999500,3999999999999999500,1999999999999999700,900000000000000100,3,,666666666666.6666,70000000,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,,35.486703,101.901875
Sesterce H100s Phase 1,Existing,Likely,Yes,18.60777574651019,2048.0000000301593,NVIDIA H100 SXM5 80GB,2048,,Sesterce,2024-07-06,"Unclear exactly where this cluster is located. This is likely the early phases of the Paris 4k H100 cluster, but it's unclear",Private,2.9222502400000003,72983193.71449463,,Cloud,H100,,2024-07-06,,11.840984455358647,,Sesterce H100s Phase 2,,,,,2048,NVIDIA,,True,,48,18.306701858331955,4052992000000000000,4052992000000000000,2026291200000000000,1013145600000000000,2.9222502400000003,,693400986768.3336,72983193.71449463,,,,,,Meta GenAI 2024a,0.08333333333333327,https://archive.ph/ZDjLF,https://archive.ph/gJllE,,,,,
Sesterce Synapse Phase 1,Existing,Likely,Yes,18.60777574651019,2048.0000000301593,NVIDIA H200 SXM,2048,France,Sesterce,2024-09-30,,Private,2.9222502400000003,73123829.94457883,"Paris, France",Cloud,H200,2048 x H200 SXM IB 3.2 available in early-september,2024-09-30,,11.8410283478729,,Sesterce Synapse Phase 2,,,,,2048,NVIDIA,,True,,55,18.306745750846208,4052992000000000000,4052992000000000000,2026496000000000000,1012736000000000000,2.9222502400000003,,693471069746.58,73123829.94457883,,,,,,xAI Colossus Memphis Phase 1,0.0204800000000002,https://web.archive.org/web/20240823055517/https://www.sesterce.com/reserved-cloud,https://archive.ph/ZDjLF,,,,48.853495,2.348392
SoftBank CHIE-2,Existing,Confirmed,Yes,18.606075961632293,2040.0000000300324,NVIDIA H100 SXM5 80GB,2040,Japan,Softbank,2024-10-31,,Private,2.9108352,77725269.47611627,Japan,Softbank,H100,Calculated from Top500,2024-10-31,,11.840984455358647,,,,,,,2040,NVIDIA,,True,,68,18.30500207345406,4037160000000000000,4037160000000000000,2018376000000000000,1009188000000000000,2.9108352,,693400986768.3336,77725269.47611627,,,,,,xAI Colossus Memphis Phase 1,0.020400000000000106,https://web.archive.org/web/20241119082609/https://top500.org/system/180329/,,,,,36.386493,138.59223
SoftBank CHIE-3,Existing,Confirmed,Yes,18.606075961632293,2040.0000000300324,NVIDIA H100 SXM5 80GB,2040,Japan,Softbank,2024-10-31,,Private,2.9108352,77725269.47611627,Japan,Softbank,H100,Calculated from Top500,2024-10-31,,11.840984455358647,,,,,,,2040,NVIDIA,,True,,68,18.30500207345406,4037160000000000000,4037160000000000000,2018376000000000000,1009188000000000000,2.9108352,,693400986768.3336,77725269.47611627,,,,,,xAI Colossus Memphis Phase 1,0.020400000000000106,https://web.archive.org/web/20241119062610/https://top500.org/system/180330/,,,,,36.386493,138.59223
Anonymized Chinese System,Existing,Likely,Yes,18.602059991327963,2000,,,China,,2021-04-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,2,18.602059991327963,3999999999999999500,,3999999999999999500,,,,,,,,,,,Sunway OceanLight,0.7,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.602059991327963,2000,,,China,,2024-09-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,61,18.602059991327963,3999999999999999500,,3999999999999999500,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,,35.486703,101.901875
Sakura's H100s Phase 1,Existing,Confirmed,Yes,18.600936321979884,2016.0000000296884,NVIDIA H100 SXM5 80GB,2016,Japan,Sakura Internet,2024-07-31,"The Ministry of Economy, Trade and Industry (METI) will subsidize 6.8 billion yen ($48.2 million) of the 13.5 billion yen it will cost to build the machine",Public/Private,2.87659008,95993910.03832601,"Ishikari City, Hokkaido Japan",Cloud,H100,"we have completed the installation of 2,016 'NVIDIA H100 Tensor Core GPUs'.",2024-07-31,,11.840984455358647,,,,,,,2016,NVIDIA,,True,,57,18.29986243380165,3989664000000000000,3989664000000000000,1994630400000000000,997315200000000000,2.87659008,,693400986768.3337,76577398.59470141,95164335,95993910.03832601,"The Ministry of Economy, Trade and Industry (METI) will subsidize 6.8 billion yen ($48.2 million) of the 13.5 billion yen it will cost to build the machine.",,,Meta GenAI 2024a,0.08203124999999996,https://archive.ph/g8HTU,https://archive.ph/m4ZML#,,,,43.172542,141.315389
Quebec 2k H100 Cluster,Existing,Likely,Yes,18.597475789870373,2000.0000000294335,NVIDIA H100 SXM5 80GB,2000,Canada,,2025-01-14,"Operational by 1/14/25 at the latest, but likely meaningfully earlier",Private,2.8028,75313115.25,"Quebec, Canada",Cloud,H100,From a private correspondence,2025-01-14,From a private correspondence about GPU cluster availability,11.848809792870606,,,,,,,2000,NVIDIA,,True,,82,18.296401901692143,3958000000000000000,3958000000000000000,1978800000000000000,989400000000000000,2.8028,,706008277436.8489,75313115.25,,,,,,xAI Colossus Memphis Phase 1,0.020000000000000004,,,,,,46.812189,-71.202785
Reka H100 Rental,Existing,Confirmed,Yes,18.597475789870373,2000.0000000294335,NVIDIA H100 SXM5 80GB,2000,,Reka AI,2023-12-15,,Private,2.9047199999999997,87430027.48771307,,Reka,H100,"""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s""
""Provider A: 2000 chips""",2023-12-15,,11.833297626692357,,,True,,,,2000,NVIDIA,,,,33,18.296401901692143,3958000000000000000,3958000000000000000,1978800000000000000,989400000000000000,2.9047199999999997,,681236057175.9069,87430027.48771307,,,,,,Microsoft Azure Eagle,0.1388888888888876,https://web.archive.org/web/20250108153733/https://publications.reka.ai/reka-core-tech-report.pdf,,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.602059991327963,2000,,2000,China,,2024-10-15,,Private,3,70000000,China,,,,,,11.823908740944317,,,,,,,2000,Anonymized,Anonymized,True,,69,18.30102999566398,3999999999999999500,3999999999999999500,1999999999999999700,900000000000000100,3,,666666666666.6666,70000000,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,,35.486703,101.901875
Meta Research SuperCluster (RSC-1) Phase 1,Existing,Confirmed,Yes,18.579088168955156,1917.089439138883,NVIDIA A100,6080,United States of America,Meta AI,2022-01-24,"The location is found from triangulating from the promo video. I'm fairly confident in it, but it isn't guaranteed to be accurate",Private,5.1787008,139229565.53258756,"6200 Technology Blvd, 23150 Sandston, Virginia, USA",Meta,A100,"At the time, “only” the first phase of the system was operational: 760 Nvidia DGX A100 nodes, totaling 6,080 Nvidia A100 GPUs",2022-01-24,,11.563837352959242,,Meta Research SuperCluster (RSC-1) Phase 2,,,,,6080,NVIDIA,,True,,5,18.27805817329118,3793920000000000000,3793920000000000000,1896960000000000000,948480000000000000,5.1787008,,366300366300.36633,139229565.53258756,,,,,,DeepSeek Fire-Flyer 2,0.6080000000000003,https://web.archive.org/web/20231215015022/http://www.hpcwire.com/2023/05/18/meta-completes-research-supercluster-announces-next-gen-datacenter/,https://web.archive.org/web/20240828114058/https://ai.meta.com/blog/ai-rsc/,,,,37.484236,-77.237219
CEA EXA1-HE Phase 2,Existing,Confirmed,Yes,18.577024164574468,1908.0000000280777,NVIDIA GH200,1908,France,French Alternative Energies and Atomic Energy Commission (CEA),2024-04-17,Phase 1 was from 2021 and didn't contain any GPUs,Public,2.0177,87101089.19249564,,Military Applications Division of the French Alternative Energies and Atomic Energy Commission,GH200,Calculated from Top500,2024-04-17,,11.971137574913461,,,,,,,1908,NVIDIA,,True,,46,18.27599416891049,3775932000000000000,3775932000000000000,1887966000000000000,943506000000000000,2.72248704,2.0177,935702036972.7908,87101089.19249564,,,,,,Meta GenAI 2024a,0.07763671874999914,https://web.archive.org/web/20240430193812/https://www.datacenterdynamics.com/en/news/eviden-delivers-second-exa1-supercomputer-to-frances-cea/,,,,,47.824905,2.618787
Tesla A100 Cluster Phase 1,Existing,Confirmed,Yes,18.555607073105634,1816.1899949736808,NVIDIA A100 SXM4 80 GB,5760,United States of America,Tesla,2021-06-20,,Private,4.9480704,180974131.32281664,,Tesla,A100,"for a whopping 5,760 A100s throughout the system",2021-06-20,,11.560141207399278,,Tesla A100 Cluster Phase 2,,,,,5760,NVIDIA,,True,,4,18.254577077441652,3594240000000000000,3594240000000000000,1797120000000000000,898560000000000000,4.9480704,,363196125907.99036,180974131.32281664,,,,,,Sunway OceanLight,0.6045386904761896,https://web.archive.org/web/20230322111214/https://www.hpcwire.com/2021/06/22/ahead-of-dojo-tesla-reveals-its-massive-precursor-supercomputer/,,,,,39.381266,-97.922211
Gcore data center Phase 2,Planned,Likely,Yes,18.542763202661927,1763.2642749122572,NVIDIA H100 SXM5 80GB,500,Korea (Republic of),"Gcore,NHN Corporation",,,Private,1.9019000000000001,23500245.76537037,"Incheon, South Korea",Cloud,"H100,H200,GB200,>1,000 GPUs, including H200s and GB200s","Gcore plans to expand this to more than 1,000 GPUs by the end of the year and will include H200s and GB200s.",Planned Q4 2024,,11.962533082636446,Gcore data center Phase 1,,,,NVIDIA GB200 NVL2,500,1000,NVIDIA,NVIDIA,,,,18.241720761068592,3489500000000000000,3489500000000000000,1744700000000000000,872350000000000000,1.9019000000000001,,917345812082.654,23500245.76537037,,,,,,,,https://web.archive.org/web/20240723133814/https://www.datacenterdynamics.com/en/news/gcore-expands-into-south-korea-with-nhn-cloud/,,,,,37.456,126.7052
Oak Ridge NL Summit,Decommissioned,Confirmed,Yes,18.538573733806853,1746.3365336285378,NVIDIA Tesla V100 SXM2,27648,United States of America,US Department of Energy,2018-06-08,,Public,13,218653393.66515836,"Oak Ridge National Laboratory
5200, 1 Bethel Valley Rd, Oak Ridge, TN 37830",US civilian research,V100,"""The IBM AC922 system consists of 4,608 compute servers, each containing two 22-core IBM Power9 processors and six NVIDIA Tesla V100 graphics processing unit accelerators""",2018-06-08,,11.424630381500018,,,,,,,27648,NVIDIA,,,,1,18.538573733806853,3456000000000000000,,3456000000000000000,433244160000000000,19.02071808,13,265846153846.15384,596012918.3397415,200000000,218653393.66515836,we estimate that Sierra represented $125 million of the $325 million of the CORAL procurement based on a flat flops cost between the two machines,True,2024-11-15,Oak Ridge NL Summit,1,https://web.archive.org/web/20240904145755/https://www.ornl.gov/news/ornl-launches-summit-supercomputer,https://web.archive.org/https://www.olcf.ornl.gov/olcf-resources/compute-systems/summit/summit-faqs,https://www.nextplatform.com/2017/10/05/clever-machinations-livermores-sierra-supercomputer/,,,36.010294,-84.269567
Microsoft Azure Meta AI Rental,Existing,Confirmed,Yes,18.52757834950539,1702.6781202878253,NVIDIA A100,5400,United States of America,Microsoft,2022-05-27,"Not marked ""possible duplicate"" since we don't have any other Microsoft supercomputers existing at this time that are this large and aren't known to be used by anyone else (eg, the 25k A100 supercomputer used by OpenAI)",Private,4.599504,121870051.90605606,,Meta,A100,"Meta AI did reveal that the company plans to utilize next-gen machine learning workloads on a reserved cluster on Microsoft Azure that would enlist 5,400 A100 GPUs from NVIDIA.",2022-05-27,,11.563837352959242,,,,,,,5400,NVIDIA,,True,,10,18.22654835384141,3369600000000000000,3369600000000000000,1684800000000000000,842400000000000000,4.599504,,366300366300.3663,121870051.90605606,,,,,,Microsoft GPT-4 cluster,0.21599999999999855,https://web.archive.org/web/20220630142849/https://wccftech.com/microsoft-azure-upgraded-amd-instinct-mi200-gpu-clusters-for-large-scale-ai-training/,,,,,39.381266,-97.922211
Argonne NL Aurora,Existing,Confirmed,Yes,18.52401917012131,1688.781162228007,Intel Data Center GPU Max 1550,63744,United States of America,US Department of Energy,2024-05-13,,Public,60,503027173.7772633,"Theory and Computing Sciences Building
Building 240, Argonne National Laboratory, 9700 S. Cass Ave, Argonne Ct, Lemont, IL 60439",Argonne National Laboratory and general scientific community,Intel Data Center GPU Max Series 1550,"The Aurora supercomputer at Argonne National Laboratory is now fully equipped with all 10,624 compute blades, boasting 63,744 Intel® Data Center GPU Max Series and 21,248 Intel® Xeon® CPU Max Series processors.",2024-05-13,From official sources,10.745867919737666,,,,,,,63744,Intel,,True,,52,18.52401917012131,3342097920000000000,,3342097920000000000,3342097920000000000,77.96146176,60,55701632000,940144813.455289,500000000,503027173.7772633,"The Aurora system was budgeted for $500 million, and HPE is still getting paid to assemble the system and Intel is getting paid to help with the software stack, we suspect.",,,Meta GenAI 2024a,0.06871668140475001,https://web.archive.org/web/20240824025729/https://www.intel.com/content/www/us/en/newsroom/news/intel-powered-aurora-supercomputer-breaks-exascale-barrier.html#gs.f9p1j0,https://web.archive.org/web/20240524235355/https://www.extremetech.com/computing/intel-aurora-supercomputer-breaks-exascale-barrier-but-fails-to-topple,https://web.archive.org/web/20240604155426/https://extremecomputingtraining.anl.gov/wp-content/uploads/sites/96/2023/08/ATPESC-2023-Track-1-Talk-3-Servesh-Mulalidharan-Aurora.pdf,,,41.718369,-87.978791
Jean Zay Supercomputer Phase 4,Existing,Confirmed,Yes,18.49706904252758,1587.1692774362077,NVIDIA H100 SXM5 80GB,1456,France,GENCI,2024-09-15,,Public,2.41672704,63517846.74947293,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,"V100,A100,H100,1,832 V100s 416 A100s 1,456 H100s","In total after this extension, Jean Zay will be equipped with 1,456 NVIDIA H100 GPUs, in addition to the 416 NVIDIA A100 Tensor Core GPUs and 1,832 NVIDIA V100 Tensor Core GPUs remaining from the old configuration",2024-09,,11.812771180659471,Jean Zay Supercomputer Phase 3,,,,NVIDIA A100 SXM4 80 GB,416,3704,NVIDIA,NVIDIA,True,,71,18.19599878194998,3141008000000000000,3141008000000000000,1570358400000000000,785179200000000000,2.41672704,,649787242832.3556,63517846.74947293,,,,,,xAI Colossus Memphis Phase 1,0.0158716927741285,https://web.archive.org/web/20240528032205/http://www.idris.fr/eng/annonces/idris-extension-jean-zay-h100-eng.html,https://web.archive.org/web/20240523101724/https://www.cnrs.fr/en/press/genci-and-cnrs-choose-eviden-make-jean-zay-supercomputer-one-most-powerful-france,,,,48.706817,2.175654
Nebius ISEG,Existing,Confirmed,Yes,18.478289382151168,1520.0000000223818,NVIDIA H100 SXM5 80GB,1520,Finland,Nebius AI,2023-11-01,,Private,2.246,50309245.90140562,"Moreenikatu 6, 04600 Mäntsälä, Finland",Cloud,H100,"""The Mäntsälä data center is also home to ISEG, among the most powerful supercomputers worldwide in both performance and energy efficiency.""
Chip count calculated from Top500
",2023-11-01,,11.825805742047494,,,,,,,1520,NVIDIA,,True,,32,18.177215493972934,3008080000000000000,3008080000000000000,1503888000000000000,751944000000000000,2.2075872,2.246,669585040071.2378,50309245.90140562,,,,,,Tesla 10k H100 Cluster,0.15200000000000002,https://web.archive.org/web/20241008144121/https://www.datacenterdynamics.com/en/news/nebius-deploys-ai-cluster-at-equinix-data-center-in-paris/,https://web.archive.org/web/20241109070701/https://www.top500.org/system/180234/,,,,60.629522,25.26336
Anonymized Chinese System,Existing,Likely,No,18.477121254719663,2000,,,China,,2022-08-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.477121254719663,3000000000000000000,,3000000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.477121254719663,2000,,,China,,2024-03-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,43,18.477121254719663,3000000000000000000,,3000000000000000000,,,,,,,,,,,Meta GenAI 2024a,0.06,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,Yes,18.477121254719663,2000,,,China,,2024-06-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,59,18.477121254719663,3000000000000000000,,3000000000000000000,,,,,,,,,,,Meta GenAI 2024a,0.06,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,18.477121254719663,2000,,,Hong Kong,,,,Public,,,Hong Kong,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30102999566398,3000000000000000000,3000000000000000000,1999999999999999700,,,,,,,,,,,,,,,,,,22.279356,114.16255
Sandia NL El Dorado,Existing,Confirmed,Yes,18.47436547251891,1506.3284487336496,AMD Instinct MI300A,1520,United States of America,US Department of Energy,2024-11-18,,Public,1.8556,62331530.08696258,"Albuquerque, New Mexico","Scientific Research,Sandia NL",AMD MI300A,Calculated from Top500,2024-11-18,,11.904851113008208,,,,,,,1520,AMD,,True,,90,18.17333547685493,2981024000000000000,2981024000000000000,1490512000000000000,745256000000000000,2.3547596800000004,1.8556,803250700582.022,62331530.08696258,,,,,,xAI Colossus Memphis Phase 1,0.015063284487114813,https://web.archive.org/web/20241125211138/https://www.top500.org/system/180309/,,,,,35.088192,-106.650945
Anonymized Chinese System,Planned,Confirmed,Unclear,18.477121254719663,1000,,,China,,,,Public,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,,,,17.999999999999996,3000000000000000000,3000000000000000000,999999999999999900,,,,,,,,,,,,,,,,,,35.486703,101.901875
NVIDIA Selene Phase 2,Existing,Confirmed,Yes,18.446462603680565,1412.592218312862,NVIDIA A100,4480,United States of America,NVIDIA,2020-11-30,,Private,4.7391,121250566.08,"Santa Clara, California",NVIDIA,A100,Configuration: 4480 NVIDIA A100 Tensor Core GPUs,2020-11-30,,11.469736735149642,NVIDIA Selene Phase 1,,,,,,4480,NVIDIA,,True,,2,18.145432608016584,2795520000000000000,2795520000000000000,1397760000000000000,698880000000000000,3.913728,4.7391,294942077609.6727,121250566.08,,,,,,Oak Ridge NL Summit,0.8088888888888891,https://web.archive.org/web/20240406153716/https://www.hpcwire.com/2020/06/22/nvidia-nabs-7-spot-on-top500-with-selene-launches-a100-pcie-cards/,https://web.archive.org/web/20240404062320/https://www.opensfs.org/wp-content/uploads/Accelerating-AI-at-Scale_Julie_Prethvi_updated051421.pdf,,,,37.35231,-121.9619
Paper on Megatron-Turing,Existing,Confirmed,Yes,18.446462603680565,1412.592218312862,NVIDIA A100 SXM4 80 GB,4480,,"Microsoft,NVIDIA",2022-01-28,,Private,3.8158847999999996,102590206.18190661,,"Microsoft,NVIDIA",A100,"""Model training is done with mixed precision using 16-bit bfloat on NVIDIA’s Selene [2] supercomputer with 560 DGX A100 nodes. Each cluster node has 8 NVIDIA 80-GB A100 GPUs""",2022-01-28,,11.563837352959242,,,True,NVIDIA Selene Phase 2,,,4480,NVIDIA,,,,8,18.145432608016584,2795520000000000000,2795520000000000000,1397760000000000000,698880000000000000,3.8158847999999996,,366300366300.36633,102590206.18190661,,,,,,DeepSeek Fire-Flyer 2,0.44800000000000045,https://arxiv.org/abs/2201.11990,,,,,,
Meta 2017 V100 Cluster,Existing,Confirmed,Yes,18.43933269383026,1389.590702395391,NVIDIA Tesla V100 SXM2,22000,United States of America,Meta AI,2017-10-01,"Unsure if this cluster has since been decommissioned, since it's relatively old
Our only information on the date is sources saying it was ""Built in 2017"". The V100 wasn't released until June 2017, so I assume this cluster was built in the second half of 2017. October 1st is the middle of the second half of 2017, so I put that as the date",Private,15.49548,475980972.5809265,,Meta,V100,"The first generation of this infrastructure, designed in 2017, has 22,000 NVIDIA V100 Tensor Core GPUs in a single cluster that performs 35,000 training jobs a day.",2017-10-01,Might be decommissioned by now,11.249127660004069,,,,,,,22000,NVIDIA,,True,,1,18.43933269383026,2750000000000000000,,2750000000000000000,344740000000000000,15.49548,,177471107703.66583,475980972.5809265,,,,True,,Meta 2017 V100 Cluster,1,https://web.archive.org/web/20240828114058/https://ai.meta.com/blog/ai-rsc/,,,,,39.381266,-97.922211
TensorWave MI300X Cluster 1 Phase 1,Existing,Likely,Yes,18.41745508506616,1321.3239009795366,AMD Instinct MI300X,1000,United States of America,TensorWave,2024-04-01,,Private,1.5288000000000002,27161808.75471603,,Cloud,MI300X,1000 MI300Xs available from 04/01/2024 to 04/01/2025 (USA),2024-04-01,,11.932057806582943,,,,,,,1000,AMD,,True,,52,18.116408480629897,2614900000000000000,2614900000000000000,1307400000000000000,653700000000000000,1.5288000000000002,,855180533751.9622,27161808.75471603,,,,,,Meta GenAI 2024a,0.053764807167971905,https://web.archive.org/web/20240420190200/https://gpulist.ai/detail/8f759e7,https://web.archive.org/web/20241009165502/https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/,,,,39.381266,-97.922211
Paper on Falcon 180B,Existing,Confirmed,Yes,18.407544537650196,1291.5128853146175,NVIDIA A100,4096,,Amazon,2023-11-28,,Private,3.3993523199999998,80618433.04566652,,Cloud,A100,"pretrain these models on up to 4,096 A100s on cloud AWS infrastructure",2023-11-28,,11.575118363368931,,,True,Amazon Titan training cluster,,,4096,NVIDIA,,,,40,18.10651454198622,2555904000000000000,2555904000000000000,1277952000000000000,638976000000000000,3.3993523199999998,,375939849624.0602,80618433.04566652,,,,,,Microsoft Azure Eagle,0.08968839481219436,https://arxiv.org/abs/2311.16867,https://web.archive.org/web/20250210123929/https://huggingface.co/tiiuae/falcon-180B,,,,,
Core42 SuperPOD,Existing,Confirmed,Yes,18.40093290551879,1272.0000000187285,NVIDIA H100 SXM5 80GB,1272,United Arab Emirates,G42,2024-10-30,,Public/Private,1.81499136,48463991.55569602,United Arab Emirates,"Cloud,G42",H100,Calculated from Top500,2024-10-30,,11.840984455358647,,,,,,,1272,NVIDIA,,True,,88,18.099859017340556,2517288000000000000,2517288000000000000,1258516800000000000,629258400000000000,1.81499136,,693400986768.3336,48463991.55569602,,,,,,xAI Colossus Memphis Phase 1,0.01272000000000009,https://web.archive.org/web/20241125214402/https://www.middleeastainews.com/p/core42s-nvidia-dgx-superpod-ranked-first,,,,,23.85206,54.256172
Tesla Training Cluster,Existing,Likely,Yes,18.40070511311989,1271.3329964815769,NVIDIA A100,4032,United States of America,Tesla,2021-08-20,I'm assuming these are A100s because H100s weren't out yet at the time and that is what the other cluster mentioned has,Private,3.46364928,137894792.57494116,,Tesla,A100,"as well as the previously mentioned 5670 system which is used for training, it has a second, 4032 GPU system for training and a 1752 GPU system for auto-labeling",2021-08-20,"Number of GPUs officially announced, but no other details about it released",11.560141207399278,,,,,,,4032,NVIDIA,,True,,9,18.099675117455913,2515968000000000000,2515968000000000000,1257984000000000000,628992000000000000,3.46364928,,363196125907.99036,137894792.57494116,,,,,,DeepSeek Fire-Flyer 2,0.4032000000000007,https://web.archive.org/web/20240221025227/https://www.datacenterdynamics.com/en/news/tesla-details-dojo-supercomputer-reveals-dojo-d1-chip-and-training-tile-module/,,,,,39.381266,-97.922211
AWS EC2 P4d,Existing,Likely,Unclear,18.397244581010384,1261.2430520650546,NVIDIA A100 SXM4 40 GB,4000,United States of America,Amazon,2020-11-02,,Private,3.4944,108448368.43979058,,Cloud,A100,"EC2 UltraClusters are comprised of More than 4,000 latest NVIDIA A100 Tensor Core GPUs",2020-11-02,,11.55284196865778,,,,,,,4000,NVIDIA,,,,,18.096214585346402,2496000000000000000,2496000000000000000,1248000000000000000,624000000000000000,3.4944,,357142857142.8571,108448368.43979058,,,,,,,,https://web.archive.org/web/20240522231925/https://pages.awscloud.com/amazon-ec2-p4d.html,,,,,39.381266,-97.922211
Ezra-1 Stability AI AWS Cluster,Existing,Confirmed,Yes,18.397244581010384,1261.2430520650546,NVIDIA A100,4000,,Amazon,2022-07-20,,Private,3.40704,90369258.41459444,,Stability AI,A100,"The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month",2022-07-20,,11.563837352959242,,,,,,,4000,NVIDIA,,True,,16,18.096214585346402,2496000000000000000,2496000000000000000,1248000000000000000,624000000000000000,3.40704,,366300366300.3663,90369258.41459444,,,,,,Microsoft GPT-4 cluster,0.15999999999999878,https://web.archive.org/web/20241207204816/https://stability.ai/news/stable-diffusion-announcement,,,,,,
Anonymized Chinese System,Existing,Confirmed,No,18.30102999566398,1000,,,China,,2022-11-15,,,,,China,,,,,,,,,True,,,,,Anonymized,Anonymized,,,,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,,,,,,,,35.486703,101.901875
JUWELS-Booster,Existing,Confirmed,Yes,18.36852042974849,1180.523496732893,NVIDIA A100,3744,Germany,Julich Supercomputing Center,2020-11-16,,Public,3.1598,101507672.85964398,"52428 Jülich, Germany",Academic research in Europe,A100,total of 1872 CPUs (AMD EPYC Rome) + total of 3744 GPUs (NVIDIA A100),2020-11-16,,11.56783083932851,,,,,,,3744,NVIDIA,,True,,3,18.067490434084508,2336256000000000000,2336256000000000000,1168128000000000000,584064000000000000,3.2707584,3.1598,369684157225.1408,101507672.85964398,,,,,,Oak Ridge NL Summit,0.6760000000000007,https://archive.ph/3RM36,,,,,50.921734,6.358342
Anonymized Chinese System,Planned,Likely,Yes,18.30102999566398,1000,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,,,,,,,,35.486703,101.901875
Huawei Pangu Ultra MoE 910Bs,Existing,Confirmed,Yes,18.353339095311302,1139.9696816741844,Huawei Ascend 910B,6000,,,,,,4.8048,,,Huawei,,"In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs",,,11.671663780456399,,,,,,,6000,Huawei,,True,,,18.353339095311302,2256000000000000000,,2256000000000000000,564000000000000000,4.8048,,469530469530.46954,,,,,,,,,,,,,,,
Paper on AFM-server,Existing,Confirmed,Yes,18.35272263746202,1138.3527034023157,Google TPU v4,8192,United States of America,Google,2024-06-10,Cloud TPU clusters implies they trained on Google infrastructure,Private,2.8387573760000007,73409062.36942618,,"Cloud,Google",TPUv4,"""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""
""The AFM models are pre-trained on v4 and v5p Cloud TPU clusters""
""AFM-server was trained on 8192 TPUv4 chips provisioned as 8 × 1024 chip slices, where slices are connected together by the data-center network (DCN)""",2024-06-10,,11.899594361796732,,,True,Google Oklahoma TPU v4 Pods,,,8192,Google,,,,66,18.35272263746202,2252800000000000000,2252800000000000000,2252800000000000000,,2.8387573760000007,,793586665435.4048,73409062.36942618,,,,,,Meta GenAI 2024a,0.046319690079164685,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,https://web.archive.org/web/20250114124728/https://arxiv.org/pdf/2407.21075,,,,39.381266,-97.922211
JCAHPC Miyabi,Existing,Confirmed,Yes,18.345663816876577,1120.0000000164925,NVIDIA GH200,1120,Japan,Joint Center for Advanced High Performance Computing (JCAHPC),2025-01-15,,Public,1.5695680000000003,54654675.09599999,Japan,Scientific Research,GH200,"Miyabi-G (Accelerator-node group): 1,120 nodes, one NVIDIA GH200 Grace Hopper Superchip per node",2025-01,,11.848853685384858,,,,,,,1120,NVIDIA,,True,,109,18.044633821212596,2216480000000000000,2216480000000000000,1108240000000000000,553840000000000000,1.5695680000000003,,706079634651.0631,54654675.09599999,,,,,,xAI Colossus Memphis Phase 1,0.0112000000000001,https://web.archive.org/web/20240417155526/https://www.jcahpc.jp/eng/supercomputer/miyabi-e.html,,,,,36.386493,138.59223
Anonymized Chinese System,Existing,Likely,Yes,18.30102999566398,1000,,,China,,2022-07-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,17,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,Microsoft GPT-4 cluster,0.1,,,,,,35.486703,101.901875
Lawrence Livermore NL Sierra,Existing,Confirmed,Yes,18.33445375115093,1091.4603335178444,NVIDIA V100,17280,United States of America,US Department of Energy,2018-06-01,,Public,11.5,136658371.04072398,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","Lawrence Livermore NL,Scientific Research",V100,"Total GPUs: 17,280",2018-06-01,,11.273755910797318,,,,,,,17280,NVIDIA,,True,,2,18.33445375115093,2160000000000000000,,2160000000000000000,270777600000000000,11.8879488,11.5,187826086956.52173,340708604.2338461,125000000,136658371.04072398,we estimate that Sierra represented $125 million of the $325 million of the CORAL procurement based on a flat flops cost between the two machines,True,,Meta 2017 V100 Cluster,0.7854545454545527,https://web.archive.org/web/20240926221231/https://en.wikipedia.org/wiki/Sierra_(supercomputer),https://www.nextplatform.com/2017/10/05/clever-machinations-livermores-sierra-supercomputer/,https://web.archive.org/web/20240927001630/https://hpc.llnl.gov/hardware/compute-platforms/sierra,,,37.682018,-121.768374
Anonymized Chinese System,Planned,Likely,Unclear,18.30102999566398,1000,,4000,China,,,,Public,3,,China,,,,,,11.522878745280337,,,,,,,4000,Anonymized,Anonymized,,,,17.999999999999996,1999999999999999700,1999999999999999700,999999999999999900,,3,,333333333333.3333,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,1000,,4000,China,,2020-09-15,,Public,3,,China,,,,,,11.522878745280337,,,,,,,4000,Anonymized,Anonymized,True,,4,17.999999999999996,1999999999999999700,1999999999999999700,1000000000000000100,,3,,333333333333.3334,,,,,,,Oak Ridge NL Summit,0.6,,,,,,35.486703,101.901875
Chan Zuckerberg Initiative GPU Cluster,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,United States of America,"CZ Biohub Network,CoreWeave",2025-01-15,CoreWeave cloud based cluster,Public,1.4350336000000001,38560315.007999994,,"Chan Zuckerberg Initiative,Healthcare,Researchers",H100,"The high-performance computing cluster, which is planned to comprise 1,000+ GPUs, will enable AI and large language models for biomedicine at scale.",2025-01,,11.848809792870606,,,True,,,,1024,NVIDIA,,,,113,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4350336000000001,,706008277436.8489,38560315.007999994,,,,,,xAI Colossus Memphis Phase 1,0.010240000000000098,https://web.archive.org/web/20241201154848/https://chanzuckerberg.com/newsroom/czscience-builds-ai-gpu-cluster-predictive-cell-models/,https://web.archive.org/web/20250221155133/https://chanzuckerberg.com/rfa/ai-computing-gpu/,,,,39.381266,-97.922211
Denvr Dataworks H100,Existing,Likely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,United States of America,Denvr Dataworks,2024-10-15,"It appears to be in their central US availablity zone, suggesting that it's in the United States. Other location information is unknown",Private,1.4611251200000002,38937096.507405296,,Cloud,H100,"The cluster features Nvidia HGX H100 servers with 1,024 Hopper architecture-based SXM5 GPUs",2024-10-15,,11.840984455358647,,,,,,,1024,NVIDIA,,True,,92,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,38937096.507405296,,,,,,xAI Colossus Memphis Phase 1,0.010240000000000098,https://web.archive.org/web/20241214104602/https://www.datacenterdynamics.com/en/news/denvr-dataworks-expands-with-cloud-based-nvidia-h100-gpu-cluster/,,,,,39.381266,-97.922211
GreenNode Bangkok Cluster,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,Thailand,VNG Corporation,2024-06-25,,Private,1.4611251200000002,36491596.857247315,"1 Soi Ramkhamhaeng 28, Hua Mak, Khet Bang Kapi, Bangkok 10240, Thailand",Cloud,H100,GreenNode's GPU cluster includes 128 bare-metal servers equipped with 1024 NVIDIA H100 Tensor Core GPUs,2024-06-25,,11.840984455358647,,,,,,,1024,NVIDIA,,True,,73,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,36491596.857247315,,,,,,Meta GenAI 2024a,0.04166666666666664,https://web.archive.org/web/20250108210408/https://greennode.ai/blog/launch-hyper-scale-ai-gpu-cluster,https://archive.ph/pi5KL,,,,13.751996,100.640384
Horizon Compute Baobab Phase 1,Existing,Likely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,United States of America,Horizon Compute,2024-02-15,"Judging by their current offerings (March 2025), they have a 2k H100 cluster and a 1k H100 cluster, so I'm interpreting this as them saying that they had two 1k H100 clusters in Q1 2024",Private,1.4611251200000002,44580651.409562655,,Cloud,H100,"Since Q1 2024, we have already deployed two large-scale clusters (2000+ H100s) ",Q1 2024,,11.840984455358647,,Horizon Compute Baobab Phase 2,,,,,1024,NVIDIA,,True,,50,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,44580651.409562655,,,,,,Microsoft Azure Eagle,0.07111111111111111,https://web.archive.org/web/20241230203302/https://horizoncompute.com/,,,,,39.381266,-97.922211
IBM Blue Vela,Existing,Likely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,United States of America,IBM,2024-07-07,"There's 128x8=1,024 GPUs per pod, and it's implied that there's multiple pods, implying that there are thousands of GPUs, but this is all the information I have",Private,1.4611251200000002,36491596.857247315,,IBM,"A100,H100","It uses 128-node Compute Pods. These contain 4 x Scalable Units, each of which contain 32 nodes. The nodes contain [8] Nvidia H100 GPUs.",2024-07-07,Very uncertain on number of chips. ,11.840984455358647,,,,,,,1024,NVIDIA,,True,,77,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,36491596.857247315,,,,,,Meta GenAI 2024a,0.04166666666666664,https://web.archive.org/web/20240804175848/https://blocksandfiles.com/2024/08/02/big-blues-storage-scale-using-blue-vela-ai-supercomputer/,https://web.archive.org/web/20240516182554/https://www.eetimes.com/ibm-refreshes-its-vela-research-ai-supercomputer/,,,,39.381266,-97.922211
LeptonAI H100 Cluster,Existing,Likely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,United States of America,Lepton AI,2024-08-20,This cluster is implied by their blog post. Unclear when it was first operational,Private,1.4611251200000002,38941979.02439725,,Cloud,H100,"We observe similar statistics in our operation of different-sized clusters as well. For example, a 128 nodes (1024 GPU) cluster experiences one interruption every 30 hours",2024-08-20,,11.840984455358647,,,,,,,1024,NVIDIA,,True,,81,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,38941979.02439725,,,,,,Meta GenAI 2024a,0.04166666666666664,https://web.archive.org/web/20250128074456/https://blog.lepton.ai/introducing-gpud-the-missing-gpu-management-for-ai-0f0d026337e3,,,,,39.381266,-97.922211
Neevcloud cluster 1,Existing,Unlikely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,India,NeevCloud,2024-04-29,"Marked ""unlikely"" because an industry expert doubts it exists",Private,1.4611251200000002,36425955.19699726,Central India,Cloud,H100,1024 H100s available from 04/29/2024,2024-04-29,,11.840984455358647,,,,,,,1024,NVIDIA,,,,,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,36425955.19699726,,,,,,,,https://web.archive.org/web/20240420194021/https://gpulist.ai/detail/ea604d5,https://web.archive.org/web/20250110180448/https://entrepreneurshipstudio.com/news/posts/neevcloud-launches-with-40-000-gpus-revolutionizing-ai-cloud-services-for-smes-in-india,,,,23.315233,87.885124
NVIDIA Eos Phase 1,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,United States of America,NVIDIA,2023-05-22,"Full system = 4608 GPUs
Pre system = (128/576)x4608 = 1024",Private,1.48721664,48736713.50979148,United States of America,NVIDIA,H100,"Nvidia’s new “Pre-Eos 128 Node DGX SuperPOD” is another one we’ve been waiting to hear word on.... In its full configuration, Eos will span 18 32-DGX H100 Pods, for a total of 576 DGX H100 systems, 4,608 H100 GPUs",2023-05-22,Ranked in Top500,11.833297626692357,,NVIDIA Eos Phase 2,,,,,1024,NVIDIA,,True,,29,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.48721664,,681236057175.9069,48736713.50979148,,,,,,Microsoft GPT-4 cluster,0.12990358974358945,https://web.archive.org/web/20240406153948/https://www.hpcwire.com/2023/05/22/top500-frontier-gains-92-petaflops-henri-gets-a-little-greener/,,,,,39.381266,-97.922211
NVIDIA Helios,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA GH200,1024,,NVIDIA,2023-11-15,"They announced in May 2023 that they were building it and that it would be operational by the end of the year. They haven't publicly confirmed that it is operational, but it seems very likely that it is",Private,1.48721664,47268443.53218431,,NVIDIA,GH200,"""Helios will include 1,024 Grace Hopper Superchips and is expected to come online by the end of the year.""",Q4 2023,Likely finished by now,11.83334151920661,,,,,,,1024,NVIDIA,,True,,46,18.005715755182223,2026496000000000000,2026496000000000000,1013248000000000000,506368000000000000,1.48721664,,681304910628.219,47268443.53218431,,,,,,Microsoft Azure Eagle,0.07111111111111111,https://web.archive.org/web/20240825120701/https://nvidianews.nvidia.com/news/nvidia-announces-dgx-gh200-ai-supercomputer,,,,,,
NVIDIA Israel-1 Phase 1,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,Israel,NVIDIA,2023-11-23,,Private,1.48721664,33813671.09865381,Israel,NVIDIA,H100,"The first phase includes Dell PowerEdge XE9680 servers based on 128 Nvidia HGX H100 systems

will feature 256 Nvidia HGX H100 systems, combining 2,048 Nvidia H100 80GB GPUs with more than 34 million CUDA cores and 1 million fourth-generation Tensor Cores, 2,560 BlueField-3 DPUs, and 80 Spectrum-4 switches",2023-11-23,"The first half of the GPUs were confirmed to be operational Nov 2023, with the second half expected to be operational in the first half of 2024",11.833297626692357,,NVIDIA Israel-1 Phase 2,,,,,1024,NVIDIA,,True,,46,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.48721664,,681236057175.9069,33813671.09865381,,,,,,Microsoft Azure Eagle,0.07111111111111111,https://web.archive.org/web/20240628053616/https://www.datacenterdynamics.com/en/news/nvidias-israel-1-supercomputer-starts-operations/,,,,,31.847261,34.841999
Ori Global Cloud H100 Cluster,Existing,Likely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,,Ori Industries,2024-10-16,,Private,1.4611251200000002,38937096.507405296,,Cloud,H100,"The cluster features 1,024 Nvidia H100s",2024-10-16,Unclear where it is located,11.840984455358647,,,,,,,1024,NVIDIA,,True,,92,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,38937096.507405296,,,,,,xAI Colossus Memphis Phase 1,0.010240000000000098,https://archive.ph/UwWFb,,,,,,
Paper on Mamba 2 Hybrid,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,,NVIDIA,2024-06-12,,Private,1.4611251200000002,36558392.18097217,,NVIDIA,H100,"""When training on NVIDIA H100 GPUs (NVIDIA 2023), with a tensor-parallel
size of four and data-parallel size of 256 (1024 total GPUs)""",2024-06-12,,11.840984455358647,,,True,NVIDIA CoreWeave Eos-DFW Phase 1,,,1024,NVIDIA,,,,71,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,36558392.18097217,,,,,,Meta GenAI 2024a,0.04166666666666664,https://arxiv.org/abs/2406.07887,,,,,,
SIAM AI HGX,Existing,Likely,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,Thailand,SIAM AI,2024-09-12,,Private,1.4611251200000002,38941979.02439725,Thailand,Cloud,H100,"currently operates a powerful cluster of 1,024 NVIDIA H100 GPUs",2024-09-12,,11.840984455358647,,,,,,,1024,NVIDIA,,True,,84,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,38941979.02439725,,,,,,xAI Colossus Memphis Phase 1,0.010240000000000098,https://web.archive.org/web/20241204114134/https://www.cmkl.ac.th/news/the-first-hgx-h200-poc--asias-first-nvidia-h200--was-presented-by-siam-ai-nvidia-cloud-partner-to-cmkl-university-and-aiei,,,,,15.127333,101.017438
Sustainable Metal Cloud Singapore Phase 1,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,Singapore,SMC - Sustainable Metal Cloud,2024-05-30,,Private,1.4611251200000002,36558392.18097217,Singapore,Cloud,H100,"H100 GPUs, up to 1,024 per cluster",2024-05-30,,11.840984455358647,,,,,,,1024,NVIDIA,,True,,68,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,,693400986768.3336,36558392.18097217,,,,,,Meta GenAI 2024a,0.04166666666666664,https://archive.ph/ElLD9,,,,,1.351616,103.808053
Ubilink.AI Supercomputer,Existing,Confirmed,Yes,18.306745750846208,1024.0000000150794,NVIDIA H100 SXM5 80GB,1024,Taiwan,Ubilink AI,2024-11-18,,Private,1.7823,39031055.26528456,"Tucheng, New Taipei, Taiwan",Cloud,H100,Calculated from Top500,2024-11-18,,11.754691055571625,,,,,,,1024,NVIDIA,,True,,104,18.005671862667974,2026496000000000000,2026496000000000000,1013145600000000000,506572800000000000,1.4611251200000002,1.7823,568448409358.6938,39031055.26528456,,,,,,xAI Colossus Memphis Phase 1,0.010240000000000098,https://web.archive.org/web/20241125215925/https://ubitus.net/ubilink-opening/,,,,,24.97428,121.44137
Anonymized Chinese System,Planned,Likely,Yes,18.30102999566398,1000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,,,,,,,,35.486703,101.901875
Scaleway Nabuchodonosor,Existing,Confirmed,Yes,18.303339502154294,1016.0000000149575,NVIDIA H100 SXM5 80GB,1016,France,Iliad SA,2023-10-05,,Private,1.4755977599999999,33695574.738149196,"Paris, France","Scaleway,Cloud",H100,"Iliad has purchased an Nvidia DGX SuperPOD with 1,016 H100 GPUs",2023-10-05,,11.833297626692357,,,,,,,1016,NVIDIA,,True,,38,18.00226561397606,2010664000000000000,2010664000000000000,1005230400000000000,502615200000000000,1.4755977599999999,,681236057175.9069,33695574.738149196,,,,,,Tesla 10k H100 Cluster,0.10159999999999972,https://web.archive.org/web/20240225054506/https://www.datacenterdynamics.com/en/news/french-telco-iliad-acquires-nvidia-dgx-superpod-with-1016-h100-gpus/,https://web.archive.org/web/20240228100033/https://www.telecoms.com/digital-ecosystem/iliad-lays-claim-to-europe-s-most-powerful-ai-supercomputer,https://web.archive.org/web/20231005173700/https://www.digitaltveurope.com/2023/09/26/xavier-niels-iliad-sets-out-plan-to-create-european-ai-champion-with-supercomputer-investment/,,,48.853495,2.348392
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,1000,,1000,China,,2024-09-15,,Public/Private,0.7,,China,,,,,,12.455931955649723,,,,,,,3000,Anonymized,Anonymized,True,,94,18.30102999566398,1999999999999999700,,1999999999999999700,400000000000000060,0.7,,2857142857142.857,,,,,,,xAI Colossus Memphis Phase 1,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,1000,,,China,,2024-01-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,54,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,Microsoft Azure Eagle,0.07,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,1000,,,China,,2024-11-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,120,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Confirmed,Yes,18.30102999566398,1000,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,1000,,,China,,2024-12-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,121,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.01,,,,,,35.486703,101.901875
Hut 8 H100 Cluster,Existing,Confirmed,Yes,18.296445794206395,1000.0000000147247,NVIDIA H100 SXM5 80GB,1000,United States of America,Hut 8,2024-09-26,,Private,1.4268800000000001,38024508.30801299,"Chicago, USA",Cloud,H100,"The cluster, hosted at a tier-three data center in Chicago, comprises multiple Hewlett Packard Enterprise (“HPE”) Cray supercomputers powered by 1,000 NVIDIA H100 GPUs.",2024-09-26,,11.840984455358647,,,,,,,1000,NVIDIA,,True,,101,17.99537190602816,1979000000000000000,1979000000000000000,989400000000000000,494700000000000000,1.4268800000000001,,693400986768.3336,38024508.30801299,,,,,,xAI Colossus Memphis Phase 1,0.010000000000000082,https://web.archive.org/web/20241217152625/https://hut8.com/2024/09/26/hut-8-gpu-as-a-service-vertical-goes-live-with-inaugural-deployment/,,,,,41.881953,-87.632362
NHN Cloud's National AI Data Center,Existing,Confirmed,Yes,18.296445794206395,1000.0000000147247,NVIDIA H100 SXM5 80GB,1000,Korea (Republic of),NHN Corporation,2023-11-01,,Public/Private,1.4523599999999999,33098188.093030013,"Oryong-dong, Buk-gu, Gwangju, South Korea",Cloud,H100,"some 1,000 Nvidia H100 graphics processing units (GPUs) are working,",2023-11-01,,11.833297626692357,,,,,,,1000,NVIDIA,,True,,44,17.99537190602816,1979000000000000000,1979000000000000000,989400000000000000,494700000000000000,1.4523599999999999,,681236057175.9069,33098188.093030013,,,,,,Tesla 10k H100 Cluster,0.1,https://web.archive.org/web/20240721155544/https://koreajoongangdaily.joins.com/news/2024-03-25/business/industry/NHN-Cloud-aims-sky-high-with-AI-data-center-in-Gwangju/2010013,,,,,35.22299,126.84876
Voltage Park Texas Phase 1,Existing,Likely,Yes,18.296445794206395,1000.0000000147247,NVIDIA H100 SXM5 80GB,1000,United States of America,Voltage Park,2024-02-27,,Private,1.4268800000000001,35810188.63318463,Texas,Cloud,H100,1000 H100s available from 02/27/2024 to 02/27/2025 (Texas),2024-02-27,This is from a posting on gpulist.ai,11.840984455358647,,,,,,,1000,NVIDIA,,True,,56,17.99537190602816,1979000000000000000,1979000000000000000,989400000000000000,494700000000000000,1.4268800000000001,,693400986768.3336,35810188.63318463,,,,,,Microsoft Azure Eagle,0.06944444444444435,https://web.archive.org/web/20240226194913/https://gpulist.ai/detail/2104b9d,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,,,31.803973,-98.822319
Paper on Chameleon,Existing,Confirmed,Yes,18.282605801041896,968.6346639859632,NVIDIA A100,3072,,Meta AI,2024-05-16,Table 2 Chemeleon Model Pre-Training Resource Usage lists 3072 Concurrent GPUs for the 34B model. ,Private,2.50478592,59935114.385067165,,,A100,"""NVIDIA A100 80GB GPUs power both environments.""",2024-05-16,,11.582805192035222,,,True,Meta Research SuperCluster (RSC-1) Phase 2,,,3072,NVIDIA,,,,76,17.981575805377915,1916928000000000000,1916928000000000000,958464000000000000,479232000000000000,2.50478592,,382653061224.48975,59935114.385067165,,,,,,Meta GenAI 2024a,0.039413845376452566,https://arxiv.org/abs/2405.09818v1,,,,,,
TSUBAME4.0,Existing,Confirmed,Yes,18.278717027245964,960.0000000141367,NVIDIA H100 SXM5 80GB,960,Japan,Tokyo Institute of Technology,2024-04-01,,Public,1.3638,34268337.12430821,"4259 Nagatsutacho, Midori Ward, Yokohama, Kanagawa 226-0026, Japan",Cloud,H100,"The Japanese system will pack in 240 nodes of HPE's Cray XD6500 hardware...
Each node has a pair of AMD's 4th generation Epyc processors with 768GB of memory and four of Nvidia's H100 Tensor Core GPUs",2024-04-01,,11.842892452959116,,,,,,,960,NVIDIA,,True,,68,17.97764313906773,1899840000000000000,1899840000000000000,949824000000000000,474912000000000000,1.3698048,1.3638,696454025516.938,34268337.12430821,,,,,,Meta GenAI 2024a,0.03906249999999996,https://web.archive.org/web/20241002114126/https://www.theregister.com/2023/05/22/hpe_gets_to_build_new/,https://web.archive.org/web/20240627054219/https://www.hpcwire.com/off-the-wire/tokyo-techs-tsubame4-0-supercomputer-now-operational/,,,,35.518594,139.529816
Paper on AFM-on-device,Existing,Confirmed,Yes,18.274172633505035,950.0070742939295,Google TPU v5p,2048,,Google,2024-06-10,Cloud TPU clusters implies they trained on Google infrastructure,Private,,13097617.677388925,,Cloud,TPUv5p,"""AFM-on-device was trained on one slice of 2048 TPUv5p chips.""
""The AFM models are pre-trained on v4 and v5p Cloud TPU clusters""",2024-06-10,,,,,True,Google Hypercomputer TPU v5p pod,,,2048,Google,,,,81,17.973142637841054,1880064000000000000,1880064000000000000,940032000000000000,,,,,13097617.677388925,,,,,,Meta GenAI 2024a,0.03865588681152095,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,https://web.archive.org/web/20250114124728/https://arxiv.org/pdf/2407.21075,,,,,
Intel Stability Gaudi 2,Existing,Confirmed,Yes,18.255272505103303,909.5502779315291,Intel Habana Gaudi2,4000,United States of America,Intel,2024-03-11,,Private,4.89216,34422689.45810412,Texas,Stability AI,Habana Gaudi2,"build a large AI supercomputer using Xeon processors and 4,000 Gaudi2 AI hardware accelerators",2024-03-11,,11.565771852736441,,,,,,,4000,Intel,,True,,59,18.255272505103303,1800000000000000000,,1800000000000000000,,4.89216,,367935635792.7786,34422689.45810412,,,,,,Microsoft Azure Eagle,0.06316321374431492,https://web.archive.org/web/20240806003107/https://www.datacenterdynamics.com/en/news/intel-and-dell-to-build-supercomputer-for-stability-ai-featuring-cpus-and-gaudi2-accelerators/,https://web.archive.org/web/20241201154514/https://stability.ai/news/putting-the-ai-supercomputer-to-work,https://web.archive.org/web/20241202232149/https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators,,,31.803973,-98.822319
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,900,,7000,China,,2024-03-15,,Public,4,,China,,,,,,11.352182518111361,,,,,,,7000,Anonymized,Anonymized,True,,61,17.954242509439325,1999999999999999700,1999999999999999700,900000000000000100,200000000000000030,4,,225000000000.00003,,,,,,,Meta GenAI 2024a,0.04,,,,,,35.486703,101.901875
IBM Vela,Existing,Likely,Unclear,18.254577077441652,908.0949974868403,NVIDIA A100 SXM4 80 GB,2880,United States of America,IBM,2022-05-30,,Private,2.4530687999999996,64997361.01656324,,IBM,A100,"Vela, which has been online since May of last year, consists of 60 racks (per Forbes) and an unspecified number of nodes; however, one might be inclined to trust the above diagram – accurate in all other respects – and guess six nodes per rack, for a total of 360 nodes and 2,880 A100 GPUs.",2022-05-30,Number of chips is an estimate,11.563837352959242,,,,,,,2880,NVIDIA,,,,,17.95354708177767,1797120000000000000,1797120000000000000,898560000000000000,449280000000000000,2.4530687999999996,,366300366300.36633,64997361.01656324,,,,,,,,https://web.archive.org/web/20230320230907/https://www.hpcwire.com/2023/02/08/ibm-introduces-vela-cloud-ai-supercomputer-powered-by-intel-nvidia/,,,,,39.381266,-97.922211
Paper on PaLM,Existing,Confirmed,Yes,18.22778390085372,853.7645275517368,Google TPU v4,6144,United States of America,Google,2022-04-05,,Private,2.2241157119999997,56257230.64621294,,Google,TPUv4,"""We trained PaLM on 6144 TPU v4 chips using Pathways,""",2022-04-05,,11.88062652272075,,,,,,,6144,Google,,True,,15,18.22778390085372,1689600000000000000,1689600000000000000,1689600000000000000,,2.2241157119999997,,759672705374.0629,56257230.64621294,,,,,,DeepSeek Fire-Flyer 2,0.27076923076923326,https://arxiv.org/abs/2204.02311,,,,,39.381266,-97.922211
Microsoft Ares/Maia,Existing,Likely,Yes,18.214419939295734,827.892875201677,Maia 100 (M100),2048,United States of America,Microsoft,2023-11-15,,Private,2.1245952,,,Cloud,Maia 100 (aka Athena aka M100),"The largest individual deployment for the backend network for Maia is 2,048, but there is nothing preventing them from scaling this up further.",2023-11-15,,11.887143743334377,,,,,,,2048,Microsoft,,True,,51,18.214419939295734,1638400000000000000,,1638400000000000000,,2.1245952,,771158665895.508,,,,,,,Microsoft Azure Eagle,0.0574925607770476,https://archive.ph/Iigef,https://web.archive.org/web/20250211085121/https://semianalysis.com/2023/11/15/microsoft-infrastructure-ai-and-cpu/,,,,39.381266,-97.922211
Iris Energy Prince George cluster,Existing,Confirmed,Yes,18.20813595296026,816.0000000120199,NVIDIA H100 SXM5 80GB,816,Canada,Iris Energy,2024-04-30,,Private,1.1643340800000002,29026933.047607195,"Prince George, Canada",Cloud,H100,"""AI Cloud Services offered by Iris Energy cater to AI customers, providing cloud compute capabilities with 816 NVIDIA H100 GPUs. This service has been operational since 2024 and is characterized by a performance-focused technology stack.""",2024-04-30,,11.840984455358647,,,,,,,816,NVIDIA,,True,,77,17.90706206478202,1614864000000000000,1614864000000000000,807350400000000000,403675200000000000,1.1643340800000002,,693400986768.3336,29026933.047607195,,,,,,Meta GenAI 2024a,0.03320312500000011,https://www.globenewswire.com/en/news-release/2023/12/21/2799869/0/en/10-EH-s-Equity-Raising-Program-Complete.html,https://irisenergy.gcs-web.com/news-releases/news-release-details/iris-energy-purchases-nvidia-h100-gpus-target-generative-ai,https://iris-energy.co/blog/iris-energy-expands-partnership-with-poolside-for-ai-cloud-services/428,,,53.915792,-122.745087
Anonymized Chinese System,Existing,Confirmed,Unclear,18.30102999566398,800,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,18.30102999566398,800,,,China,,2024-01-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,56,18.30102999566398,1999999999999999700,,1999999999999999700,,,,,,,,,,,Microsoft Azure Eagle,0.05,,,,,,35.486703,101.901875
Argonne NL Polaris,Existing,Confirmed,Yes,18.145432608016584,706.2961091564309,NVIDIA A100,2240,United States of America,US Department of Energy,2021-11-01,,Public,1.9242495999999998,76476747.43904659,"9700 S Cass Ave, Lemont, IL 60439",Argonne National Laboratory and general scientific community,A100,"That includes a total of 560 AMD Epyc Milan CPUs and 2,240 Nvidia 40GB A100 GPUs, connected by HPE's Slingshot networking",2021-11-01,From gov website,11.560141207399278,,,,,,,2240,NVIDIA,,True,,13,17.844402612352606,1397760000000000000,1397760000000000000,698880000000000000,349440000000000000,1.9242495999999998,,363196125907.99036,76476747.43904659,,,,,,DeepSeek Fire-Flyer 2,0.2240000000000002,https://web.archive.org/web/20230512044406/https://www.hpcwire.com/2022/08/09/argonne-deploys-polaris-supercomputer-for-science-in-advance-of-aurora/,https://web.archive.org/web/20240530041315/https://www.top500.org/system/180016/,https://web.archive.org/web/20240228132238/https://www.datacenterdynamics.com/en/news/argonnes-44-petaflops-polaris-supercomputer-goes-live/,,,41.718369,-87.978791
NAVER Corp Sejong,Existing,Confirmed,Yes,18.145432608016584,706.2961091564309,NVIDIA A100,2240,Korea (Republic of),NAVER,2023-11-15,,Private,1.8590208,44191045.38038602,"824, Haengbok-daero, Sejong-si, South Korea","NAVER,Cloud",A100,Calculated from Top500,2023-11-15,,11.575118363368931,,,,,,,2240,NVIDIA,,True,,52,17.844402612352606,1397760000000000000,1397760000000000000,698880000000000000,349440000000000000,1.8590208,,375939849624.0602,44191045.38038602,,,,,,Microsoft Azure Eagle,0.04904834091291875,https://web.archive.org/web/20241109103411/https://www.top500.org/system/180222/,,,,,36.479996,127.306825
NVIDIA Selene Phase 1,Existing,Confirmed,Yes,18.145432608016584,706.2961091564309,NVIDIA A100,2240,United States of America,NVIDIA,2020-06-22,,Private,2.4075,61625087.03642732,"Santa Clara, California",NVIDIA,A100,"Altogether, Selene comprises 280 DGX A100s, housing a total 2,240 A100 GPUs and 494 Mellanox Quantum 200G InfiniBand switches, providing 56 TB/s network fabric",2020-06-22,,11.462836316556032,,NVIDIA Selene Phase 2,,,,,2240,NVIDIA,,True,,4,17.844402612352606,1397760000000000000,1397760000000000000,698880000000000000,349440000000000000,1.956864,2.4075,290292834890.96576,61625087.03642732,,,,,,Oak Ridge NL Summit,0.4044444444444445,https://web.archive.org/web/20240406153716/https://www.hpcwire.com/2020/06/22/nvidia-nabs-7-spot-on-top500-with-selene-launches-a100-pcie-cards/,https://web.archive.org/web/20241229072511/https://top500.org/system/179842/,,,,37.35231,-121.9619
Microsoft Azure Voyager-EUS2,Existing,Confirmed,Yes,18.119878503544196,665.9363314903499,NVIDIA A100,2112,United States of America,Microsoft,2021-11-01,,Private,1.81429248,72106647.58538677,"101 Herbert Dr, Boydton, VA 23917","Cloud,Azure",A100,Calculated from Top500,2021-11-01,Listed in Top500,11.560141207399278,,,,,,,2112,NVIDIA,,True,,15,17.818848507880215,1317888000000000000,1317888000000000000,658944000000000000,329472000000000000,1.81429248,,363196125907.9903,72106647.58538677,,,,,,DeepSeek Fire-Flyer 2,0.21120000000000042,https://web.archive.org/web/20240716100550/https://www.zdnet.com/article/microsoft-now-has-one-of-the-worlds-fastest-supercomputers-and-no-it-doesnt-run-on-windows/,https://web.archive.org/web/20240808114950/https://www.top500.org/system/180024/,,,,36.67676,-78.376502
Paper on LLaMA,Existing,Confirmed,Yes,18.10651454198622,645.756442657314,NVIDIA A100 SXM4 80 GB,2048,,Meta AI,2023-02-27,,Private,1.6996761599999999,45851025.559974864,,Meta,A100,"""Finally, we estimate that we used 2048 A100-80GB for a period of approximately 5 months to develop our models.""",2023-02-27,,11.575118363368931,,,True,Meta Research SuperCluster (RSC-1) Phase 1,,,2048,NVIDIA,,,,30,17.805484546322234,1277952000000000000,1277952000000000000,638976000000000000,319488000000000000,1.6996761599999999,,375939849624.0602,45851025.559974864,,,,,,Microsoft GPT-4 cluster,0.08192000000000015,https://arxiv.org/abs/2302.13971,,,,,,
Softbank SuperPOD,Existing,Confirmed,Yes,18.10651454198622,645.756442657314,NVIDIA A100,2048,Japan,Softbank,2023-09-30,"Estimated ""over 2,000"" as 2,048",Private,1.6996761599999999,40484720.946194254,Japan,Softbank,A100,"This Japan top-level*2 computing platform comprises an NVIDIA DGX SuperPOD(Open in a new window)™ AI supercomputer with over 2,000 NVIDIA Tensor Core GPUs(Open in a new window), NVIDIA Networking(Open in a new window) and NVIDIA AI Enterprise(Open in a new window) software",2023-09-30,,11.575118363368931,,,,,,,2048,NVIDIA,,True,,42,17.805484546322234,1277952000000000000,1277952000000000000,638976000000000000,319488000000000000,1.6996761599999999,,375939849624.0602,40484720.946194254,,,,,,Tesla 10k H100 Cluster,0.06457564426478053,https://web.archive.org/web/20240623042927/https://www.softbank.jp/en/corp/news/press/sbkk/2023/20231031_01/,,,,,36.386493,138.59223
Petrobras Pegasus (Pégaso),Existing,Confirmed,Yes,18.099675117455913,635.6664982407935,NVIDIA A100,2016,Brazil,Petrobras,2022-11-01,,Public/Private,1.8039,45553132.31061671,"Rio de Janeiro, Brazil",Petrobras,A100,"With 21 petaflops of power, 678TB of RAM memory, 2,016 GPUs, and a 400GB/s Internet link, Pegasus will be the most powerful supercomputer in Latin America.",2022-11-01,,11.542432663226768,,,,,,,2016,NVIDIA,,True,,28,17.798645121791928,1257984000000000000,1257984000000000000,628992000000000000,314496000000000000,1.7171481599999998,1.8039,348684516880.09314,45553132.31061671,,,,,,Microsoft GPT-4 cluster,0.08064000000000016,https://web.archive.org/web/20240423211648/https://www.datacenterdynamics.com/en/news/petrobras-announces-new-pegasus-supercomputer-in-rio-de-janeiro/,,,,,-22.909534,-43.209934
Azure OpenAI GPT-3 Cluster,Existing,Likely,Yes,18.096910013008053,631.6321374524499,NVIDIA V100,10000,United States of America,"Microsoft,OpenAI",2020-04-15,"It has ""since been upgraded to include tens of thousands of A100 chips"". Rumors suggest that it is in Washington State",Private,6.552,191957806.98237887,"1515 Port Industrial Way, Quincy, WA 98848",OpenAI,V100,"""The supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.""
""We understand the supercomputer uses AMD second-generation Rome Epyc server processors, and Nvidia V100 GPUs, from sources in the industry.""",2020-04,,11.280536124255693,,,,,,,10000,NVIDIA,,True,,4,18.096910013008053,1250000000000000000,,1250000000000000000,156700000000000000,6.552,,190781440781.4408,191957806.98237887,,,,True,,Oak Ridge NL Summit,0.36168981481481394,https://web.archive.org/web/20240902024137/https://news.microsoft.com/source/features/ai/openai-azure-supercomputer/,https://web.archive.org/web/20240307163254/https://www.theregister.com/2020/05/20/microsoft_openai_supercomputer/,,,,47.237925,-119.884972
ExxonMobil Discovery 5,Existing,Confirmed,Yes,18.096214585346402,630.6215260325272,NVIDIA A100 SXM4 40 GB,2000,United States of America,ExxonMobil,2022-11-01,,Private,1.6834,44379774.37352165,United States of America,ExxonMobil,A100,Calculated from Top500,2022-11-01,,11.568997266842544,,,,,,,2000,NVIDIA,,True,,30,17.795184589682425,1248000000000000000,1248000000000000000,624000000000000000,312000000000000000,1.70352,1.6834,370678388974.6941,44379774.37352165,,,,,,Microsoft GPT-4 cluster,0.07999999999999938,https://web.archive.org/web/20241109060139/https://www.top500.org/system/180131/,,,,,39.381266,-97.922211
ND A100 v4,Existing,Likely,Yes,18.096214585346402,630.6215260325272,NVIDIA A100,2000,United States of America,Microsoft,2021-06-01,"They ran the high performance LINPACK benchmark on a pool of 164 8xA100 VMs, so the clusters can include at least 1312 A100s. This might be a slice of a larger cluster.
Estimating as 2k since it says ""thousands""",Private,1.71808,53988631.725456126,,Cloud,A100,"""The ND A100 v4 series starts with a single VM and eight NVIDIA Ampere A100 40GB Tensor Core GPUs. ND A100 v4-based deployments can scale up to thousands of GPUs""",2021-06-01,,11.560141207399278,,,,,,,2000,NVIDIA,,True,,12,17.795184589682425,1248000000000000000,1248000000000000000,624000000000000000,312000000000000000,1.71808,,363196125907.9903,53988631.725456126,,,,,,Sunway OceanLight,0.2099092675264544,https://web.archive.org/web/20241114112532/https://azure.microsoft.com/de-de/blog/azure-announces-general-availability-of-scaleup-scaleout-nvidia-a100-gpu-instances-claims-title-of-fastest-public-cloud-super/,https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndasra100v4-series?tabs=sizebasic,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,17.999999999999996,600,,5000,China,,2019-12-15,,Public,3,,China,,,,,,11.367976785294593,,,,,,4000,9000,Anonymized,Anonymized,True,,4,17.845098040014257,999999999999999900,999999999999999900,700000000000000000,,3,,233333333333.33334,,,,,,,Oak Ridge NL Summit,0.3,,,,,,35.486703,101.901875
Corvex B200s,Planned,Likely,Yes,18.06145247908719,582.1121778761791,NVIDIA B200,256,United States of America,Corvex.ai,,,,0.5125120000000001,,Delaware,,,,Planned Q2 2025,,12.05071844496806,,,,,,,256,NVIDIA,,,,,17.76042248342321,1152000000000000000,1152000000000000000,576000000000000000,288000000000000000,0.5125120000000001,,1123876123876.1238,,,,,,,,,Private correspondence with Corvex.ai staff,,,,,38.997878,-75.505563
Google TPU v4 Pod,Existing,Confirmed,Yes,18.051692641798034,569.1763517011531,Google TPU v4,4096,United States of America,Google,2021-05-19,"Used in July 2021 MLPerf benchmark, mentioned May 2021, likely operational beforehand",Private,1.495416832,38153578.87978986,,Google,TPUv4,"""Google deploys the chips in its own data centers, combining them in pods of 4,096 TPUs""
""TPU v3. 4,096 of these TPU v4 chips are networked together to create a TPU v4 Pod""",2021-05-19,,11.876930377160788,,,,,,,4096,Google,,True,,13,18.051692641798034,1126400000000000000,1126400000000000000,1126400000000000000,,1.495416832,,753234801091.2317,38153578.87978986,,,,,,Sunway OceanLight,0.18945656966490257,https://web.archive.org/web/20250221082528/https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4,https://web.archive.org/web/20241005134423/https://www.datacenterdynamics.com/en/news/google-unveils-fourth-generation-tpu-ai-chip/,,,,39.381266,-97.922211
Paper on Gemma 2 9B,Existing,Confirmed,Yes,18.051692641798034,569.1763517011531,Google TPU v4,4096,United States of America,Google,2024-06-27,,Private,1.4193786880000003,36637468.85247168,,Google,TPUv4,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips,""",2024-06-27,,11.899594361796732,,,True,Google Oklahoma TPU v4 Pods,,,4096,Google,,,,105,18.051692641798034,1126400000000000000,1126400000000000000,1126400000000000000,,1.4193786880000003,,793586665435.4048,36637468.85247168,,,,,,Meta GenAI 2024a,0.02315984503958215,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,,39.381266,-97.922211
NVIDIA Taipei-1,Existing,Confirmed,Yes,18.04372371212676,558.8276907611325,NVIDIA H100 SXM5 80GB,512,Taiwan,NVIDIA,2024-06-01,,Private,0.8871116800000001,18279196.090486083,"Kaohsiung Software Technology Park
No. 10號, Fusing 4th Rd, Cianjhen District, Kaohsiung City, Taiwan 806","Cloud,NVIDIA","H100,L40","It will include 64 DGX H100 systems – each of which includes eight H100 GPUs (for a total of 512) – and 64 Omniverse-focused OVX systems, each of which includes four L40 GPUs and a BlueField-3 DPU...
the system is aimed at wide-ranging use cases like large language models (LLMs), healthcare, robotics, industrial digital twins and – like Taiwania 4 – climate science",2024-06-01,"Listed as operational in Top500, details of system confirmed earlier",11.79467520480357,,,,,NVIDIA L40,256,768,NVIDIA,NVIDIA,True,,98,17.742653502148947,1105920000000000000,1105920000000000000,552908800000000000,276459520000000000,0.8871116800000001,,623268538184.5045,18279196.090486083,,,,,,Meta GenAI 2024a,0.0227387569479534,https://web.archive.org/web/20240423000708/https://www.hpcwire.com/2023/05/29/nvidia-announces-four-supercomputers-with-two-in-taiwan/,,,,,22.604703,120.299511
Samsung SSC-21,Existing,Confirmed,Yes,18.040697257496575,554.9469429086291,NVIDIA A100,1760,Korea (Republic of),Samsung,2021-11-01,,Private,1.5119103999999999,60088872.98782232,Korea (Republic of),Samsung,A100,Calculated from Top500,2021-11-01,Listed in Top500,11.560141207399278,,,,,,,1760,NVIDIA,,True,,20,17.73966726183259,1098240000000000000,1098240000000000000,549120000000000000,274560000000000000,1.5119103999999999,,363196125907.99036,60088872.98782232,,,,,,DeepSeek Fire-Flyer 2,0.17600000000000166,"https://web.archive.org/web/20240117215326/https://www.kedglobal.com/artificial-intelligence/newsView/ked202305310010#:~:text=Samsung%20Electronics%20Co.%2C%20the%20world's,with%201.19%20exaflops%20of%20performance.",https://web.archive.org/web/20240717134459/https://www.top500.org/system/180041/,,,,,
Tesla Auto-Labeling Cluster,Existing,Likely,Yes,18.038718691514486,552.4244568044976,NVIDIA A100,1752,United States of America,Tesla,2021-08-20,I'm assuming these are A100s because H100s weren't out yet at the time and that is what the other cluster mentioned has,Private,1.5050380799999998,59918570.583158955,,Tesla,A100,"as well as the previously mentioned 5670 system which is used for training, it has a second, 4032 GPU system for training and a 1752 GPU system for auto-labeling",2021-08-20,"Number of GPUs officially announced, but no other details about it released",11.560141207399278,,,,,,,1752,NVIDIA,,True,,18,17.7376886958505,1093248000000000000,1093248000000000000,546624000000000000,273312000000000000,1.5050380799999998,,363196125907.99036,59918570.583158955,,,,,,DeepSeek Fire-Flyer 2,0.17520000000000122,https://web.archive.org/web/20240221025227/https://www.datacenterdynamics.com/en/news/tesla-details-dojo-supercomputer-reveals-dojo-d1-chip-and-training-tile-module/,,,,,39.381266,-97.922211
Tesla Dojo 1 Phase 1,Planned,Confirmed,Yes,18.03582982525283,548.7620010186947,Tesla D1 Dojo,3000,United States of America,Tesla,,,Private,2.4024,,"San Jose, California","Tesla,but ""“It's probably going to make more sense to have Dojo operate in like an Amazon Web Services manner than to try to sell it to someone else",Tesla Dojo D1,"Once complete, the entire system will house around 120 tiles, holding 3,000 custom D1 chips.",Planned,Musk plans for a scaled up version of this cluster to be operational by the end of 2024. Unclear if the smaller version ever ran,11.655184506061904,,,,,,,3000,Tesla,,,,,18.03582982525283,1086000000000000000,,1086000000000000000,67800000000000000,2.4024,,452047952047.952,,,,$1B,,,,,https://web.archive.org/web/20241208081438/https://www.datacenterdynamics.com/en/news/tesla-begins-installing-dojo-supercomputer-cabinets-trips-local-substation/,https://web.archive.org/web/20240903213050/https://techcrunch.com/2024/08/10/teslas-dojo-a-timeline/,https://web.archive.org/web/20250125054824/https://dgtlinfra.com/elon-musk-data-centers/,,,37.336157,-121.890608
DeepL Mercury,Existing,Confirmed,Yes,18.032044693904574,544.0000000080087,NVIDIA H100 SXM5 80GB,544,Sweden,DeepL,2023-08-02,,Private,0.79008384,19573215.020004388,"Slaggvarpsvägen 21, 791 77 Falun, Sweden",DeepL,H100,DeepL has added a DGX SuperPOD with 68 NVIDIA DGX H100 systems which are supplied by DELTA Computer Products,2023-08-02,,11.833297626692357,,,,,,,544,NVIDIA,,True,,46,17.73097080572634,1076576000000000000,1076576000000000000,538233600000000000,269116800000000000,0.79008384,,681236057175.9069,19573215.020004388,,,,,,Microsoft GPT-4 cluster,0.0690112820512816,https://web.archive.org/web/20241120225551/https://digests.digitalisationworld.com/news/65928/deepl-deploys-largest-nvidia-dgx-h100-superpod-in-europe,,,,,60.610563,15.610352
Paper on AlphaCode,Existing,Confirmed,Yes,18.01336396155798,521.0965133982759,Google TPU v4,3750,United States of America,Google,2022-02-08,,Private,1.3574925,34919722.83217412,,Google,TPUv4,,2022-02-08,,11.88062652272075,,,True,Google TPU v4 Pod,,,3750,Google,,,,24,18.01336396155798,1031250000000000000,1031250000000000000,1031250000000000000,,1.3574925,,759672705374.0629,34919722.83217412,,,,,,DeepSeek Fire-Flyer 2,0.1652644230769243,https://arxiv.org/abs/2203.07814,,,,,39.381266,-97.922211
Paper on Phi-3.5-Mini,Existing,Confirmed,Yes,18.005715755182223,512.0000000075355,NVIDIA H100 SXM5 80GB,512,,Microsoft,2024-04-22,,Private,0.7305625600000001,18212977.59849863,,Cloud,H100,,2024-04-22,,11.840984455358647,,,True,Microsoft Azure Eagle,,,512,NVIDIA,,,,94,17.704641867003993,1013248000000000000,1013248000000000000,506572800000000000,253286400000000000,0.7305625600000001,,693400986768.3336,18212977.59849863,,,,,,Meta GenAI 2024a,0.020833333333333145,https://arxiv.org/abs/2404.14219,,,,,,
Lawrence Livermore NL El Capitan Phase 1,Existing,Confirmed,Yes,18.00179184554997,507.3948458892298,AMD Instinct MI300A,512,United States of America,US Department of Energy,2024-06-01,,Public,0.793182208,20944115.35794477,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","US Government,Lawrence Livermore NL,Scientific Research",AMD MI300A,Calculated from Top500,2024-06-01,From Top500,11.80138888597411,,Lawrence Livermore NL El Capitan Phase 2,,,,,512,AMD,,True,,104,17.70076184988599,1004134400000000000,1004134400000000000,502067200000000000,251033600000000000,0.793182208,,632978393786.6644,20944115.35794477,,,,,,Meta GenAI 2024a,0.02064594913255849,https://web.archive.org/web/20240406154137/https://www.hpcwire.com/2022/06/21/amds-mi300-apus-to-power-exascale-el-capitan-supercomputer/,https://web.archive.org/web/20240517002417/https://en.wikipedia.org/wiki/El_Capitan_(supercomputer),https://web.archive.org/web/20240806222453/https://www.nextplatform.com/2023/07/10/lining-up-the-el-capitan-supercomputer-against-the-ai-upstarts/,https://web.archive.org/web/20240611014231/https://top500.org/system/180283/,,37.682018,-121.768374
Lawrence Livermore NL RZAdams,Existing,Confirmed,Yes,18.00179184554997,507.3948458892298,AMD Instinct MI300A,512,United States of America,US Department of Energy,2024-06-01,,Public,0.793182208,20944115.35794477,"7000 East Ave, Livermore, CA 94550","Lawrence Livermore NL,Researchers",AMD MI300A,Calculated from Top500,2024-06-01,,11.80138888597411,,,,,,,512,AMD,,True,,104,17.70076184988599,1004134400000000000,1004134400000000000,502067200000000000,251033600000000000,0.793182208,,632978393786.6644,20944115.35794477,,,,,,Meta GenAI 2024a,0.02064594913255849,https://web.archive.org/web/20250121085639/https://top500.org/system/180284/,,,,,37.680665,-121.7099
Anonymized Chinese System,Existing,Confirmed,Yes,17.999999999999996,500,,,China,,2023-01-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,39,17.999999999999996,1000000000000000100,,1000000000000000100,,,,,,,,,,,Microsoft GPT-4 cluster,0.06,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Confirmed,Yes,17.999999999999996,500,,,Hong Kong,,,,Private,,,Hong Kong,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.999999999999996,1000000000000000000,,1000000000000000100,,,,,,,,,,,,,,,,,,22.279356,114.16255
Anonymized Chinese System,Existing,Confirmed,Yes,17.999999999999996,500,,400,China,,2021-08-15,,Public,0.3,10000000,China,,,,,,11.522878745280337,,,,,,,2000,Anonymized,Anonymized,True,,17,17,1000000000000000100,1000000000000000100,100000000000000020,60000000000000000,0.3,,333333333333.3334,10000000,,,,,,DeepSeek Fire-Flyer 2,0.2,,,,,,35.486703,101.901875
Ahrefs Yep1,Existing,Confirmed,Yes,17.998876330651917,504.00000000741784,NVIDIA H100 SXM5 80GB,504,United States of America,Ahrefs,2024-06-01,,Private,0.71914752,17993583.65157224,"Ashburn, Virginia",Ahrefs,H100,Calculated from Top500,2024-06-01,,11.840984455358647,,,,,,,504,NVIDIA,,True,,108,17.697802442473687,997416000000000000,997416000000000000,498657600000000000,249328800000000000,0.71914752,,693400986768.3337,17993583.65157224,,,,,,Meta GenAI 2024a,0.020507812499999816,https://web.archive.org/web/20250212094644/https://top500.org/system/180265/,,,,,39.043555,-77.487449
Recursion BioHive-2,Existing,Confirmed,Yes,17.998876330651917,504.00000000741784,NVIDIA H100 SXM5 80GB,504,United States of America,Recursion Pharmaceuticals,2024-05-13,,Private,0.71914752,17928399.82352209,"41 400 W, Salt Lake City, UT 84101","Recursion Pharmaceuticals,Pharmaceutical Research",H100,BioHive-2 is equipped with 504 Nvidia H100 Tensor Core GPUs,2024-05-13,,11.840984455358647,,,,,,,504,NVIDIA,,True,,100,17.697802442473687,997416000000000000,997416000000000000,498657600000000000,249328800000000000,0.71914752,,693400986768.3337,17928399.82352209,,,,,,Meta GenAI 2024a,0.020507812499999816,https://web.archive.org/web/20240621074430/https://the-decoder.com/biohive-2-to-speed-up-drug-development-by-several-years/,,,,,40.767676,-111.902638
Yandex Chervonenkis,Existing,Confirmed,Yes,17.99712765308407,501.9747347218914,NVIDIA A100 SXM4 80 GB,1592,Russia,Yandex,2021-11-01,,Private,1.3675916799999999,65354449.21430119,"ул. Пушкина, 21, Sasovo, Ryazan Oblast, Russia, 391431",Yandex,A100,"GPU
1,592
NVIDIA A100 80G",2021-11-01,,11.560141207399278,,,,,,,1592,NVIDIA,,True,,23,17.696097657420093,993408000000000000,993408000000000000,496704000000000000,248352000000000000,1.3675916799999999,,363196125907.99036,65354449.21430119,,,,,,DeepSeek Fire-Flyer 2,0.15919999999999995,https://web.archive.org/web/20231115155013/https://yandex.com/supercomputers,,,,,54.373441,41.939594
BNY Mellon Supercomputer,Existing,Likely,Yes,17.995415798542414,500.0000000073623,NVIDIA H100 SXM5 80GB,500,,BNY Mellon,2024-03-18,"""Dozens"" probably means at least 36 and less than 100. Multiply this by 8 since it's DGXs. I estimated around 500 GPUs for this",Private,0.7134400000000001,17843147.256982166,,BNY Mellon,H100,"The system, equipped with dozens of NVIDIA DGX systems and NVIDIA InfiniBand networking",2024-03-18,Very uncertain about exact number,11.840984455358647,,,,,,,500,NVIDIA,,True,,85,17.69434191036418,989500000000000000,989500000000000000,494700000000000000,247350000000000000,0.7134400000000001,,693400986768.3336,17843147.256982166,,,,,,Meta GenAI 2024a,0.020345052083333287,https://web.archive.org/web/20240725023803/https://blogs.nvidia.com/blog/bny-mellon-superpod/,,,,,,
Saudi Aramco Dammam-7,Existing,Confirmed,Yes,17.99519629159718,499.7473471523815,NVIDIA Tesla V100 SXM2,7912,Saudi Arabia,Saudi Aramco,2021-01-21,,Private,5.0975433599999995,164171823.0132,"Dhahran, Saudi Arabia",Saudi Aramco,V100,Calculated from Top500,2021-01-21,Listed in Top500,11.287835362997193,,,,,,,7912,NVIDIA,,True,,10,17.99519629159718,989000000000000000,,989000000000000000,123981040000000000,5.0975433599999995,,194015024523.4991,164171823.0132,,,,,,Oak Ridge NL Summit,0.28616898148148256,https://web.archive.org/web/20240823132655/https://www.hpcwire.com/2021/01/21/saudi-aramco-unveils-dammam-7-its-new-top-ten-supercomputer/,https://web.archive.org/web/20240423152513/https://www.top500.org/system/179885/,,,,24.775309,46.648883
Anonymized Chinese System,Existing,Confirmed,Yes,17.999999999999996,500,,2000,China,,2021-12-15,,Private,1,,China,,,,,,11.698970004336019,,,True,,,,2000,Anonymized,Anonymized,,,25,17.698970004336015,1000000000000000100,1000000000000000100,500000000000000060,,1,,500000000000.00006,,,,,,,DeepSeek Fire-Flyer 2,0.2,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.999999999999996,500,,40000,China,,2020-11-15,,Public,30,,China,,,,,,10.522878745280337,,,,,,,40000,Anonymized,Anonymized,True,,8,17.999999999999996,1000000000000000100,,1000000000000000100,,30,,33333333333.333336,,,,,,,Oak Ridge NL Summit,0.3,,,,,,35.486703,101.901875
Condor Galaxy 3 (CG-3),Planned,Confirmed,Yes,17.98227123303957,485.0934815634862,Cerebras CS-3,64,United States of America,"G42,Cerebras Systems",,"Third of 9 planned supercomputers that will be interconnected
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,258201599.99999997,"Dallas, Texas",Cloud,Cerebras CS3,"""• 8 ExaFLOPs
• 64 x CS-3s
• 108 TB of Memory""
""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,Reported on their official website,,,,,,,,64,Cerebras,,,,,17.98227123303957,960000000000000000,,960000000000000000,,,,,258201599.99999997,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,32.781339,-96.799759
AGH Cyfronet Helios,Existing,Confirmed,Yes,17.962963774761278,464.00000000683394,NVIDIA GH200,440,Poland,AGH University of Krakow,2024-06-01,,Public,0.5297,21016071.518073153,"Nawojki 11, 30-950 Kraków, Poland","AGH University of Krakow,Researchers in Poland,Cloud","GH200,H100",The GPU partition contains 440 Nvidia GH200 Grace Hopper Superchips and the final INT partition is equipped with 24 Nvidia H100 Tensor Core GPUs and high-speed NVMe local storage.,2024-06-01,,11.937901535968024,,,,,NVIDIA H100 SXM5 80GB,24,464,NVIDIA,NVIDIA,True,,115,17.66193150890362,918256000000000000,918256000000000000,459125600000000000,229452800000000000,0.66207232,0.5297,866765338871.0591,21016071.518073153,,,,,,Meta GenAI 2024a,0.01888020833333336,https://web.archive.org/web/20240514095456/https://www.datacenterdynamics.com/en/news/hpe-to-build-new-35-petaflop-supercomputer-in-poland/,"https://web.archive.org/web/20240527154516/https://www.cyfronet.pl/en/news/19869,42,komunikat,helios__athena_and_ares_-_3_supercomputers_from_cyfronet_among_the_fastest_and_most_energy-efficient_computers_in_the_world.html",https://web.archive.org/web/20240806125635/https://top500.org/system/180244/,,,50.068984,19.909203
Eni HPC5,Existing,Confirmed,Yes,17.95904139232109,459.828196065384,NVIDIA V100,7280,Italy,Eni,2020-02-21,,Private,4.0337,139622268.26883802,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,V100,"In total, the system comprises 7,280 NVIDIA V100 GPUs.",2020-02-21,,11.353337797202265,,,,,,,7280,NVIDIA,,True,,5,17.95904139232109,910000000000000000,,910000000000000000,114077600000000000,4.769856,4.0337,225599325681.13644,139622268.26883802,,,,,,Oak Ridge NL Summit,0.2633101851851848,https://web.archive.org/web/20231120074252/https://energyindustryreview.com/tech/new-supercomputing-system-hpc5/,https://web.archive.org/web/20240906185041/https://en.wikipedia.org/wiki/HPC5,,,,45.101078,8.859647
CSIRO Virga,Existing,Confirmed,Yes,17.94772380820454,448.00000000659713,NVIDIA H100 SXM5 80GB,448,Australia,CSIRO,2024-07-11,,Public,0.6392422400000001,10935360.369755471,"54 Sheppard St, Hume ACT 2620, Australia","Researchers,Academia,Healthcare",H100,There are 448 H100 GPUs in total in Virga,2024-07-11,,11.840984455358647,,,,,,,448,NVIDIA,,True,,130,17.646649920026306,886592000000000000,886592000000000000,443251200000000000,221625600000000000,0.6392422400000001,,693400986768.3336,15965073.625045698,10850000,10935360.369755471,"""The system actually cost AU$16.3m (US$10.85m).""",,,Meta GenAI 2024a,0.018229166666666647,https://web.archive.org/web/20240720195802/https://blocksandfiles.com/2024/07/03/australia-dell-based-virga-ai-workload-cluster/,https://www.datacenterdynamics.com/en/news/australias-csiro-unveils-virga-supercomputer/,,,,-35.388934,149.167928
SURF Snellius Phase 3,Existing,Confirmed,Yes,17.94266272358471,442.80949975386926,NVIDIA H100 SXM5 80GB,352,Netherlands,SURF,2024-07-11,,Public,0.7370854400000001,18173029.00390342,"Science Park 120, 1098 XG Amsterdam, Netherlands","Academia,Researchers","H100,A100",The expansion includes the addition of 352 NVIDIA H100 (Hopper) GPUs,2024-07-11,,11.774080004600702,SURF Snellius Phase 1,,,,NVIDIA A100 SXM4 80 GB,288,640,NVIDIA,NVIDIA,True,,131,17.641597837059283,876320000000000000,876320000000000000,438124800000000000,219062400000000000,0.7370854400000001,,594401647657.0205,18173029.00390342,,,,,,Meta GenAI 2024a,0.01801796467070915,https://web.archive.org/web/20240712185633/https://www.surf.nl/en/news/gpu-expansion-of-supercomputer-snellius-enables-even-faster-data-processing-for-dutch,https://web.archive.org/web/20240725104223/https://visualization.surf.nl/snellius-virtual-tour/#/,,,,52.357104,4.950874
Anonymized Chinese System,Existing,Confirmed,Yes,17.939898470692583,400,,400,Hong Kong,,2024-04-15,,Public,0.6,20000000,Hong Kong,,,,,,11.823908740944317,,,,,,,400,Anonymized,Anonymized,True,,104,17.60205999132796,870760000000000000,870760000000000000,400000000000000060,199980000000000000,0.6,,666666666666.6667,20000000,,,,,,Meta GenAI 2024a,0.02,,,,,,22.279356,114.16255
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308998699194,400,,,China,,2024-03-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,91,17.90308998699194,800000000000000100,,800000000000000100,,,,,,,,,,,Meta GenAI 2024a,0.02,,,,,,35.486703,101.901875
Saudi Aramco Tuwaiq-1,Existing,Confirmed,Yes,17.915539124833135,416.00000000612255,NVIDIA H100 SXM5 80GB,416,Saudi Arabia,Saudi Aramco,2024-06-01,,Private,0.5935820800000001,14851846.823519947,Saudi Arabia,Saudi Aramco,H100,Calculated from Top500,2024-06-01,,11.840984455358647,,,,,,,416,NVIDIA,,True,,119,17.6144652366549,823264000000000000,823264000000000000,411590400000000000,205795200000000000,0.5935820800000001,,693400986768.3336,14851846.823519947,,,,,,Meta GenAI 2024a,0.016927083333333176,https://web.archive.org/web/20241205003221/https://top500.org/system/180260/,,,,,23.384784,44.652426
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308998699194,400,,2000,China,,2023-05-15,,Public/Private,1,,China,,,,,,11.602059991327963,,,,,,,2000,Anonymized,Anonymized,True,,49,17.60205999132796,800000000000000100,800000000000000100,400000000000000060,,1,,400000000000.00006,,,,,,,Microsoft GPT-4 cluster,0.05,,,,,,35.486703,101.901875
Microsoft Azure Pioneer-EUS,Existing,Confirmed,Yes,17.913118424722065,413.6877210773397,NVIDIA A100,1312,United States of America,Microsoft,2021-06-01,,Private,1.12706048,35416542.41189921,Virginia,"Cloud,Azure",A100,Calculated from Top500,2021-06-01,Listed in Top500,11.560141207399278,,,,,,,1312,NVIDIA,,True,,18,17.612088429058083,818688000000000000,818688000000000000,409344000000000000,204672000000000000,1.12706048,,363196125907.9903,35416542.41189921,,,,,,Sunway OceanLight,0.1377004794973547,https://web.archive.org/web/20241109163038/https://www.top500.org/system/179969/,,,,,37.677592,-78.619053
Microsoft Azure Pioneer-SCUS,Existing,Confirmed,Yes,17.913118424722065,413.6877210773397,NVIDIA A100,1312,United States of America,Microsoft,2021-06-01,,Private,1.12706048,35416542.41189921,Texas,"Cloud,Azure",A100,Calculated from Top500,2021-06-01,Listed in Top500,11.560141207399278,,,,,,,1312,NVIDIA,,True,,18,17.612088429058083,818688000000000000,818688000000000000,409344000000000000,204672000000000000,1.12706048,,363196125907.9903,35416542.41189921,,,,,,Sunway OceanLight,0.1377004794973547,https://web.archive.org/web/20241109090838/https://www.top500.org/system/179970/,,,,,31.803973,-98.822319
Microsoft Azure Pioneer-WEU,Existing,Confirmed,Yes,17.913118424722065,413.6877210773397,NVIDIA A100,1312,Netherlands,Microsoft,2021-06-01,,Private,1.12706048,35416542.41189921,Netherlands,"Cloud,Azure",A100,Calculated from Top500,2021-06-01,Listed in Top500,11.560141207399278,,,,,,,1312,NVIDIA,,True,,18,17.612088429058083,818688000000000000,818688000000000000,409344000000000000,204672000000000000,1.12706048,,363196125907.9903,35416542.41189921,,,,,,Sunway OceanLight,0.1377004794973547,https://web.archive.org/web/20241109142236/https://www.top500.org/system/179968/,,,,,51.972466,5.613491
Microsoft Azure Pioneer-WUS2,Existing,Confirmed,Yes,17.913118424722065,413.6877210773397,NVIDIA A100,1312,United States of America,Microsoft,2021-06-01,,Private,1.12706048,35416542.41189921,Washington,"Cloud,Azure",A100,Calculated from Top500,2021-06-01,Listed in Top500,11.560141207399278,,,,,,,1312,NVIDIA,,True,,18,17.612088429058083,818688000000000000,818688000000000000,409344000000000000,204672000000000000,1.12706048,,363196125907.9903,35416542.41189921,,,,,,Sunway OceanLight,0.1377004794973547,https://web.archive.org/web/20241109144557/https://www.top500.org/system/179967/,,,,,48.025005,-120.094293
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308998699194,400,,,China,,2021-07-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,24,,800000000000000100,800000000000000100,,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.1,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308998699194,400,,30000.000000000004,China,,2022-04-15,,Public/Private,,,China,,,,,,,,,,,,,30000.000000000004,Anonymized,Anonymized,True,,36,17.90308998699194,800000000000000100,,800000000000000100,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.1,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308998699194,400,,,China,,2018-03-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,2,17.90308998699194,800000000000000100,,800000000000000100,,,,,,,,,,,Meta 2017 V100 Cluster,0.3,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308998699194,400,,400,China,,2023-08-15,,,0.6,10000000,China,,,,,,11.823908740944317,,,,,,,400,Anonymized,Anonymized,True,,63,17.60205999132796,800000000000000100,800000000000000100,400000000000000060,200000000000000030,0.6,,666666666666.6667,10000000,,,,,,Microsoft GPT-4 cluster,0.05,,,,,,35.486703,101.901875
Princeton Della Phase 2,Existing,Confirmed,Yes,17.893744012750904,395.6382011174993,NVIDIA H100 SXM5 80GB,296,United States of America,Princeton University,2024-03-15,,Public,0.6800102400000001,16807979.653981995,"300 Forrestal Rd, Princeton, NJ 08544",Academia,"H100,A100",Princeton University has invested in a new cluster of 300 Nvidia H100 GPUs to support AI research at the institution.,2024-03-15,,11.76016572633548,Princeton Della Phase 1,,,,NVIDIA A100 SXM4 80 GB,316,612,NVIDIA,NVIDIA,True,,98,17.592681178956436,782968000000000000,782968000000000000,391454400000000000,195727200000000000,0.6800102400000001,,575659566538.292,16807979.653981995,,,,,,Meta GenAI 2024a,0.01609855961554658,https://web.archive.org/web/20240320225612/https://www.datacenterdynamics.com/en/news/princeton-university-acquires-300-strong-nvidia-h100-gpu-cluster/,https://web.archive.org/web/20240521031936/https://researchcomputing.princeton.edu/systems/della,,,,40.34444,-74.61551
Anonymized Chinese System,Existing,Confirmed,Yes,17.845098040014257,400,,1000,China,,2023-06-15,,,1,30000000,China,,,,,,11.602059991327963,,,,,,,1000,Anonymized,Anonymized,True,,61,17.60205999132796,700000000000000000,700000000000000000,400000000000000060,200000000000000030,1,,400000000000.00006,30000000,,,,,,Microsoft GPT-4 cluster,0.05,,,,,,35.486703,101.901875
Microsoft Explorer-WUS3,Existing,Confirmed,Yes,17.866500002672172,371.58160687762984,AMD Radeon Instinct MI250X,1920,United States of America,Microsoft,2023-06-01,Chip count from Top500,Private,1.9918079999999998,48843651.94102182,"Phoenix, Arizona","Cloud,Azure",AMD MI250X,"""The highest ranked new entry to the June 2023 Top500 list is #11 - a #Microsoft #Azure #AI supercomputer with nearly 2000 #AMD MI250X #GPUs and InfiniBand HDR interconnect: https://lnkd.in/e2WrGDGa""
""The cluster was spun up in Azure’s West US3 datacenter, and Microsoft has confirmed it is a permanent system that has or will run real HPC and AI workloads""
Chip count from Top500",2023-06-01,,11.567252530311055,,,,,,,1920,AMD,,True,,60,17.866500002672172,735360000000000000,735360000000000000,735360000000000000,183744000000000000,1.9918079999999998,,369192211297.4745,48843651.94102182,,,,,,Microsoft GPT-4 cluster,0.04713846153846146,https://web.archive.org/web/20240406153948/https://www.hpcwire.com/2023/05/22/top500-frontier-gains-92-petaflops-henri-gets-a-little-greener/,https://archive.ph/83Jbr,https://web.archive.org/web/20241109085342/https://www.top500.org/system/180171/,,,33.44823,-112.075098
Naver DGX Superpod,Existing,Confirmed,Yes,17.844402612352606,353.1480545782183,NVIDIA A100,1120,Korea (Republic of),NAVER,2020-10-05,,Private,0.978432,30365543.163141362,,NAVER,A100,NAVER CLOVA is using its DGX SuperPOD built with 140 DGX A100,2020-10-05,,11.55284196865778,,,,,,,1120,NVIDIA,,True,,10,17.543372616688625,698880000000000000,698880000000000000,349440000000000000,174720000000000000,0.978432,,357142857142.8571,30365543.163141362,,,,,,Oak Ridge NL Summit,0.20222222222222389,https://web.archive.org/web/20240825043935/https://nvidianews.nvidia.com/news/nvidia-announces-ready-made-nvidia-dgx-superpods-offered-by-global-network-of-certified-partners,,,,,36.536236,128.168944
University of Florida HiPerGator 3.0 Superpod,Existing,Confirmed,Yes,17.844402612352606,353.1480545782183,NVIDIA A100,1120,United States of America,University of Florida,2021-01-31,,Public,1.0261,30312641.52,"Gainesville, FL 32611",University of Florida,A100,"UF’s HiPerGator 3 supercomputer will integrate 140 NVIDIA DGX A100 systems powered by a combined 1,120 NVIDIA A100 Tensor Core GPUs",2021-01-31,,11.532182929078632,,,,,,,1120,NVIDIA,,True,,14,17.543372616688625,698880000000000000,698880000000000000,349440000000000000,174720000000000000,0.9621247999999999,1.0261,340551603157.587,30312641.52,,,,,,Oak Ridge NL Summit,0.20222222222222389,https://web.archive.org/web/20240521065742/https://blogs.nvidia.com/blog/university-of-florida-nvidia-ai-supercomputer/,,,,,29.649763,-82.343513
PCSS Poznan Proxima,Existing,Confirmed,Yes,17.838025038152974,348.0000000051227,NVIDIA H100 SXM5 80GB,348,Poland,Polish Academy of Sciences,2024-06-01,,Public,0.49655424000000004,12424141.09275226,"Poznań, Poland",Academia,H100,Calculated from Top500,2024-06-01,,11.840984455358647,,,,,,,348,NVIDIA,,True,,134,17.53695114997474,688692000000000000,688692000000000000,344311200000000000,172155600000000000,0.49655424000000004,,693400986768.3336,12424141.09275226,,,,,,Meta GenAI 2024a,0.014160156249999906,https://web.archive.org/web/20250121170241/https://top500.org/system/180290/,,,,,52.407333,16.93123
Yandex Lyapunov,Existing,Confirmed,Yes,17.83499514383077,345.5805962658242,NVIDIA A100 SXM4 40 GB,1096,Russia,Yandex,2021-11-01,,Private,0.94150784,29515251.556721993,"ул. Пушкина, 21, Sasovo, Ryazan Oblast, Russia, 391431",Yandex,A100,"GPU
1,096
NVIDIA A100 40G",2021-11-01,,11.560141207399278,,,,,,,1096,NVIDIA,,True,,35,17.533965148166793,683904000000000000,683904000000000000,341952000000000000,170976000000000000,0.94150784,,363196125907.99036,29515251.556721993,,,,,,DeepSeek Fire-Flyer 2,0.1095999999999998,https://web.archive.org/web/20231115155013/https://yandex.com/supercomputers,,,,,54.373441,41.939594
Yandex Galushkin,Existing,Confirmed,Yes,17.831813485044584,343.05811016169656,NVIDIA A100 SXM4 80 GB,1088,Russia,Yandex,2021-11-01,,Private,0.9346355199999999,44664347.20173348,"Ulitsa Energetikov, 34, Vladimir, Vladimir Oblast, Russia, 600902",Yandex,A100,"GPU
1,088
NVIDIA A100 80G",2021-11-01,,11.560141207399278,,,,,,,1088,NVIDIA,,True,,36,17.530783489380603,678912000000000000,678912000000000000,339456000000000000,169728000000000000,0.9346355199999999,,363196125907.99036,44664347.20173348,,,,,,DeepSeek Fire-Flyer 2,0.10880000000000058,https://web.archive.org/web/20231115155013/https://yandex.com/supercomputers,,,,,56.127846,40.40194
NVIDIA SATURN V Phase 2,Existing,Likely,Yes,17.819543935541866,333.5017685748941,NVIDIA V100,5280,United States of America,NVIDIA,2018-03-26,"This was first mentioned November 2017, but it appears to not be operational yet at that time
A later version of the supercomputer was actually four clustesrs, but its unclear if this was",Private,3.6324288,104199705.225,,NVIDIA,V100,"If you look at our Saturn V Supercomputer add-in video which we use for our internal training, that has 5,280 GPUs",2018-03-26,,11.259346825185755,NVIDIA SATURN V Phase 1,NVIDIA SATURN V Phase 3,,,,,5280,NVIDIA,,True,,3,17.819543935541866,660000000000000000,,660000000000000000,82737600000000000,3.6324288,,181696610268.03885,104199705.225,,,,,,Meta 2017 V100 Cluster,0.24000000000000019,https://web.archive.org/web/20240528104941/https://insidehpc.com/2018/04/inside-new-nvidia-dgx-2-supercomputer-nvswitch/,https://top500.org/system/178928/,,,,39.381266,-97.922211
SK Telecom Titan Phase 2,Existing,Confirmed,Yes,17.8122179289812,327.92319353691437,NVIDIA A100,1040,Korea (Republic of),SK Telecom,2023-02-14,,Private,0.8631167999999999,23330676.095516853,Korea (Republic of),SK Telecom,A100,"by increasing its capacity to 1,040 NVIDIA A100 GPUs",2023-02-14,,11.575118363368931,,,,,,,1040,NVIDIA,,True,,58,17.51118793331722,648960000000000000,648960000000000000,324480000000000000,162240000000000000,0.8631167999999999,,375939849624.0602,23330676.095516853,,,,,,Microsoft GPT-4 cluster,0.04159999999999971,https://web.archive.org/web/20240725135908/https://insidehpc.com/2023/02/sk-telecom-doubles-capacity-of-titan-supercomputer/,,,,,,
Nemotron-3-8B training cluster,Existing,Confirmed,Yes,17.805484546322234,322.8782213286543,NVIDIA A100,1024,,NVIDIA,2024-04-04,,Private,0.8349286400000001,20047992.401498884,,NVIDIA,A100,"""1,024 A100s were used for 19 days to train the model""",2024-04-04,,11.582805192035222,,,True,NVIDIA Selene Phase 2,,,1024,NVIDIA,,,,117,17.504454550658252,638976000000000000,638976000000000000,319488000000000000,159744000000000000,0.8349286400000001,,382653061224.48975,20047992.401498884,,,,,,Meta GenAI 2024a,0.013137948458817517,https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k,,,,,,
Paper on HyperCLOVA,Existing,Confirmed,Yes,17.805484546322234,322.8782213286543,NVIDIA A100,1024,Korea (Republic of),NAVER,2021-09-10,,Private,0.8796569599999999,35020899.70157236,,NAVER,A100,"""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""",2021-09-10,,11.560141207399278,,,True,Naver DGX Superpod,,,1024,NVIDIA,,,,32,17.504454550658252,638976000000000000,638976000000000000,319488000000000000,159744000000000000,0.8796569599999999,,363196125907.99036,35020899.70157236,,,,,,DeepSeek Fire-Flyer 2,0.10240000000000014,https://arxiv.org/abs/2109.04650,,,,,36.536236,128.168944
Gcore data center Phase 1,Existing,Confirmed,Yes,17.8015957725263,320.00000000471215,NVIDIA H100 SXM5 80GB,320,Korea (Republic of),"Gcore,NHN Corporation",2024-04-15,,Private,0.45660160000000005,11422779.041436069,"Incheon, South Korea",Cloud,H100,"Gcore will initially have the data center equipped with 320 Nvidia H100 GPUs stacked onto 40 servers. Each server also has 2 terabytes of data storage and 112 CPU cores, with a bandwidth of 3.2 Tbps.",2024-04-15,,11.840984455358647,,,,,,,320,NVIDIA,,True,,120,17.500521884348068,633280000000000000,633280000000000000,316608000000000000,158304000000000000,0.45660160000000005,,693400986768.3336,11422779.041436069,,,,,,Meta GenAI 2024a,0.013020833333333315,https://web.archive.org/web/20240723133814/https://www.datacenterdynamics.com/en/news/gcore-expands-into-south-korea-with-nhn-cloud/,,,,,37.456,126.7052
Paper on OPT,Existing,Confirmed,Yes,17.791696261836602,312.7882769121352,NVIDIA A100 SXM4 80 GB,992,,Meta AI,2022-05-02,,Private,0.8449459199999999,22320916.847170066,,Meta,A100,"""We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs,""",2022-05-02,,11.563837352959242,,,True,Meta Research SuperCluster (RSC-1) Phase 1,,,992,NVIDIA,,,,46,17.49066626617262,619008000000000000,619008000000000000,309504000000000000,154752000000000000,0.8449459199999999,,366300366300.36633,22320916.847170066,,,,,,Microsoft GPT-4 cluster,0.03967999999999991,https://arxiv.org/abs/2205.01068,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,17.778151250383644,300,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.778151250383644,600000000000000000,,600000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Wroclaw Centre for Networking and Supercomputing Lem,Existing,Confirmed,Yes,17.779319377815145,304.0000000044739,NVIDIA H100 SXM5 80GB,304,Poland,Wroclaw Tech (Wrocław University of Science and Technology),2024-06-01,,Public,0.43377152,10853272.678726114,"plac Grunwaldzki 9, 50-366 Wrocław, Poland","Academia,Researchers",H100,Calculated from Top500,2024-06-01,,11.840984455358647,,,,,,,304,NVIDIA,,True,,143,17.478245489636915,601616000000000000,601616000000000000,300777600000000000,150388800000000000,0.43377152,,693400986768.3336,10853272.678726114,,,,,,Meta GenAI 2024a,0.012369791666666543,https://web.archive.org/web/20250121115648/https://top500.org/system/180272/,,,,,51.109827,17.057534
AIST ABCI 2.0,Existing,Confirmed,Yes,17.77745582272199,302.69833249561333,NVIDIA A100 SXM4 40 GB,960,Japan,National Institute of Advanced Industrial Science and Technology (AIST),2021-05-30,"Builds off of ABCI 1.0, which was first operational in August 2018",Public,2.8185,115981977.91473988,"Kashiwa Campus, The University of Tokyo
Japan, 〒277-0882 Chiba, Kashiwa, Kashiwanoha, 5 Chome−１−5","industry,academia,and government in Japan","A100,V100","120 Compute Nodes (A) that form in total 960 NVIDIA A100 GPU accelerators, 1,088 Compute Nodes (V) that form in total 4,352 NVIDIA GPU V100 accelerators",2021-05-30,Info from detailed official spec sheet,11.47607734508567,AIST ABCI 1.0,AIST ABCI 3.0,,,NVIDIA Tesla V100 SXM2,4352,5312,NVIDIA,NVIDIA,True,,21,17.92609538424188,599040000000000000,599040000000000000,843520000000000000,217955840000000000,3.62858496,2.8185,299279758736.9168,115981977.91473988,,,,,,Sunway OceanLight,0.10075644841269821,https://web.archive.org/web/20240901124715/https://abci.ai/en/about_abci/computing_resource.html,,,,,35.68882,139.692526
Anonymized Chinese System,Existing,Confirmed,Yes,17.778151250383644,300,,20000,China,,2020-09-15,,Public,20,,China,,,,,,10.477121254719663,,,,,,,20000,Anonymized,Anonymized,True,,10,17.778151250383644,600000000000000000,,600000000000000000,,20,,30000000000,,,,,,,Oak Ridge NL Summit,0.2,,,,,,35.486703,101.901875
NEC Corp Japan Supercomputer,Existing,Confirmed,Yes,17.762732565901285,292.60838807909363,NVIDIA A100 SXM4 80 GB,928,Japan,NEC Corporation,2023-07-24,,Private,0.7701657599999999,20652087.220453434,Japan,NEC,A100,"116 GPU servers, each equipped with 8 NVIDIA A100 80 GB Tensor Core GPUs per node",2023-07-24,,11.575118363368931,,,,,,,928,NVIDIA,,True,,73,17.461702570237303,579072000000000000,579072000000000000,289536000000000000,144768000000000000,0.7701657599999999,,375939849624.0602,20652087.220453434,,,,,,Microsoft GPT-4 cluster,0.03711999999999985,https://web.archive.org/web/20240314221209/https://www.nec.com/en/global/rd/aisupercomputer/index.html,,,,,36.386493,138.59223
Paper on CoCa,Existing,Confirmed,Yes,17.750662646134053,284.58817585057653,Google TPU v4,2048,United States of America,Google,2022-05-04,,Private,0.741371904,18738807.132278845,,Google,TPUv4,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""",2022-05-04,,11.88062652272075,,,True,Paper on PaLM,,,2048,Google,,,,49,17.750662646134053,563200000000000000,563200000000000000,563200000000000000,,0.741371904,,759672705374.0629,18738807.132278845,,,,,,Microsoft GPT-4 cluster,0.03610256410256384,https://arxiv.org/abs/2205.01917v2,,,,,39.381266,-97.922211
AIST ABCI 1.0,Existing,Confirmed,Yes,17.735598899698175,274.8863062193055,NVIDIA Tesla V100 SXM2,4352,Japan,National Institute of Advanced Industrial Science and Technology (AIST),2018-06-01,,Public,2.99400192,177109248.86877826,"Kashiwa Campus, The University of Tokyo
Japan, 〒277-0882 Chiba, Kashiwa, Kashiwanoha, 5 Chome−１−5","industry,academia,and government in Japan",V100,The original ABCI system 4352 Nvidia V100 GPUs,2018-06-01,Confirmed by Top500,11.259346825185755,,AIST ABCI 2.0,,,,,4352,NVIDIA,,True,,5,17.735598899698175,544000000000000000,,544000000000000000,68195840000000000,2.99400192,,181696610268.03885,93816848.25718153,162000000,177109248.86877826,"$172 for machine, prototype, and the associated datacenter (which was $10M), subtracting out cost of datacenter gives $162",,,Meta 2017 V100 Cluster,0.19781818181818114,https://web.archive.org/web/20240421015736/https://www.nextplatform.com/2021/07/08/inside-look-inside-japans-abci-ai-supercomputer-upgrade/,,,,,35.68882,139.692526
GENCI Adastra,Existing,Confirmed,Yes,17.714175465574236,261.6553815096622,AMD Radeon Instinct MI250X,1352,France,GENCI,2022-06-01,,Public,1.6092,34758841.57027074,"Montpellier, France","Academic research in Europe,Industry in France",AMD MI250X,Adastra contains a total of 1352 AMD MI250X on its 338 accelerated nodes.,2022-06-01,Listed in Top500,11.507565441675016,,,,,,,1352,AMD,,True,,54,17.714175465574236,517816000000000000,517816000000000000,517816000000000000,129386400000000000,1.4394744,1.6092,321784737757.8921,34758841.57027074,,,,,,Microsoft GPT-4 cluster,0.033193333333333006,https://web.archive.org/web/20240411122725/https://dci.dci-gitlab.cines.fr/webextranet/architecture/index.html,,,,,43.610073,3.87727
Anonymized Chinese System,Existing,Confirmed,Yes,17.698970004336015,300,,2000,China,,2023-08-15,,,1,,China,,,,,,11.698970004336019,,,,,,,2000,Anonymized,Anonymized,True,,80,17.698970004336015,500000000000000060,,500000000000000060,300000000000000000,1,,500000000000.00006,,,,,,,Microsoft GPT-4 cluster,0.03,,,,,,35.486703,101.901875
MITRE Federal AI Sandbox,Planned,Confirmed,Yes,17.704685759518245,256.0000000037698,NVIDIA H100 SXM5 80GB,256,United States of America,MITRE,,,Public/Private,0.35875840000000003,12032125.831869628,"Ashburn, Virginia","MITRE,US government agencies",H100,The organization's upcoming Federal AI Sandbox is basically an Nvidia DGX SuperPOD – a modular supercomputer built from 256 Nv H100 GPUs,Planned Q4 2024,,11.848809792870606,,,,,,,256,NVIDIA,,,,,17.403611871340008,506624000000000000,506624000000000000,253286400000000000,126643200000000000,0.35875840000000003,,706008277436.8489,12032125.831869628,,,,,,,,https://web.archive.org/web/20240927024647/https://www.theregister.com/2024/05/08/mitre_ai_computer/,,,,,39.043555,-77.487449
MPT-30B training cluster,Existing,Confirmed,Yes,17.704685759518245,256.0000000037698,NVIDIA H100 SXM5 80GB,256,,MosaicML,2023-06-20,,,0.37180416,9220185.836846774,,,H100,"""Training was completed on 256 H100-80GBs""",2023-06-20,,11.833297626692357,,,,,,,256,NVIDIA,,True,,75,17.403611871340008,506624000000000000,506624000000000000,253286400000000000,126643200000000000,0.37180416,,681236057175.9069,9220185.836846774,,,,,,Microsoft GPT-4 cluster,0.032475897435897357,https://huggingface.co/mosaicml/mpt-30b/commits/main,,,,,,
Paper on Falcon Mamba 7B,Existing,Confirmed,Yes,17.704685759518245,256.0000000037698,NVIDIA H100 SXM5 80GB,256,,Technology Innovation Institute,2024-07-24,Possibly used cloud,Public,0.36528128000000004,9130592.913654234,,Technology Innovation Institute,H100,"""Falcon-Mamba-7B was trained on 256 H100 80GB GPUs""",2024-07-24,,11.840984455358647,,,True,,,,256,NVIDIA,,,,168,17.403611871340008,506624000000000000,506624000000000000,253286400000000000,126643200000000000,0.36528128000000004,,693400986768.3336,9130592.913654234,,,,,,Meta GenAI 2024a,0.010416666666666656,https://huggingface.co/tiiuae/falcon-mamba-7b/tree/main,,,,,,
Google MLPerf 0.7 Submission,Existing,Confirmed,Yes,17.70226505940717,254.57705912451675,Google TPU v3,4096,United States of America,Google,2020-07-29,,Private,1.9680460800000001,39128671.257303536,,Google,TPU v3,From MLPerf 0.7,2020-07-29,,11.408229796584491,,,,,,,4096,Google,,True,,11,17.70226505940717,503808000000000000,,503808000000000000,,1.9680460800000001,,255994005994.00598,39128671.257303536,,,,,,Oak Ridge NL Summit,0.14577777777777834,https://web.archive.org/web/20250217182530/https://github.com/mlcommons/training_results_v0.7/blob/master/all_hyper_params.md,,,,,39.381266,-97.922211
Paper on Gopher,Existing,Confirmed,Yes,17.70226505940717,254.57705912451675,Google TPU v3,4096,United States of America,Google DeepMind,2021-12-08,4096 TPUV3 chips reported in table A27,Private,1.9352453120000002,38807760.926564634,"Georgia, USA",Google,TPU v3,,2021-12-08,,11.41552903532599,,,True,Google MLPerf 0.7 Submission,,,4096,Google,,,,42,17.70226505940717,503808000000000000,,503808000000000000,,1.9352453120000002,,260332887451.5315,38807760.926564634,,,,,,DeepSeek Fire-Flyer 2,0.08073846153846191,https://arxiv.org/abs/2112.11446,,,,,32.330571,-83.257256
Anonymized Chinese System,Existing,Likely,Yes,17.698970004336015,300,,,China,,2023-12-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,108,17.698970004336015,500000000000000060,,500000000000000060,,,,,,,,,,,Microsoft Azure Eagle,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.698970004336015,300,,,China,,2024-03-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,117,17.698970004336015,500000000000000060,,500000000000000060,,,,,,,,,,,Meta GenAI 2024a,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,17.698970004336015,300,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.698970004336015,500000000000000060,,500000000000000060,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.698970004336015,300,,,China,,2021-07-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,32,17.698970004336015,500000000000000060,,500000000000000060,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.08,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Confirmed,Yes,17.698970004336015,300,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,500,Anonymized,Anonymized,,,,17.698970004336015,500000000000000060,,500000000000000060,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Confirmed,Yes,17.698970004336015,300,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.698970004336015,500000000000000060,,500000000000000060,,,,,,,,,,,,,,,,,,35.486703,101.901875
EuroHPC MeluXina,Existing,Confirmed,Yes,17.698274576674365,252.24861041301094,NVIDIA A100,800,Luxembourg,EuroHPC JU,2021-06-07,,Public,0.6873,37543482.02606429,"1 ZA et Commerciale Klengbusbierg 7795, 7795 Bissen, Luxembourg","European researchers,Cloud",A100,"200 GPU nodes are part of the Accelerator Module, each featuring 2 AMD Rome CPUs (32 cores @ 2.35 GHz - 128HT cores total) and 4 NVIDIA A100-40 GPUs",2021-06-07,,11.560098237101325,,,,,,,800,NVIDIA,,True,,30,17.397244581010387,499200000000000000,499200000000000000,249600000000000000,124800000000000000,0.687232,0.6873,363160192055.8708,21595452.69018245,35770200,37543482.02606429,"""MeluXina is funded via a joint investment of about EUR 30 million from the European Union and Luxembourg""",,,Sunway OceanLight,0.08396370701058178,https://web.archive.org/web/20240406154520/https://www.hpcwire.com/off-the-wire/meluxina-named-eus-greenest-supercomputer/,https://web.archive.org/web/20241014163423/https://docs.lxp.lu/system/overview/,https://eurohpc-ju.europa.eu/meluxina-new-eurohpc-world-class-supercomputer-luxembourg-2020-09-29_en,,,49.786299,6.090425
Paper on YaLM,Existing,Likely,Yes,17.698274576674365,252.24861041301094,NVIDIA A100,800,,Yandex,2022-06-23,,,0.681408,18062089.084416587,,,A100,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards""",2022-06-23,,11.563837352959242,,,True,Yandex Chervonenkis,,,800,NVIDIA,,,,58,17.397244581010387,499200000000000000,499200000000000000,249600000000000000,124800000000000000,0.681408,,366300366300.3663,18062089.084416587,,,,,,Microsoft GPT-4 cluster,0.03199999999999976,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,,,,,
KT SuperPOD,Existing,Confirmed,Yes,17.693909771271915,249.7261243088812,NVIDIA A100,792,Korea (Republic of),KT,2023-06-01,,Private,0.6572966399999999,17686696.136917263,Korea (Republic of),KT,A100,Calculated from Top500,2023-06-01,,11.575118363368931,,,,,,,792,NVIDIA,,True,,79,17.392879775607934,494208000000000000,494208000000000000,247104000000000000,123552000000000000,0.6572966399999999,,375939849624.0602,17686696.136917263,,,,,,Microsoft GPT-4 cluster,0.03167999999999981,https://web.archive.org/web/20241205015244/https://top500.org/system/180163/,,,,,,
SberCloud Christofari Neo,Existing,Confirmed,Yes,17.693909771271915,249.7261243088812,NVIDIA A100 SXM4 80 GB,792,Russia,SberCloud,2021-11-01,"SberCloud, which owns this supercomputer, was originally owned by Sber, but was sold off to Noviye Vozmozhnosti in March 2022
Sberbank is majority owned by the Russian Government.",Private,0.6803596799999999,32513017.448320694,Russia,Cloud,A100,Calculated from Top500,2021-11-01,,11.560141207399278,,,,,,,792,NVIDIA,,True,,45,17.392879775607934,494208000000000000,494208000000000000,247104000000000000,123552000000000000,0.6803596799999999,,363196125907.99036,32513017.448320694,,,,,,DeepSeek Fire-Flyer 2,0.07920000000000015,https://web.archive.org/web/20230517125435/https://en.wikipedia.org/wiki/Christofari,,,,,61.994573,96.669705
Cineca Marconi-100,Existing,Confirmed,Yes,17.693726948923647,249.62102072121044,NVIDIA V100,3952,Italy,Cineca,2020-04-20,,Public,2.6436,75928622.78444445,"Bologna, Italy",Academic research in Europe,V100,"Nodes: 980
Processors: 2x16 cores IBM POWER9 AC922 at 2.6(3.1) GHz
Accelerators: 4 x NVIDIA Volta V100 GPUs/node, Nvlink 2.0, 16GB",2020-04-20,,11.271531205725253,,,,,,,3952,NVIDIA,,True,,10,17.693726948923647,494000000000000000,,494000000000000000,61927840000000000,2.5893504,2.6436,186866394310.78833,75928622.78444445,,,,,,Oak Ridge NL Summit,0.14293981481481574,https://web.archive.org/web/20240523132015/https://wiki.u-gov.it/confluence/pages/viewpage.action?pageId=336727645,https://web.archive.org/web/20240520090110/https://www.top500.org/system/179845/,,,,44.495735,11.344672
Anonymized Chinese System,Existing,Confirmed,Yes,17.698970004336015,200,,1000,China,,2022-05-15,,Public/Private,0.6,,China,,,,,,11.522878745280337,,,,,,,1000,Anonymized,Anonymized,True,,58,17.30102999566398,500000000000000060,500000000000000060,200000000000000030,,0.6,,333333333333.3334,,,,,,,Microsoft GPT-4 cluster,0.03,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.698970004336015,200,,1000,China,,2021-09-15,,Public/Private,0.6,,China,,,,,,11.698970004336019,,,,,,,1000,Anonymized,Anonymized,True,,39,17.47712125471966,500000000000000060,500000000000000060,300000000000000000,,0.6,,500000000000,,,,,,,DeepSeek Fire-Flyer 2,0.08,,,,,,35.486703,101.901875
Fastweb NeXXt AI Factory,Existing,Confirmed,Yes,17.690897475032614,248.0000000036531,NVIDIA H100 SXM5 80GB,248,Italy,Fastweb,2024-07-09,31x8=248,Private,0.35386624000000005,8837808.613864584,"Via di San Clemente, 53 24036 Ponte San Pietro Bergamo, Italy","Cloud,Fastweb",H100,Fastweb entered an agreement with NVIDIA to acquire 31 NVIDIA DGX H100 systems,2024-07-09,,11.840984455358647,,,,,,,248,NVIDIA,,True,,177,17.389823586854376,490792000000000000,490792000000000000,245371200000000000,122685600000000000,0.35386624000000005,,693400986768.3336,8837808.613864584,,,,,,Meta GenAI 2024a,0.010091145833333367,https://web.archive.org/web/20240714211301/https://www.hpcwire.com/off-the-wire/fastweb-unveils-italys-1st-nvidia-dgx-superpod-ai-supercomputer/,,,,,45.706852,9.590379
Opera Iceland KEF-1 SuperPOD,Existing,Confirmed,Yes,17.690897475032614,248.0000000036531,NVIDIA H100 SXM5 80GB,248,Iceland,Opera,2024-02-28,,Private,0.35386624000000005,8880926.781029787,"Sjónarhóli 6, 260 Reykjanesbæ, Iceland",Opera,H100,Calculated from Top500,2024-02-28,,11.840984455358647,,,,,,,248,NVIDIA,,True,,122,17.389823586854376,490792000000000000,490792000000000000,245371200000000000,122685600000000000,0.35386624000000005,,693400986768.3336,8880926.781029787,,,,,,Microsoft Azure Eagle,0.017222222222222295,https://web.archive.org/web/20240705020431/https://press.opera.com/2024/02/07/opera-deploying-green-powered-ai-data-cluster-in-iceland/,,,,,63.964569,-22.538465
Condor Galaxy 1 (CG-1),Existing,Confirmed,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,United States of America,"G42,Cerebras Systems",2023-11-13,"First of 9 planned supercomputers that will be interconnected
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,223267457.52627325,"Colovore LLC Data Center
1101 Space Park Dr, Santa Clara, CA 95054",Cloud,Cerebras CS2,"CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs",2023-11-13,Reported on their official website,,,,,,,,64,Cerebras,,True,,107,17.681241237375588,480000000000000000,,480000000000000000,,,,,223267457.52627325,,,,,,NVIDIA CoreWeave Eos-DFW Phase 1,0.022558290622969976,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,37.376673,-121.953162
Condor Galaxy 4 (CG-4),Planned,Likely,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Fourth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.20576134,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.681241237375588,480000000000000000,,480000000000000000,,,,,221766010.20576134,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,,
Condor Galaxy 5 (CG-5),Planned,Likely,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Fifth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.20576134,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.681241237375588,480000000000000000,,480000000000000000,,,,,221766010.20576134,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,,
Condor Galaxy 6 (CG-6),Planned,Likely,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Sixth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.20576134,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.681241237375588,480000000000000000,,480000000000000000,,,,,221766010.20576134,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,,
Condor Galaxy 7 (CG-7),Planned,Likely,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Seventh of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.20576134,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.681241237375588,480000000000000000,,480000000000000000,,,,,221766010.20576134,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,,
Condor Galaxy 8 (CG-8),Planned,Likely,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Eighth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.20576134,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.681241237375588,480000000000000000,,480000000000000000,,,,,221766010.20576134,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,,
Condor Galaxy 9 (CG-9),Planned,Likely,Yes,17.681241237375588,242.54674078174307,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Ninth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.20576134,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.681241237375588,480000000000000000,,480000000000000000,,,,,221766010.20576134,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,,,
FZJ JURECA,Existing,Confirmed,Yes,17.680545809713934,242.15866599649075,NVIDIA A100 SXM4 40 GB,768,Germany,Julich Supercomputing Center,2021-06-23,,Public,0.6765,20682757.865464754,"52428 Jülich, Germany","Academia,Researchers",A100,"In total, 192 of the 768 nodes are equipped with four NVIDIA A100 graphics processing units (GPUs)",2021-06-23,,11.549248013116312,,,,,,,768,NVIDIA,,True,,33,17.379515814049952,479232000000000000,479232000000000000,239616000000000000,119808000000000000,0.65974272,0.6765,354199556541.01996,20682757.865464754,,,,,,Sunway OceanLight,0.0806051587301586,https://web.archive.org/web/20231211004104/https://www.hpcwire.com/off-the-wire/julich-upgrades-jureca-supercomputer-for-large-data-volumes/,,,,,50.921734,6.358342
Max-Planck-Gesellschaft Raven,Existing,Confirmed,Yes,17.680545809713934,242.15866599649075,NVIDIA A100,768,Germany,Max Planck Society,2021-06-01,,Public,0.664,20731634.58257515,"Gießenbachstraße 2, 85748 Garching bei München, Germany","Academia,Max Planck Society",A100,"is creating with Lenovo a system called Raven-GPU, powered by 768 NVIDIA A100 GPUs.",2021-06-01,,11.557347734681937,,,,,,,768,NVIDIA,,True,,31,17.379515814049952,479232000000000000,479232000000000000,239616000000000000,119808000000000000,0.65974272,0.664,360867469879.51807,20731634.58257515,,,,,,Sunway OceanLight,0.0806051587301586,https://web.archive.org/web/20240520113356/https://www.datacenterdynamics.com/en/news/atos-to-build-215m-supercomputer-for-german-science-and-technology-organization/,https://web.archive.org/web/20241212234331/https://blogs.nvidia.com/blog/hpc-supercomputers-a100-gpus/,,,,48.261899,11.671739
Paper on GLM-130B,Existing,Confirmed,Yes,17.680545809713934,242.15866599649075,NVIDIA A100,768,,Zhipu AI,2022-10-05,,Public/Private,0.6541516799999999,17354466.596410636,,,A100,"""Specifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes""",2022-10-05,,11.563837352959242,,,True,,,,768,NVIDIA,,,,69,17.379515814049952,479232000000000000,479232000000000000,239616000000000000,119808000000000000,0.6541516799999999,,366300366300.36633,17354466.596410636,,,,,,Microsoft GPT-4 cluster,0.030719999999999803,https://arxiv.org/abs/2210.02414,,,,,,
Paper on xTrimoPGLM,Existing,Confirmed,Yes,17.680545809713934,242.15866599649075,NVIDIA A100,768,,BioMap Research,2023-07-06,,Public/Private,0.6373785599999999,17108567.052815683,,,A100,"xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers. Calculation - 96 servers×8 GPUs/server= 768 GPUs
​
",2023-07-06,,11.575118363368931,,,True,,,,768,NVIDIA,,,,87,17.379515814049952,479232000000000000,479232000000000000,239616000000000000,119808000000000000,0.6373785599999999,,375939849624.0602,17108567.052815683,,,,,,Microsoft GPT-4 cluster,0.030719999999999803,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4.article-info,,,,,,
NSC Berzelius Phase 2,Existing,Confirmed,Yes,17.671402430274064,237.1136937882306,NVIDIA A100 SXM4 80 GB,752,Sweden,Linköping University,2023-06-01,,Public,0.62409984,16793428.655254778,"583 30 Linköping, Sweden",Swedish academic research groups,"A100,752 A100s (480 40GB, 272 80GB)","addition will bring 34 additional nodes to the SuperPod, also based on the DGX A100 structure; this time, though, the A100s in the systems will be the 80GB variants rather than the 40GB GPUs in the remainder of the system",2023-06-01,,11.575118363368931,NSC Berzelius Phase 1,,,,,,752,NVIDIA,,True,,87,17.370372434610083,469248000000000000,469248000000000000,234624000000000000,117312000000000000,0.62409984,,375939849624.0602,16793428.655254778,,,,,,Microsoft GPT-4 cluster,0.030079999999999812,https://web.archive.org/web/20230712130753/https://www.hpcwire.com/2023/01/26/sweden-plans-expansion-for-nvidia-powered-berzelius-supercomputer/,,,,,58.394294,15.561061
Karlsruher Institut für Technologie HoreKa,Existing,Confirmed,Yes,17.664416309413397,233.32996463203506,NVIDIA A100,740,Germany,Karlsruhe Institute of Technology,2021-06-01,,Public,0.6319,19975793.738418765,"Karlsruhe, Germany",Academia,A100,to build a new 17-petaflops system that will pack 740 A100 GPUs on an NVIDIA Mellanox 200 Gbit/s InfiniBand network,2021-06-01,,11.56273795838543,,,,,,,740,NVIDIA,,True,,32,17.36338631374942,461760000000000000,461760000000000000,230880000000000000,115440000000000000,0.6356896,0.6319,365374268080.39246,19975793.738418765,,,,,,Sunway OceanLight,0.07766642898478812,https://web.archive.org/web/20210816050117/https://www.datacenterdynamics.com/en/news/horeka-hybrid-supercomputer-inaugurated-at-karlsruhe-institute-of-technology/,https://web.archive.org/web/20241212234331/https://blogs.nvidia.com/blog/hpc-supercomputers-a100-gpus/,,,,49.009716,8.401216
Paper on AlphaZero,Existing,Likely,Yes,17.66275783168157,232.4406265825018,Google TPU v1,5000,United States of America,Google,2017-12-05,,Private,0.880425,51389012.90928065,,Google,TPU v1,"Training proceeded
for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters,
using 5,000 first-generation TPUs (15) to generate self-play games",2017-12-05,,,,,,,,,5000,Google,,True,,2,,460000000000000000,460000000000000000,,,0.880425,,,51389012.90928065,,,,,,Meta 2017 V100 Cluster,0.1672727272727273,https://arxiv.org/pdf/1712.01815,,,,,39.381266,-97.922211
KT Internal MI250 Cluster,Existing,Confirmed,Yes,17.662380020016244,232.2385042985166,AMD Radeon Instinct MI250X,1200,Korea (Republic of),KT,2022-12-15,,Private,1.27764,30794896.330490954,Korea (Republic of),KT,AMD MI250X,"1,200 MI250s for KT's internal LLM development",2022-12-15,,11.555971519901366,,,,,,,1200,AMD,,True,,77,17.662380020016244,459600000000000000,459600000000000000,459600000000000000,114840000000000000,1.27764,,359725744341.12897,30794896.330490954,,,,,,Microsoft GPT-4 cluster,0.029461538461538157,https://web.archive.org/web/20241005005645/https://moreh.io/blog/training-221b-parameter-korean-llm-on-1200-amd-mi250-gpu-cluster-230814,,,,,,
NSTDA Supercomputer Center (ThaiSC) LANTA,Existing,Confirmed,Yes,17.642757248824534,221.97877716344988,NVIDIA A100,704,Thailand,National Science and Technology Development Agency (NSTDA),2022-11-01,,Public,0.5421,15907443.029104248,"Pathum Thani, Thailand","Academia,NSTDA,Thai Government",A100,"With 704 A100 GPUs, the new public high performance computing system will be Southeast Asia’s biggest.",2022-11-01,,11.60764784587996,,,,,,,704,NVIDIA,,True,,76,17.341727253160553,439296000000000000,439296000000000000,219648000000000000,109824000000000000,0.5996390399999999,0.5421,405179856115.1079,15907443.029104248,,,,,,Microsoft GPT-4 cluster,0.02815999999999982,https://web.archive.org/web/20240523025236/https://www.nvidia.com/en-sg/news/nvidia-powers-thailand-research-agency-new-supercomputer-with-region-largest-gpu-cluster/,,,,,14.020396,100.534043
Petrobras Gaia,Existing,Confirmed,Yes,17.642757248824534,221.97877716344988,NVIDIA A100,704,Brazil,Petrobras,2023-06-01,,Public/Private,0.9767,15721507.677259792,"Rio de Janeiro, Brazil",Petrobras,A100,Calculated from Top500,2023-06-01,,11.351966065441776,,,,,,,704,NVIDIA,,True,,91,17.341727253160553,439296000000000000,439296000000000000,219648000000000000,109824000000000000,0.58426368,0.9767,224887887785.3998,15721507.677259792,,,,,,Microsoft GPT-4 cluster,0.02815999999999982,https://web.archive.org/web/20240813050918/https://www.top500.org/system/180173/,,,,,-22.909534,-43.209934
Anonymized Chinese System,Planned,Likely,Yes,17.60205999132796,200,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.60205999132796,400000000000000060,,400000000000000060,,,,,,,,,,,,,,,,,,35.486703,101.901875
TotalEnergies Pangea III,Existing,Confirmed,Yes,17.62634036737504,213.74431531390957,NVIDIA V100,3384,France,TotalEnergies,2019-06-18,,Private,2.4892,66661774.26493671,"Av. Larribau, 64000 Pau, France",TotalEnergies,V100,Calculated from Top500,2019-06-18,Listed in Top500,11.23028057506261,,,,,,,3384,NVIDIA,,True,,8,17.62634036737504,423000000000000000,,423000000000000000,53027280000000000,2.25415008,2.4892,169934115378.43484,66661774.26493671,,,,,,Oak Ridge NL Summit,0.12239583333333331,https://web.archive.org/web/20240518130419/https://www.top500.org/system/179689/,,,,,43.317339,-0.319579
Paper on Flamingo ,Existing,Confirmed,Yes,17.625723909525753,213.44113188793239,Google TPU v4,1536,United States of America,Google,2022-04-29,,Private,0.5560289279999999,14054105.349209137,,Google,TPUv4,"""All training and evaluation was performed on TPUv4 instances. The
largest model containing 80 billion parameters is trained on 1536 chips""",2022-04-29,,11.88062652272075,,,True,Paper on PaLM,,,1536,Google,,,,60,17.625723909525753,422400000000000000,422400000000000000,422400000000000000,,0.5560289279999999,,759672705374.0629,14054105.349209137,,,,,,DeepSeek Fire-Flyer 2,0.06769230769230775,https://arxiv.org/abs/2204.14198,,,,,39.381266,-97.922211
University of Illinois NCSA Delta,Existing,Confirmed,Yes,17.62246271231184,211.84436584445376,NVIDIA A100,480,United States of America,University of Illinois Urbana-Champaign (UIUC),2022-10-11,,Public,0.6643728,10353794.202848867,"1205 W Clark St, Urbana, IL 61801",University of Illinois,"A100,A40","100 quad A100 GPU nodes consisting of: Four NVIDIA A100 GPUs with 40 GB HBM2 RAM and NVLink
Five eight-way A100 GPU nodes consisting of: Eight NVIDIA A100 GPUs with 40 GB HBM2 RAM and NVLink
100 quad A40 GPU nodes consisting of: Four NVIDIA A40 GPUs with 48 GB GDDR6 RAM",2022-10-11,,11.499062307280875,,,,,NVIDIA A40 PCIe,400,880,NVIDIA,NVIDIA,True,,76,17.321474151030554,419240000000000000,419240000000000000,209640000000000000,104800000000000000,0.6643728,,315545729746.9132,14995688.717814624,10000000,10353794.202848867,"The original Delta machine, which we profiled here and which cost $10 million as well",,,Microsoft GPT-4 cluster,0.026874358974358928,https://web.archive.org/web/20240503170712/https://www.datacenterdynamics.com/en/news/ncsa-announces-delta-supercomputer-is-ready-for-operations/,https://web.archive.org/web/20240927183658/https://www.ncsa.illinois.edu/research/project-highlights/delta/,"https://www.nextplatform.com/2023/07/11/ncsa-builds-out-delta-supercomputer-with-an-ai-extension/#:~:text=The%20original%20Delta%20machine%2C%20which,and%20now%20controlled%20by%20HPE.",,,40.114918,-88.224885
AIRAWAT-PSAI Phase 2,Existing,Confirmed,Yes,17.612088429058083,206.84386053866982,NVIDIA A100,656,India,Center for Development of Advanced Computing (C-DAC),2023-05-23,This consists of an original portion of 336 GPUs (PARAM Siddhi - AI) and a newer portion of 320 GPUs (AIRAWAT PoC),Public,0.5444275199999999,14649586.699264806,"Innovation Park, 34/B/1, Panchawati Rd, Mansarovar, Panchawati, Pashan, Pune, Maharashtra 411008, India",Cloud,A100,656 (82 nodes * 8 gpus per node),2023-05-23,,11.575118363368931,PARAM Siddhi-AI Phase 1,,,,,,656,NVIDIA,,True,,91,17.311058433394102,409344000000000000,409344000000000000,204672000000000000,102336000000000000,0.5444275199999999,,375939849624.06024,14649586.699264806,,,,,,Microsoft GPT-4 cluster,0.026239999999999913,https://drive.google.com/file/d/1bLDkmBo4dklKX5Psch-ifNpzNDcbELXA/view?usp=sharing,https://web.archive.org/web/20240720151011/https://cdac.in/index.aspx?id=hpc_nsf_siddhi-spec,,,,18.533898,73.811637
Anonymized Chinese System,Planned,Confirmed,Yes,17.60205999132796,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.60205999132796,400000000000000060,,400000000000000060,,,,,,,,,,,,,,,,,,35.486703,101.901875
MSU-270,Existing,Likely,Yes,17.60205999132796,202.1222839847841,,,Russia,Moscow State Univeristy,2023-09-02,,Public,,,"Moscow, Russia","Academia,Researchers",,with a peak computational power of 400 'AI' PetaFLOPS,2023-09-02,,,,,,,,,100,Unknown,,True,,109,17.60205999132796,400000000000000000,,400000000000000000,,,,,,,,,,,Tesla 10k H100 Cluster,0.02021222839818079,https://web.archive.org/web/20240909123929/https://www.tomshardware.com/news/russian-400-petaflops-supercomputer-for-ai-comes-online,,,,,55.750446,37.617494
Anonymized Chinese System,Planned,Confirmed,Yes,17.60205999132796,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.60205999132796,400000000000000060,,400000000000000060,,,,,,,,,,,,,,,,,,35.486703,101.901875
NVIDIA Cambridge-1,Existing,Confirmed,Yes,17.60136456366631,201.79888833040883,NVIDIA A100,640,United Kingdom of Great Britain and Northern Ireland,NVIDIA,2021-06-01,,Private,0.5497856,54262990.443092965,"London Rd, Harlow CM17 9NA, United Kingdom","Biomedical scientific community,including companies and the UK gov,NVIDIA",A100,"The Cambridge-A1 consists of 80 DGX A100 systems connected by Infiniband networking. Each DGX A100 has 8 A100 GPUs, 320GB GPU RAM, two AMD EPYC 7742 CPUs (64 cores each), up to 2TB system RAM, up to 30TB NVME data cache drives, and 2 1.92TB NVME SSDs.",2021-06-01,Reported by official NVIDIA post,11.560141207399278,,,,,,,640,NVIDIA,,True,,35,17.30033456800233,399360000000000000,399360000000000000,199680000000000000,99840000000000000,0.5497856,,363196125907.99036,17276362.152145956,51700000,54262990.443092965,NVIDIA will invest around £40 million ($51.7 million) in Cambridge-1.,,,Sunway OceanLight,0.06717096560846546,https://web.archive.org/web/20240617042217/https://www.theregister.com/2021/07/07/nvidia_launches_cambridge1_supercomputer_in/,https://nvidianews.nvidia.com/news/nvidia-building-uks-most-powerful-supercomputer-dedicated-to-ai-research-in-healthcare,,,,51.742089,0.133448
Calcul Québec Narval,Existing,Confirmed,Yes,17.598641705330838,200.53764527834502,NVIDIA A100,636,Canada,Compute Canada,2021-11-01,From Top500,Public,0.548,21713933.647872157,"1100 Notre-Dame St W, Montreal, Quebec H3C 1K3, Canada",Academia,A100,it features more than 632 state-of-the-art graphics processing units (GPUs) that are particularly well-suited to work in artificial intelligence.,2021-11-01,In Top500,11.558831151182487,,,,,,,636,NVIDIA,,True,,54,17.297611709666853,396864000000000000,396864000000000000,198432000000000000,99216000000000000,0.54634944,0.548,362102189781.0219,21713933.647872157,,,,,,DeepSeek Fire-Flyer 2,0.06360000000000045,https://web.archive.org/web/20240714041506/https://www.calculquebec.ca/en/nouvelle/narval-a-new-supercomputer-dedicated-to-scientific-research/,,,,,45.494947,-73.562848
Lawrence Livermore NL Lassen Phase 2,Existing,Confirmed,Yes,17.59769518592551,200.10106114493658,NVIDIA V100,3168,United States of America,US Department of Energy,2020-08-19,,Public,2.0756736,62708285.23869111,"7000 East Ave, Livermore, CA 94550",Lawrence Livermore NL,"V100,CS-1","""At Livermore, the Cerebras CS-1 machine was recently integrated into the National Nuclear Security Administration’s Lassen supercomputer, the unclassified companion system to Sierra.""
V100 count is from Top500",2020-08-19,Listed in Top500,11.280536124255693,Lawrence Livermore NL Lassen Phase 1,,,,Cerebras CS-1,1,3169,NVIDIA,Cerebras,True,,15,17.59769518592551,396000000000000000,,396000000000000000,49642560000000000,2.0756736,,190781440781.44077,62708285.23869111,,,,,,Oak Ridge NL Summit,0.1145833333333333,https://web.archive.org/web/20240327161123/https://www.hpcwire.com/2020/08/19/cerebras-llnl-deployment-teases-second-gen-wafer-scale-ai-chip/,https://web.archive.org/web/20240613070121/https://www.top500.org/system/179567/,,,,37.680665,-121.7099
hessian.AI fortytwo,Existing,Confirmed,Yes,17.595901667964807,199.27640222627883,NVIDIA A100,632,Germany,hessian.AI,2023-06-15,,Public,0.52450944,15850702.073282288,"Planckstraße 1, 64291 Darmstadt, Germany","Academia,Industry",A100,"In fortytwo‘s engine room, a total of 632 NVIDIA A100 Tensor Core GPUs, each with 80GB of memory, as well as four IPU units from Graphcore are performing their duties",2023-06-15,,11.575118363368931,,,,,,,632,NVIDIA,,True,,101,17.294871672300825,394368000000000000,394368000000000000,197184000000000000,98592000000000000,0.52450944,,375939849624.0602,14113626.210267315,15490930,15850702.073282288,"""The 14.5 million project"" presumably this is Euros",,,Microsoft GPT-4 cluster,0.025279999999999834,https://web.archive.org/web/20240623043843/https://hessian.ai/supercomputer-for-cutting-edge-ai-research-in-hesse/,,,,,49.931453,8.677595
Japanese Research Institute 2 Supercomputer 1,Existing,Confirmed,Yes,17.590369179364846,196.75391612214872,NVIDIA A100,624,Japan,,2022-06-01,,,0.53149824,14082761.553588701,Japan,,A100,Calculated from Top500,2022-06-01,,11.563837352959242,,,,,,,624,NVIDIA,,True,,72,17.289339183700864,389376000000000000,389376000000000000,194688000000000000,97344000000000000,0.53149824,,366300366300.36633,14082761.553588701,,,,,,Microsoft GPT-4 cluster,0.024959999999999836,https://web.archive.org/web/20241210014053/https://top500.org/system/180083/,,,,,36.386493,138.59223
EuroHPC Karolina,Existing,Confirmed,Yes,17.555607073105637,181.61899949736957,NVIDIA A100,576,Czechia,EuroHPC JU,2021-06-01,,Public,0.5477,18428093.599513467,"Studentská 6231/1b, 708 00 Ostrava 8, Czechia",European researchers,A100,"an accelerated part with 72 servers, and each of them is equipped with 8 GPU accelerators providing a performance of 11.6 PFlop/s for standard HPC simulations and up to 360 PFlop/s for artificial intelligence computations,  ",2021-06-01,,11.516034336512869,,,,,,,576,NVIDIA,,True,,37,17.254577077441652,359424000000000000,359424000000000000,179712000000000000,89856000000000000,0.49480704,0.5477,328121234252.32794,15548725.936931362,17557684,18428093.599513467,"The cost of the procured system was almost EUR 15 million, where 35%, i.e.  EUR 5.13 million, of which was paid by the EuroHPC JU. The remaining costs amounting to EUR 9.73 million was funded using the resources of the European Structural and Investment Funds",,,Sunway OceanLight,0.06045386904761945,https://web.archive.org/web/20241118204224/https://www.it4i.cz/en/infrastructure/karolina,https://www.acrossproject.eu/karolina-supercomputer-belongs-to-the-worlds-top-energy-efficient-supercomputers/,,,,49.834718,18.159458
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,200,,,China,,2023-04-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,93,17.47712125471966,300000000000000000,,300000000000000000,,,,,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,,35.486703,101.901875
Lawrence Livermore NL Lassen Phase 1,Existing,Confirmed,Yes,17.53402610605613,172.8145528069905,NVIDIA V100,2736,United States of America,US Department of Energy,2018-11-01,,Public,1.8822585600000001,54388512.36229927,"7000 East Ave, Livermore, CA 94550",Lawrence Livermore NL,V100,Chip count from Top500,2018-11-01,Listed in Top500,11.259346825185755,,Lawrence Livermore NL Lassen Phase 2,,,,,2736,NVIDIA,,True,,8,17.53402610605613,342000000000000000,,342000000000000000,42873120000000000,1.8822585600000001,,181696610268.03885,54388512.36229927,,,,,,Oak Ridge NL Summit,0.09895833333333319,https://web.archive.org/web/20240327161123/https://www.hpcwire.com/2020/08/19/cerebras-llnl-deployment-teases-second-gen-wafer-scale-ai-chip/,https://web.archive.org/web/20240613070121/https://www.top500.org/system/179567/,,,,37.680665,-121.7099
Jean Zay Supercomputer Phase 2,Existing,Confirmed,Yes,17.527629900871336,170.2880242571809,NVIDIA Tesla V100 SXM2,2696,France,GENCI,2020-06-01,=261*4+31*8+351*4,Public,1.7664191999999999,56284456.09864049,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,V100,"261 four-GPU accelerated compute nodes with:
2 Intel Cascade Lake 6248 processors (20 cores at 2.5 GHz), namely 40 cores per node
192 GB of memory per node
4 Nvidia Tesla V100 SXM2 GPUs (32 GB)
31 eight-GPU accelerated compute nodes, currently dedicated to the AI community with:
2 Intel Cascade Lake 6226 processors (12 cores at 2.7 GHz), namely 24 cores per node
20 nodes with 384 GB of memory and 11 nodes with 768 GB of memory
8 Nvidia Tesla V100 SXM2 GPUs (32 GB)
Extension in the summer of 2020, 351 four-GPU accelerated compute nodes with:
2 Intel Cascade Lake 6248 processors (20 cores at 2.5 GHz), namely 40 cores per node
192 GB of memory per node
4 Nvidia Tesla V100 SXM2 GPUs (16 GB)",2020-06-01,,11.280536124255693,Jean Zay Supercomputer Phase 1,Jean Zay Supercomputer Phase 3,,,,,2696,NVIDIA,,True,,14,17.527629900871336,337000000000000000,,337000000000000000,42246320000000000,1.7664191999999999,,190781440781.4408,56284456.09864049,,,,,,Oak Ridge NL Summit,0.09751157407407406,https://web.archive.org/web/20240528032205/http://www.idris.fr/eng/annonces/idris-extension-jean-zay-h100-eng.html,https://web.archive.org/web/20220319163248/http://www.idris.fr/eng/jean-zay/cpu/jean-zay-cpu-hw-eng.html,,,,48.706817,2.175654
Aleph Alpha alpha ONE,Existing,Confirmed,Yes,17.504454550658252,161.43911066432713,NVIDIA A100,512,Germany,Aleph Alpha,2022-06-01,,Private,0.43610112,11555086.402944576,"Max-Urich-Straße 3, 13355 Berlin, Germany","German Government Agencies,Private Sector,Aleph Alpha",A100,Calculated from Top500,2022-06-01,,11.563837352959242,,,,,,,512,NVIDIA,,True,,76,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.43610112,,366300366300.36633,11555086.402944576,,,,,,Microsoft GPT-4 cluster,0.020479999999999863,https://web.archive.org/web/20241210021004/https://top500.org/system/180080/,,,,,52.539459,13.383962
Los Alamos NL Chicoma,Existing,Likely,Yes,17.504454550658252,161.43911066432713,NVIDIA A100,512,United States of America,US Department of Energy,2021-12-20,,Public,0.43982847999999997,17533893.938290603,"Los Alamos, New Mexico",Los Alamos National Laboratory,A100,the remaining 128 are equipped with individual AMD Epyc 7713 CPUs and quadruple Nvidia A100 GPUs,2021-12-20,"Slightly unclear when the GPUs were installed, but they seem to be installed by December 2021 at the latest",11.560141207399278,,,,,,,512,NVIDIA,,True,,60,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.43982847999999997,,363196125907.99036,17533893.938290603,,,,,,DeepSeek Fire-Flyer 2,0.051200000000000065,https://web.archive.org/web/20240722055614/https://www.hpcwire.com/2022/03/17/los-alamos-chicoma-supercomputer-to-host-75-new-projects/,,,,,35.881462,-106.301781
Oracle 2020 A100 Cluster,Existing,Likely,Yes,17.504454550658252,161.43911066432713,NVIDIA A100,512,United States of America,Oracle,2020-09-30,"Slightly uncertain if they were just saying that they could build clusters up to 512 A100 in size, or if this was actually already available and built. It seems likely it was available, though",Private,0.4472832,13881391.160293194,,Cloud,A100,allowing customers to scale up to 512 GPUs in a single cluster,2020-09-30,,11.55284196865778,,,,,,,512,NVIDIA,,True,,20,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.4472832,,357142857142.8571,13881391.160293194,,,,,,Oak Ridge NL Summit,0.09244444444444448,https://web.archive.org/web/20240219152835/https://blogs.oracle.com/cloud-infrastructure/post/oracle-cloud-infrastructure-compute-and-high-performance-computing-roadmap-update,,,,,39.381266,-97.922211
Paper on BloombergGPT,Existing,Confirmed,Yes,17.504454550658252,161.43911066432713,NVIDIA A100,512,,Bloomberg,2023-03-30,,Private,0.42491903999999997,11466651.113806635,,,A100,"""This yields a total of 512 40GB A100 GPUs""",2023-03-30,,11.575118363368931,,,True,,,,512,NVIDIA,,,,95,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.42491903999999997,,375939849624.0602,11466651.113806635,,,,,,Microsoft GPT-4 cluster,0.020479999999999863,https://arxiv.org/abs/2303.17564,,,,,,
Paper on CodeFuse,Existing,Confirmed,Yes,17.504454550658252,161.43911066432713,NVIDIA A100,512,,Ant Group,2023-10-10,,Private,0.42491903999999997,10121180.236548563,,,A100,"CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with a Hardware FLOPs Utilization (HFU) of approximately 60%. The training process took approximately 40 days to complete.",2023-10-10,,11.575118363368931,,,True,,,,512,NVIDIA,,,,123,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.42491903999999997,,375939849624.0602,10121180.236548563,,,,,,Tesla 10k H100 Cluster,0.016143911066194998,https://arxiv.org/abs/2310.06266,,,,,,
Paper on Florence,Existing,Confirmed,Yes,17.504454550658252,161.43911066432713,NVIDIA A100 SXM4 40 GB,512,,Microsoft,2021-11-22,,Private,0.43982847999999997,13791613.293652141,,Microsoft,A100,"""The model takes 10 days to train on 512 NVIDIA-A100 GPUs
with 40GB memory per GPU.""",2021-11-22,,11.560141207399278,,,True,Microsoft Azure Voyager-EUS2,,,512,NVIDIA,,,,59,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.43982847999999997,,363196125907.99036,13791613.293652141,,,,,,DeepSeek Fire-Flyer 2,0.051200000000000065,https://arxiv.org/abs/2111.11432v1,,,,,,
Paper on Stable LM,Existing,Confirmed,Yes,17.504454550658252,161.43911066432713,NVIDIA A100 SXM4 40 GB,512,,Stability AI,2023-01-18,,Private,0.42491903999999997,11279538.291043062,,,A100,"""Hardware: Stable LM 2 1.6B was trained on the Stability AI cluster across 512 NVIDIA A100 40GB GPUs (AWS P4d instances).""",2023-01-18,,11.575118363368931,,,True,,,,512,NVIDIA,,,,91,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.42491903999999997,,375939849624.0602,11279538.291043062,,,,,,Microsoft GPT-4 cluster,0.020479999999999863,https://web.archive.org/web/20241218081859/https://huggingface.co/stabilityai/stablelm-2-1_6b,,,,,,
Paper on StarCoder,Existing,Confirmed,Yes,17.504454550658252,161.43911066432713,NVIDIA A100,512,,,2023-05-09,,,0.42491903999999997,11457697.202668522,,,A100,"""We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes.""",2023-05-09,,11.575118363368931,,,True,,,,512,NVIDIA,,,,96,17.20342455499427,319488000000000000,319488000000000000,159744000000000000,79872000000000000,0.42491903999999997,,375939849624.0602,11457697.202668522,,,,,,Microsoft GPT-4 cluster,0.020479999999999863,https://arxiv.org/abs/2305.06161,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,200,,500,China,,2023-09-15,,Private,0.4,10000000,China,,,,,,11.698970004336019,,,,,,,500,Anonymized,Anonymized,True,,119,17.30102999566398,300000000000000000,300000000000000000,200000000000000030,80000000000000000,0.4,,500000000000.00006,10000000,,,,,,Tesla 10k H100 Cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,200,,1000,China,,2023-07-15,,Private,0.5,,China,,,,,,11.778151250383642,,,,,,,1000,Anonymized,Anonymized,True,,117,17.47712125471966,300000000000000000,,300000000000000000,200000000000000030,0.5,,600000000000,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,200,,,China,,2022-05-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,73,17.47712125471966,300000000000000000,,300000000000000000,,,,,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Confirmed,Yes,17.47712125471966,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.47712125471966,300000000000000000,,300000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,17.47712125471966,200,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.47712125471966,300000000000000000,,300000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,17.47712125471966,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.47712125471966,300000000000000000,,300000000000000000,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,200,,,China,,2023-06-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,114,17.47712125471966,300000000000000000,,300000000000000000,,,,,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,,35.486703,101.901875
NSC Berzelius Phase 1,Existing,Confirmed,Yes,17.47642582705801,151.34916624780664,NVIDIA A100 SXM4 40 GB,480,Sweden,Linköping University,2021-03-23,,Public,0.41233919999999996,12957271.614109471,"583 30 Linköping, Sweden",Swedish academic research groups,A100,"Berzelius, as it stands, is an Nvidia SuperPod consisting of 60 DGX A100 nodes. Each node, in turn, carries eight A100 (40GB) GPUs",2021-03-23,,11.560141207399278,,NSC Berzelius Phase 2,,,,,480,NVIDIA,,True,,28,17.175395831394027,299520000000000000,299520000000000000,149760000000000000,74880000000000000,0.41233919999999996,,363196125907.99036,12957271.614109471,,,,,,Sunway OceanLight,0.0503782242063491,https://web.archive.org/web/20230712130753/https://www.hpcwire.com/2023/01/26/sweden-plans-expansion-for-nvidia-powered-berzelius-supercomputer/,,,,,58.394294,15.561061
PLaMO-13B training cluster,Existing,Likely,Yes,17.47642582705801,151.34916624780664,NVIDIA A100,480,Japan,Preferred Networks Inc,2023-09-25,,Private,0.3983616,9488606.471764278,,,A100,A100 40GB x480,2023-09-25,,11.575118363368931,,,True,AIST ABCI 2.0,,,480,NVIDIA,,,,132,17.175395831394027,299520000000000000,299520000000000000,149760000000000000,74880000000000000,0.3983616,,375939849624.0602,9488606.471764278,,,,,,Tesla 10k H100 Cluster,0.015134916624557807,https://huggingface.co/pfnet/plamo-13b,https://web.archive.org/web/20250216174632/https://tech.preferred.jp/en/blog/llm-plamo/,https://www.preferred.jp/en/news/pr20230928/,,,36.386493,138.59223
Pawsey Supercomputing Centre Setonix,Existing,Confirmed,Yes,17.468559994000135,148.63264275105197,AMD Radeon Instinct MI250X,768,Australia,Pawsey Supercomputing Centre,2022-07-01,,Public,0.8323,19752614.097740326,"1 Bryce Ave, Kensington WA 6151, Australia",Academia,AMD MI250X,Calculated from Top500,2022-07-01,,11.548280099367187,,,,,,,768,AMD,,True,,83,17.468559994000135,294144000000000000,294144000000000000,294144000000000000,73497600000000000,0.8176896,0.8323,353411029676.79926,19752614.097740326,,,,,,Microsoft GPT-4 cluster,0.01885538461538459,https://web.archive.org/web/20240616091315/https://www.hpcwire.com/off-the-wire/pawseys-setonix-supercomputer-fires-up-for-researchers/,,,,,-31.992752,115.883558
Paper on GLaM,Existing,Confirmed,Yes,17.449632650470072,142.29408792528824,Google TPU v4,1024,United States of America,Google,2021-12-13,,Private,0.373854208,9518052.054392463,,Google,TPUv4,"""Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we
train using 1,024 TPU-v4 chips for 574 hours (with 280B
tokens).""",2021-12-13,,11.876930377160788,,,True,Google TPU v4 Pod,,,1024,Google,,,,63,17.449632650470072,281600000000000000,281600000000000000,281600000000000000,,0.373854208,,753234801091.2317,9518052.054392463,,,,,,DeepSeek Fire-Flyer 2,0.04512820512820515,https://arxiv.org/abs/2112.06905,,,,,39.381266,-97.922211
Paper on Minerva,Existing,Confirmed,Yes,17.449632650470072,142.29408792528824,Google TPU v4,1024,United States of America,Google,2022-06-29,,Private,0.370685952,9401336.144358981,,Google,TPUv4,"""the 540B model was trained for 29 days on a v4-1024.""",2022-06-29,,11.88062652272075,,,True,Google Oklahoma TPU v4 Pods,,,1024,Google,,,,83,17.449632650470072,281600000000000000,281600000000000000,281600000000000000,,0.370685952,,759672705374.0629,9401336.144358981,,,,,,Microsoft GPT-4 cluster,0.018051282051281918,https://arxiv.org/abs/2206.14858,,,,,39.381266,-97.922211
Paper on PaLI,Existing,Confirmed,Yes,17.449632650470072,142.29408792528824,Google TPU v4,1024,United States of America,Google,2022-09-14,,Private,0.370685952,9418434.468592497,,Google,TPUv4,"""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days.""",2022-09-14,,11.88062652272075,,,True,Google Oklahoma TPU v4 Pods,,,1024,Google,,,,88,17.449632650470072,281600000000000000,281600000000000000,281600000000000000,,0.370685952,,759672705374.0629,9418434.468592497,,,,,,Microsoft GPT-4 cluster,0.018051282051281918,https://arxiv.org/abs/2209.06794v4,,,,,39.381266,-97.922211
Paper on ViT-22B,Existing,Confirmed,Yes,17.449632650470072,142.29408792528824,Google TPU v4,1024,United States of America,Google,2023-02-10,,Private,0.361181184,9341262.43111022,,Google,TPUv4,"""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""",2023-02-10,,11.89190753313044,,,True,Google Oklahoma TPU v4 Pods,,,1024,Google,,,,99,17.449632650470072,281600000000000000,281600000000000000,281600000000000000,,0.361181184,,779664092357.5908,9341262.43111022,,,,,,Microsoft GPT-4 cluster,0.018051282051281918,https://arxiv.org/abs/2302.05442v1,,,,,39.381266,-97.922211
University of Edinburgh DiRAC Tursa,Existing,Confirmed,Yes,17.44646260368057,141.25922183128736,NVIDIA A100,448,United Kingdom of Great Britain and Northern Ireland,Distributed Research using Advanced Computing (DiRAC),2021-11-01,,Public,0.4296,15295349.487809317,"Edinburgh, Scotland",Academia,A100,"114 GPU nodes: 4x Nvidia RedStone A100-40 per node, 640 Tensor cores, 6,912 CUDA cores",2021-11-01,,11.512368335325085,,,,,,,448,NVIDIA,,True,,61,17.145432608016584,279552000000000000,279552000000000000,139776000000000000,69888000000000000,0.38484991999999996,0.4296,325363128491.6201,15295349.487809317,,,,,,DeepSeek Fire-Flyer 2,0.04480000000000041,https://web.archive.org/web/20220121113452/https://www.epcc.ed.ac.uk/hpc-services/dirac-tursa-gpu,,,,,55.953346,-3.188375
MosaicML MPT training cluster,Existing,Confirmed,Yes,17.43863726616861,138.7367357271561,NVIDIA A100,440,,MosaicML,2023-03-05,,,0.3651648,9850806.272650851,,,A100,"""over 9.5 days on 440 GPUs,""",2023-03-05,,11.575118363368931,,,,,,,440,NVIDIA,,True,,106,17.137607270504628,274560000000000000,274560000000000000,137280000000000000,68640000000000000,0.3651648,,375939849624.0602,9850806.272650851,,,,,,Microsoft GPT-4 cluster,0.01759999999999988,https://www.databricks.com/blog/mpt-7b,,,,,,
Petrobras Dragão,Existing,Confirmed,Yes,17.434568904034197,137.44315310965385,NVIDIA V100,2176,Brazil,Petrobras,2021-06-01,,Public/Private,1.661,41189375.6188358,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,2021-06-01,,11.214199271582803,,,,,,,2176,NVIDIA,,True,,42,17.434568904034197,272000000000000000,,272000000000000000,34097920000000000,1.40195328,1.661,163756773028.2962,41189375.6188358,,,,,,Sunway OceanLight,0.04574945574294535,https://web.archive.org/web/20241126215645/https://top500.org/system/179941/,,,,,-22.909534,-43.209934
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,100,,500,China,,2022-12-15,,Private,0.3,,China,,,,,,11.522878745280337,,,True,,,,500,Anonymized,Anonymized,,,101,17,300000000000000000,300000000000000000,100000000000000020,,0.3,,333333333333.3334,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,100,,2000,China,,2019-11-15,,Private,1,40000000,China,,,,,,11.47712125471966,,,,,,,2000,Anonymized,Anonymized,True,,10,17.47712125471966,300000000000000000,,300000000000000000,30000000000000000,1,,300000000000,40000000,,,,,,Oak Ridge NL Summit,0.08,,,,,,35.486703,101.901875
Jean Zay Supercomputer Phase 3,Existing,Confirmed,Yes,17.414277920309164,131.1692774147658,NVIDIA A100,416,France,GENCI,2022-06-01,,Public,2.07659088,64360101.46104754,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,"V100,A100","The power boost will come from an additional 52 nodes made up of HPE Apollo 6500 Gen10 nodes, each with eight Nvidia A100 80GB GPUs – for a total of 640GB of high-bandwidth memory (HBM2) per server – and some minor additional updates that include three Apollo 6500 Gen10 nodes, each with eight Nvidia A100 40GB PCIe GPUs",2022-06-01,,11.351772462046277,Jean Zay Supercomputer Phase 2,,,,NVIDIA Tesla V100 SXM2,2696,3112,NVIDIA,NVIDIA,True,,86,17.66912340438784,259584000000000000,259584000000000000,466792000000000000,107142320000000000,2.07659088,,224787657740.2671,64360101.46104754,,,,,,Microsoft GPT-4 cluster,0.016639999999999888,https://web.archive.org/web/20240528032205/http://www.idris.fr/eng/annonces/idris-extension-jean-zay-h100-eng.html,https://web.archive.org/web/20240425051414/https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/,,,,48.706817,2.175654
Paper on BLOOM,Existing,Confirmed,Yes,17.414277920309164,131.1692774147658,NVIDIA A100,416,,,2022-11-09,,,0.35433215999999995,9399852.699016146,,,A100,"Training was conducted on 48 nodes, each
having 8 NVIDIA A100 80GB GPUs (a total of 384 GPUs); due to possible hardware failures during training, we also maintained a reserve of 4 spare nodes.",2022-11-09,,11.563837352959242,,,True,,,,416,NVIDIA,,,,100,17.113247924645183,259584000000000000,259584000000000000,129792000000000000,64896000000000000,0.35433215999999995,,366300366300.36633,9399852.699016146,,,,,,Microsoft GPT-4 cluster,0.016639999999999888,https://arxiv.org/abs/2211.05100,,,,,,
AWS Fast BERT Training,Existing,Confirmed,Yes,17.408239965311846,129.35826175026193,NVIDIA V100,2048,United States of America,Amazon,2019-12-03,,Private,1.36421376,39839471.712,,Amazon,V100,"With TensorFlow, we achieved unprecedented scale with 2,048 GPUs on 256 P3dn.24xlarge instances to train BERT in 62 minutes",2019-12-03,,11.27335753962857,,,,,,,2048,NVIDIA,,True,,11,17.408239965311846,256000000000000000,,256000000000000000,32092160000000000,1.36421376,,187653876178.46634,39839471.712,,,,,,Oak Ridge NL Summit,0.074074074074074,https://web.archive.org/web/20241003202056/https://aws.amazon.com/blogs/machine-learning/amazon-web-services-achieves-fastest-training-times-for-bert-and-mask-r-cnn/,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125471966,100,,,China,,2021-06-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,45,,300000000000000000,300000000000000000,,,,,,,,,,,,Sunway OceanLight,0.04,,,,,,35.486703,101.901875
Taiwania 2,Existing,Confirmed,Yes,17.40140054078154,127.33703891041411,NVIDIA Tesla V100 SXM2,2016,Taiwan,National Applied Research Laboratories Taiwan,2018-11-01,,Public,1.4999,15302391.306770073,"Hsinchu, Taiwan",Taiwan Cloud,V100,"Powered by 2,016 NVIDIA V100 Tensor Core GPUs, TAIWANIA 2 can deliver 9 petaflops of computing power to support high performance computing and AI workloads at scale",2018-11-01,,11.225338235656464,,,,,,,2016,NVIDIA,,True,,9,17.40140054078154,252000000000000000,,252000000000000000,31590720000000000,1.38692736,1.4999,168011200746.71646,43816148.90661022,13882937,15302391.306770073,"""“Taiwania 2” system, an NT$430 million""",,,Oak Ridge NL Summit,0.0729166666666666,https://web.archive.org/web/20240420074136/https://blogs.nvidia.com/blog/taiwania-2/,https://web.archive.org/web/20240303090755/https://www.nchc.org.tw/Message/MessageView/288?mid=92&page=4,"https://www.nextplatform.com/2024/10/09/taiwans-fastest-ai-supercomputer-goes-to-foxconn/#:~:text=The%20NCHC%2C%20as%20the%20center,supercomputer%20rankings%20at%20number%2020.",,,24.806633,120.968683
Paper on ViT,Existing,Confirmed,Yes,17.401235063743187,127.28852956225731,Google TPU v3,2048,,Google,2021-06-08,,Private,0.9676226560000001,19445351.83247404,,Google,TPU v3,"""For models ViT-g and ViT-G, to speed up the training, we scale up the batch size at most to 32 768 and distribute the training to 2048 TPUv3 chips.""",2021-06-08,,11.41552903532599,,,True,Google MLPerf 0.7 Submission,,,2048,Google,,,,47,17.401235063743187,251904000000000000,,251904000000000000,,0.9676226560000001,,260332887451.5315,19445351.83247404,,,,,,Sunway OceanLight,0.04236937830687802,https://arxiv.org/abs/2106.04560,,,,,,
NVIDIA In-house DGX A100 Cluster,Existing,Likely,Yes,17.397244581010387,126.12430520650648,NVIDIA A100,400,United States of America,NVIDIA,2020-05-14,50 nodes x 8 A100 per node,Private,0.34944,11082112.842592591,,NVIDIA,A100,"The tech company also built its own in-house DGX A100 supercomputer, consisting of 50 nodes",2020-05-14,,11.55284196865778,,,,,,,400,NVIDIA,,True,,17,17.096214585346402,249600000000000000,249600000000000000,124800000000000000,62400000000000000,0.34944,,357142857142.8571,11082112.842592591,,,,,,Oak Ridge NL Summit,0.07222222222222277,https://web.archive.org/web/20241129110307/https://fedscoop.com/argonne-national-lab-ai-supercomputer/,,,,,39.381266,-97.922211
Paper on Code Llama,Existing,Likely,Yes,17.397244581010387,126.12430520650648,NVIDIA A100,400,,Meta AI,2023-08-24,,Private,0.331968,8915080.852008082,,Meta,A100,,2023-08-24,,11.575118363368931,,,True,Meta Research SuperCluster (RSC-1) Phase 2,,,400,NVIDIA,,,,144,17.096214585346402,249600000000000000,249600000000000000,124800000000000000,62400000000000000,0.331968,,375939849624.0602,8915080.852008082,,,,,,Microsoft GPT-4 cluster,0.016000000000000007,https://arxiv.org/abs/2308.12950,,,,,,
AGH Cyfronet Athena,Existing,Confirmed,Yes,17.379515814049952,121.07933299824536,NVIDIA A100 SXM4 40 GB,384,Poland,AGH University of Krakow,2022-06-01,,Public,0.32707583999999995,4172106.1735073826,"Podole 60, 30-394 Kraków, Poland","AGH University of Krakow,Researchers in Poland,Cloud",A100,"Athena's configuration includes: 48 servers with AMD EPYC processors and 1 TB of RAM (6,144 CPU compute cores in total) as well as 384 NVIDIA A100 GPGPU cards.",2022-06-01,,11.563837352959242,,,,,,,384,NVIDIA,,True,,92,17.07848581838597,239616000000000000,239616000000000000,119808000000000000,59904000000000000,0.32707583999999995,,366300366300.36633,10212759.00044682,4034620,4172106.1735073826,"the new supercomputer, which cost 20 million zloty (€4.15 million) to build, not including infrastructure costs",,,Microsoft GPT-4 cluster,0.015359999999999898,https://web.archive.org/web/20230205120608/https://www.hpcwire.com/2022/10/10/poland-inaugurates-athena-supercomputer/,https://web.archive.org/web/20240426185305/https://www.datacenterdynamics.com/en/news/polands-agh-launches-athena-supercomputer-in-krakow/,https://web.archive.org/web/20240715093943/https://top500.org/system/180055/,https://notesfrompoland.com/2022/10/05/polands-fastest-supercomputer-launched-with-blessing-from-archbishop/,,50.021256,19.886228
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,400,China,,2023-06-15,,Public/Private,0.3,,China,,,,,,11.522878745280337,,,,,,,400,Anonymized,Anonymized,True,,136,17,200000000000000030,200000000000000030,100000000000000020,,0.3,,333333333333.3334,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,,35.486703,101.901875
Indiana University Bloomington Jetstream2,Existing,Confirmed,Yes,17.35148709044971,113.51187468585499,NVIDIA A100 SXM4 40 GB,360,United States of America,Indiana University Bloomington,2022-02-20,Jetstream2 is a larger project across multiple universities. Indiana University has the largest portion. ,Public,0.30663359999999995,10551295.264380665,"Bloomington, Indiana","Academia,Researchers",A100,GPUs: 360 (NVIDIA A100 SXM4 40GB),2022-02-20,,11.563837352959242,,,,,,,360,NVIDIA,,True,,77,17.050457094785727,224640000000000000,224640000000000000,112320000000000000,56160000000000000,0.30663359999999995,,366300366300.36633,9769389.418554684,10000000,10551295.264380665,"Indiana University has been awarded a $10 million NSF grant to build ‘Jetstream 2,’",,,DeepSeek Fire-Flyer 2,0.03600000000000004,https://web.archive.org/web/20211207065233/https://docs.jetstream-cloud.org/overview/config/,https://web.archive.org/web/20240721082615/https://news.iu.edu/it/live/news/30451-jetstream2-ai-for-everyone,,,,39.166596,-86.534885
University of Tokyo Wisteria/BDEC-01 (Aquarius),Existing,Confirmed,Yes,17.35148709044971,113.51187468585499,NVIDIA A100,360,Japan,The University of Tokyo,2021-06-01,,Public,0.3239,9717953.710582102,"7 Chome-3-1 Hongo, Bunkyo City, Tokyo 113-8654, Japan",Academia,A100,Calculated from Top500,2021-06-01,,11.540046146775552,,,,,,,360,NVIDIA,,True,,47,17.050457094785727,224640000000000000,224640000000000000,112320000000000000,56160000000000000,0.3092544,0.3239,346773695585.0571,9717953.710582102,,,,,,Sunway OceanLight,0.03778366815476183,https://web.archive.org/web/20241126224540/https://top500.org/system/179963/,,,,,35.71755,139.744241
NSCC ASPIRE 2A Phase 2,Existing,Confirmed,Yes,17.341727253160553,110.98938858172492,NVIDIA A100,352,Singapore,National Supercomputing Center Singapore,2022-11-01,,Public,0.29981951999999995,7953721.514552124,Singapore,Cloud,A100,Calculated from Top500,2022-11-01,Top500,11.563837352959242,NSCC ASPIRE 2A Phase 1,,,,,,352,NVIDIA,,True,,109,17.04069725749657,219648000000000000,219648000000000000,109824000000000000,54912000000000000,0.29981951999999995,,366300366300.36633,7953721.514552124,,,,,,Microsoft GPT-4 cluster,0.014079999999999909,https://drive.google.com/file/d/1ARed1E6KguYXjkL3Qta2ZKv_isej3xNn/view?usp=drive_link,https://web.archive.org/web/20240530103453/https://www.top500.org/system/180077/,https://web.archive.org/web/20231004100738/https://www.hpcwire.com/2021/04/28/hpe-will-build-singapores-new-national-supercomputer/,,,1.351616,103.808053
Osaka University SQUID,Existing,Confirmed,Yes,17.321523867072266,105.94441637346466,NVIDIA A100 SXM4 80 GB,336,Japan,Osaka University,2021-05-01,42x8=336,Public,0.28863744,10581771.818189401,"Osaka, Japan","Osaka University,Academia,Researchers",A100,42 Nodes; GPU：NVIDIA A100 8 units,2021-05-01,,11.560141207399278,,,,,,,336,NVIDIA,,True,,34,17.020493871408284,209664000000000000,209664000000000000,104832000000000000,52416000000000000,0.28863744,,363196125907.9903,10581771.818189401,,,,,,Sunway OceanLight,0.03526475694444438,https://archive.ph/fcLwW#selection-705.1-705.24,,,,,34.683595,135.500784
PARAM Siddhi-AI Phase 1,Existing,Confirmed,Yes,17.321523867072266,105.94441637346466,NVIDIA A100,336,India,Center for Development of Advanced Computing (C-DAC),2020-11-01,8 GPUs per node,Public,0.2935296,9109662.948942408,"Innovation Park, 34/B/1, Panchawati Rd, Mansarovar, Panchawati, Pashan, Pune, Maharashtra 411008, India",Cloud,A100,42 x DGX A100 Nodes,2020-11-01,,11.55284196865778,,AIRAWAT-PSAI Phase 2,,,,,336,NVIDIA,,True,,26,17.020493871408284,209664000000000000,209664000000000000,104832000000000000,52416000000000000,0.2935296,,357142857142.8572,9109662.948942408,,,,,,Oak Ridge NL Summit,0.06066666666666669,https://drive.google.com/file/d/1bLDkmBo4dklKX5Psch-ifNpzNDcbELXA/view?usp=sharing,https://web.archive.org/web/20240720151011/https://cdac.in/index.aspx?id=hpc_nsf_siddhi-spec,,,,18.533898,73.811637
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,17.30102999566398,200000000000000030,,200000000000000030,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,400,China,,2023-05-15,,Public,0.3,,China,,,,,,11.522878745280337,,,,,,,400,Anonymized,Anonymized,True,,132,17,200000000000000030,200000000000000030,100000000000000020,,0.3,,333333333333.3334,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,400,China,,2021-05-15,,Public,0.3,,China,,,,,,11.522878745280337,,,,,,,400,Anonymized,Anonymized,True,,36,17,200000000000000030,200000000000000030,100000000000000000,,0.3,,333333333333.3333,,,,,,,Sunway OceanLight,0.03,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,,China,,2022-09-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,106,17.30102999566398,200000000000000030,,200000000000000030,,,,,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,400,China,,2018-03-15,,Public,,,China,,,,,,,,,,,,,400,Anonymized,Anonymized,True,,4,17.30102999566398,200000000000000030,,200000000000000030,,,,,,,,,,,Meta 2017 V100 Cluster,0.07,,,,,,35.486703,101.901875
Leonardo SpA davinci-1,Existing,Confirmed,Yes,17.30033456800233,100.89944416520522,NVIDIA A100,320,Italy,Leonardo SpA,2020-11-01,,Public/Private,0.279552,8675869.475183245,"Genoa, Italy","Aerospace, Defense, and Security,Leonardo SpA",A100,Calculated from Top500,2020-11-01,,11.55284196865778,,,,,,,320,NVIDIA,,True,,28,16.99930457233835,199680000000000000,199680000000000000,99840000000000000,49920000000000000,0.279552,,357142857142.8571,8675869.475183245,,,,,,Oak Ridge NL Summit,0.05777777777777825,https://web.archive.org/web/20240512034721/https://www.leonardo.com/en/press-release-detail/-/detail//02-12-2020-davinci-1-is-the-name-of-the-new-leonardo-supercomputer,,,,,44.408115,8.931178
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,100,,300,China,,2022-10-15,,Private,0.3,7000000,China,,,,,,11.522878745280337,,,,,,,300,Anonymized,Anonymized,True,,111,17,200000000000000030,200000000000000030,100000000000000000,50000000000000000,0.3,,333333333333.3333,7000000,,,,,,Microsoft GPT-4 cluster,0.01,,,,,,35.486703,101.901875
Recursion BioHive-1,Existing,Confirmed,Yes,17.30033456800233,100.89944416520522,NVIDIA A100,320,United States of America,Recursion Pharmaceuticals,2021-04-15,,Private,0.2748928,8638181.076072978,"41 400 W, Salt Lake City, UT 84101","Recursion Pharmaceuticals,Pharmaceutical Research",A100,BioHive-1 consists of 40 NVIDIA DGX A100 640GB nodes,2021-04-15,,11.560141207399278,,,,,,,320,NVIDIA,,True,,36,16.99930457233835,199680000000000000,199680000000000000,99840000000000000,49920000000000000,0.2748928,,363196125907.99036,8638181.076072978,,,,,,Sunway OceanLight,0.033585482804233,https://web.archive.org/web/20240908095141/https://www.genengnews.com/gen-edge/from-model-to-molecule-nvidia-doubles-down-on-ai-drug-discovery/,,,,,40.767676,-111.902638
University of Cambridge Wilkes-3,Existing,Confirmed,Yes,17.30033456800233,100.89944416520522,NVIDIA A100,320,United Kingdom of Great Britain and Northern Ireland,University of Cambridge,2021-06-01,,Public,0.2581,8638181.076072978,"Cambridge, England","Academia,Researchers",A100,Calculated from Top500,2021-06-01,,11.587516567794479,,,,,,,320,NVIDIA,,True,,52,16.99930457233835,199680000000000000,199680000000000000,99840000000000000,49920000000000000,0.2748928,0.2581,386826811313.4444,8638181.076072978,,,,,,Sunway OceanLight,0.033585482804233,https://web.archive.org/web/20241210022223/https://top500.org/system/179930/,,,,,52.203482,0.123582
Princeton Della Phase 1,Existing,Confirmed,Yes,17.294871672300825,99.63820111313939,NVIDIA A100,316,United States of America,Princeton University,2023-02-01,,Public,0.26225472,7088936.198253197,"300 Forrestal Rd, Princeton, NJ 08544",Academia,A100,Specifications on website,2023-02-01,,11.575118363368931,,Princeton Della Phase 2,,,,,316,NVIDIA,,True,,126,16.993841676636848,197184000000000000,197184000000000000,98592000000000000,49296000000000000,0.26225472,,375939849624.0602,7088936.198253197,,,,,,Microsoft GPT-4 cluster,0.012639999999999915,https://web.archive.org/web/20230201020753/https://researchcomputing.princeton.edu/systems/della,,,,,40.34444,-74.61551
NVIDIA DGX SuperPOD 2019,Existing,Confirmed,Yes,17.283301228703547,97.01869631269646,NVIDIA Tesla V100 DGXS 32 GB,1536,,NVIDIA,2019-06-17,,Private,0.8526336,47352411.127468064,,NVIDIA,V100,"NVIDIA sets the bar once again in supercomputing, building a well-balanced system with 96 NVIDIA® DGX-2H™ servers containing 1,536 NVIDIA Tesla® V100 SXM3 GPUs. The DGX SuperPOD has earned the 22nd spot on the June 2019 TOP500 list.",2019-06-17,,11.352538785676195,,,,,,,1536,NVIDIA,,True,,11,17.283301228703547,192000000000000000,,192000000000000000,24069120000000000,0.8526336,,225184651414.1596,47352411.127468064,,,,,,Oak Ridge NL Summit,0.055555555555555504,https://web.archive.org/web/20240530072101/https://developer.nvidia.com/blog/dgx-superpod-world-record-supercomputing-enterprise/,https://web.archive.org/web/20240704024932/https://developer.nvidia.com/blog/training-bert-with-gpus/,,,,,
RPI AiMOS,Existing,Confirmed,Yes,17.27646180417324,95.5027791828106,NVIDIA V100,1512,United States of America,Rensselaer Polytechnic Institute,2019-12-11,,Public,0.9288,29412734.974875,"Troy, New York","Researchers,Academia",V100,"AiMOS is comprised of 252 compute nodes with a total of 504 IBM Power9 processors and 1,512 Nvidia Volta GPUs.",2019-12-11,Listed in Top500,11.308539597442726,,,,,,,1512,NVIDIA,,True,,15,17.27646180417324,189000000000000000,,189000000000000000,23693040000000000,1.0071734399999999,0.9288,203488372093.02325,29412734.974875,,,,,,Oak Ridge NL Summit,0.05468749999999996,https://web.archive.org/web/20201206000554/https://www.hpcwire.com/2019/12/11/rpi-powers-up-aimos-ai-supercomputer/,,,,,42.731153,-73.68903
Google TensorFlow Research Cloud,Existing,Likely,Yes,17.265572461743115,93.13794846018871,Google TPU v2,4096,United States of America,Google,2017-05-17,"1 cloud TPU = 4 TPU chips, according to the Forbes doccument and the FLOP counts we know for TPU v2",Private,2.692644864,38946125.27231902,,"Google,Cloud",TPUv2,"""Google also announced the TensorFlow Research Cloud, a 1,000-TPU (4,000 Cloud TPU Chip) supercomputer delivering 180 PetaFlops""

""the TensorFlow Research Cloud (TFRC), a cluster of 1,000 Cloud TPUs""...""Up to 180 teraflops of floating-point performance per Cloud TPU""",2017-05-17,,10.8353933841488,,,,,,,4096,Google,,True,,1,17.265572461743115,184320000000000000,,184320000000000000,12288000000000000,2.692644864,,68453141542.84254,38946125.27231902,,,,,,Google TensorFlow Research Cloud,1,https://web.archive.org/web/20240724203521/https://research.google/blog/introducing-the-tensorflow-research-cloud/,https://web.archive.org/web/20240619124934/https://www.forbes.com/sites/moorinsights/2017/05/22/google-cloud-tpu-strategic-implications-for-google-nvidia-and-the-machine-learning-industry/,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,90,,1000,China,,2018-11-15,,Private,1,30000000,China,,,,,,11.30102999566398,,,,,,,1000,Anonymized,Anonymized,True,,12,17.30102999566398,200000000000000030,,200000000000000030,20000000000000000,1,,200000000000.00003,30000000,,,,,,Oak Ridge NL Summit,0.05,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,90,,1000,China,,2018-11-15,,Private,1,30000000,China,,,,,,11.30102999566398,,,,,,,1000,Anonymized,Anonymized,True,,12,17.30102999566398,200000000000000030,,200000000000000030,20000000000000000,1,,200000000000.00003,30000000,,,,,,Oak Ridge NL Summit,0.05,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,90,,1000,China,,2018-11-15,,Private,1,30000000,China,,,,,,11.30102999566398,,,,,,,1000,Anonymized,Anonymized,True,,12,17.30102999566398,200000000000000030,,200000000000000030,20000000000000000,1,,200000000000.00003,30000000,,,,,,Oak Ridge NL Summit,0.05,,,,,,35.486703,101.901875
SURF Snellius Phase 1,Existing,Confirmed,Yes,17.254577077441652,90.80949974868403,NVIDIA A100,288,Netherlands,SURF,2021-09-16,,Public,0.24740352,9849628.041067226,"Science Park 120, 1098 XG Amsterdam, Netherlands","Academia,Researchers",A100,The 72 Phase 1 GPU nodes each contain an NVIDIA HGX A100 baseboard with 4 GPUs,2021-09-16,,11.560141207399278,,SURF Snellius Phase 3,,,,,288,NVIDIA,,True,,74,16.95354708177767,179712000000000000,179712000000000000,89856000000000000,44928000000000000,0.24740352,,363196125907.99036,9849628.041067226,,,,,,DeepSeek Fire-Flyer 2,0.02880000000000004,https://web.archive.org/web/20240712185633/https://www.surf.nl/en/news/gpu-expansion-of-supercomputer-snellius-enables-even-faster-data-processing-for-dutch,https://web.archive.org/web/20240725104223/https://visualization.surf.nl/snellius-virtual-tour/#/,https://web.archive.org/web/20240716063628/https://www.cwi.nl/en/stories/hurray-a-new-national-supercomputer-snellius/,,,52.357104,4.950874
Anonymized Chinese System,Existing,Likely,Yes,17.30102999566398,90,,40000,China,,2017-11-15,,Public,20,,China,,,,,,,,,,,,,40000,Anonymized,Anonymized,True,,3,,200000000000000030,,,200000000000000030,20,,,,,,,,,Meta 2017 V100 Cluster,0.06,,,,,,35.486703,101.901875
NVIDIA SATURN V Phase 3,Existing,Unlikely,Yes,17.24234262102464,88.28701364455384,NVIDIA A100,280,United States of America,NVIDIA,2020-05-14,"This is actually four clusters, so the minimum size of the maximum constituent cluster is 1/4 the total size
This builds off earlier versions of the SaturnV, which has been around in different forms since 2016
Very uncertain about number of chips here",Private,3.704064,109200578.25648147,,NVIDIA,"A100,V100","""Nvidia has updated its own internal supercomputer known as SaturnV with four DGX SuperPODS - or 1,120 A100s - to add another 2.8 exaFLOPS of power""
""Saturn-V is not really one system. It sits in four different locations under one management interface""",2020-05-14,Uncertain about what chips are in which of the four clusters,11.304851367990311,NVIDIA SATURN V Phase 2,,,,NVIDIA V100,5280,5560,NVIDIA,NVIDIA,,,,17.873529849943704,174720000000000000,174720000000000000,747360000000000000,126417600000000000,3.704064,,201767572050.5909,109200578.25648147,,,,,,,,https://web.archive.org/web/20240416013949/https://www.hpcwire.com/2020/05/14/nvidias-ampere-a100-gpu-2-5x-the-hpc-20x-the-training/,https://web.archive.org/web/20240307200601/https://www.theregister.com/2020/05/14/ampere_nvidias_gpu/,https://web.archive.org/web/20240730095809/https://blogs.nvidia.com/blog/dgx-superpod-a100/,https://web.archive.org/web/20240416004832/https://www.nextplatform.com/2017/11/30/inside-nvidias-next-gen-saturn-v-ai-cluster/,,39.381266,-97.922211
U.S. Army Jean,Existing,Confirmed,Yes,17.24234262102464,88.28701364455384,NVIDIA A100,280,United States of America,US Department of Defense,2021-05-30,,Public,0.24053119999999997,7558408.441563858,"Aberdeen Proving Ground,
Maryland, 21005","US Department of Defense,US Army",A100,"Jean incorporates 1,202 Cascade Lake-AP CPUs, 280 A100 GPUs",2021-05-30,"No exact confirmation on when it was operational, but it said ""mid fiscal 2021"" for when it was expected to be operational. Known to be operational by 2023",11.560141207399278,,,,,,,280,NVIDIA,,True,,50,16.94131262536066,174720000000000000,174720000000000000,87360000000000000,43680000000000000,0.24053119999999997,,363196125907.99036,7558408.441563858,,,,,,Sunway OceanLight,0.029387297453703626,https://web.archive.org/web/20231004094910/https://www.hpcwire.com/2020/08/24/dod-orders-two-ai-focused-supercomputers-from-liqid/,,,,,39.484141,-76.140516
TU Dresden AlphaCentauri,Existing,Confirmed,Yes,17.229753493716622,85.76452754042411,NVIDIA A100 SXM4 40 GB,272,Germany,Technische Universität Dresden,2021-02-28,,Public,0.2882,7336080.256749999,"Dresden, Germany","Academia,Researchers",A100,At the heart of the system and essential for the computing power are a total of 272 NVIDIA A100 GPUs,2021-02-28,,11.46902952157467,,,,,,,272,NVIDIA,,True,,41,16.92872349805264,169728000000000000,169728000000000000,84864000000000000,42432000000000000,0.23365887999999999,0.2882,294462179042.3317,7336080.256749999,,,,,,Oak Ridge NL Summit,0.04911111111111132,https://web.archive.org/web/20241202181003/https://tu-dresden.de/tu-dresden/newsportal/news/alpha-centauri-platzierung?set_language=en,,,,,51.049072,13.738695
University of Minnesota Agate,Existing,Confirmed,Yes,17.216788516552253,83.2420414362937,NVIDIA A100 SXM4 40 GB,264,United States of America,University of Minnesota,2021-11-01,,Public,0.22678656,7109513.148699459,"117 Pleasant St SE # 5, Minneapolis, MN 55455","Academia,Researchers",A100,264 NVIDIA A100 Tensor Core GPUs,2021-11-01,,11.560141207399278,,,,,,,264,NVIDIA,,True,,87,16.91575852088827,164736000000000000,164736000000000000,82368000000000000,41184000000000000,0.22678656,,363196125907.9903,7109513.148699459,,,,,,DeepSeek Fire-Flyer 2,0.02640000000000004,https://web.archive.org/web/20241202175121/https://www-archive.msi.umn.edu/content/agate,,,,,44.975284,-93.236347
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,80,,300,China,,2023-04-15,,Public,0.2,,China,,,,,,11.698970004336019,,,,,,,400,Anonymized,Anonymized,True,,146,17,200000000000000030,200000000000000030,100000000000000020,,0.2,,500000000000.00006,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,,35.486703,101.901875
G42 Artemis,Existing,Confirmed,Yes,17.209515014542628,81.85952501383768,NVIDIA V100,1296,United Arab Emirates,G42,2019-11-01,,Public/Private,1.2255,25278626.298907787,"Abu Dhabi, United Arab Emirates",Scientific research and cloud,V100,Calculated from Top500,2019-11-01,,11.121201698954534,,,,,,,1296,NVIDIA,,True,,19,17.209515014542628,162000000000000000,,162000000000000000,20308320000000000,0.86329152,1.2255,132190942472.46022,25278626.298907787,,,,,,Oak Ridge NL Summit,0.04687499999999998,https://web.archive.org/web/20240530181044/https://blogs.nvidia.com/blog/artemis-genomics/,,,,,24.453524,54.377438
JAMSTEC ZettaScaler-2.2 Gyoukou,Existing,Likely,Yes,17.205204363948145,81.05103587789907,PEZY-SC2,10000,Japan,Japan Agency for Marine-Earth Science and Technology,2017-11-15,,Public,2.5996,,,Japan Agency for Marine-Earth Science and Technology,PEZY-SC2,"10,000 PEZY-SC2 processor modules",2017-11,,10.79029783565311,JAMSTEC ZettaScaler-2.0 Gyoukou,,,,,,10000,PEZY,,True,,4,17.205204363948145,160400000000000000,,160400000000000000,81920000000000000,4.22604,2.5996,61701800276.96569,,,,,,,Meta 2017 V100 Cluster,0.05832727272727317,https://web.archive.org/web/20241222180018/https://en.wikipedia.org/wiki/Gyoukou,https://www.top500.org/system/179102/,,,,36.386493,138.59223
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,80,,1000,China,,2020-07-15,,Private,0.8,20000000,China,,,,,,11.397940008672036,,,,,,,1000,Anonymized,Anonymized,True,,31,17.30102999566398,200000000000000030,,200000000000000030,20000000000000000,0.8,,250000000000.00003,20000000,,,,,,Oak Ridge NL Summit,0.05,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,17.30102999566398,80,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,17.30102999566398,200000000000000030,,200000000000000030,,,,,,,,,,,,,,,,,,35.486703,101.901875
Indiana University Big Red 200,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,United States of America,Indiana University Bloomington,2021-11-30,"Unclear exactly when this was first operational. It was first listed in Top500 in November 2021, but it was planned to be operational Fall 2020",Public,0.21991423999999998,10054424.080592304,"Bloomington, Indiana",Indiana University,A100,Big Red 200 is accelerated by 256 NVIDIA A100 Tensor Core GPUs,2021-11-30,,11.560141207399278,,,,,,,256,NVIDIA,,True,,92,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21991423999999998,,363196125907.99036,8742397.146255614,9600000,10054424.080592304,is a $9.6 million Cray Shasta system,,,DeepSeek Fire-Flyer 2,0.02560000000000003,https://web.archive.org/web/20240709091139/https://news.iu.edu/it/live/news/30305-indiana-university-unveils-supercomputer-big-red,https://web.archive.org/web/20230530192726/https://www.hpcwire.com/off-the-wire/big-red-200-is-a-leap-forward-for-indiana-university-students-and-researchers/,https://web.archive.org/web/20240915161038/https://www.top500.org/system/180020/,https://www.hpcwire.com/2020/01/24/indiana-university-dedicates-big-red-200-cray-shasta-supercomputer/,,39.166596,-86.534885
NSCC ASPIRE 2A Phase 1,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,Singapore,National Supercomputing Center Singapore,2022-06-01,,Public,0.21805056,5777543.201472288,Singapore,Cloud,A100,Calculated from Top500,2022-06-01,Top500,11.563837352959242,,NSCC ASPIRE 2A Phase 2,,,,,256,NVIDIA,,True,,116,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21805056,,366300366300.36633,5777543.201472288,,,,,,Microsoft GPT-4 cluster,0.01023999999999993,https://drive.google.com/file/d/1ARed1E6KguYXjkL3Qta2ZKv_isej3xNn/view?usp=drive_link,https://web.archive.org/web/20240530103453/https://www.top500.org/system/180077/,https://web.archive.org/web/20231004100738/https://www.hpcwire.com/2021/04/28/hpe-will-build-singapores-new-national-supercomputer/,,,1.351616,103.808053
Paper on EVA,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,,,2022-11-14,,Public/Private,0.21805056,5784524.73785609,,,A100,"""These modifications allow us to train a 1.1B CLIP with a batch size of
41k on 256× NVIDIA A100 40GB GPUs.""",2022-11-14,,11.563837352959242,,,True,,,,256,NVIDIA,,,,134,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21805056,,366300366300.36633,5784524.73785609,,,,,,Microsoft GPT-4 cluster,0.01023999999999993,https://arxiv.org/abs/2211.07636,,,,,,
Paper on Polyglot,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,,"EleutherAI,Stability AI",2023-06-04,,Private,0.21245951999999999,5716911.882639924,,,A100,"""Additionally, we received invaluable assistance from Stability AI2 , who provided access to 256 A100s (8 * 32 nodes) on HPC clusters""",2023-06-04,,11.575118363368931,,,True,Paper on Stable Diffusion,,,256,NVIDIA,,,,161,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21245951999999999,,375939849624.0602,5716911.882639924,,,,,,Microsoft GPT-4 cluster,0.01023999999999993,https://arxiv.org/abs/2306.02254,,,,,,
Paper on Pythia,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,,EleutherAI,2023-04-03,Appendix D table with GPU counts for each model size,Private,0.21245951999999999,5733325.556903318,,Cloud,A100,"""All GPUs are A100s with 40GB of memory"", ",2023-04-03,,11.575118363368931,,,True,,,,256,NVIDIA,,,,148,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21245951999999999,,375939849624.0602,5733325.556903318,,,,,,Microsoft GPT-4 cluster,0.01023999999999993,https://arxiv.org/abs/2304.01373,,,,,,
Paper on SAM,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,,Meta AI,2023-04-05,,Private,0.21245951999999999,5733325.556903318,,,A100,"""Wedistribute training across 256 GPUs,""",2023-04-05,,11.575118363368931,,,True,Meta Research SuperCluster (RSC-1) Phase 1,,,256,NVIDIA,,,,148,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21245951999999999,,375939849624.0602,5733325.556903318,,,,,,Microsoft GPT-4 cluster,0.01023999999999993,https://arxiv.org/abs/2304.02643,,,,,,
Paper on Stable Diffusion,Existing,Likely,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,,Stability AI,2021-12-20,,Private,0.21991423999999998,8766946.969145302,,Cloud,A100,,2021-12-20,,11.560141207399278,,,True,,,,256,NVIDIA,,,,95,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21991423999999998,,363196125907.99036,8766946.969145302,,,,,,DeepSeek Fire-Flyer 2,0.02560000000000003,https://arxiv.org/abs/2112.10752,,,,,,
Paper on XGLM,Existing,Confirmed,Yes,17.20342455499427,80.71955533216355,NVIDIA A100,256,,Meta AI,2021-12-20,,Private,0.21991423999999998,8766946.969145302,,Meta,A100,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",2021-12-20,,11.560141207399278,,,True,Meta Research SuperCluster (RSC-1) Phase 1,,,256,NVIDIA,,,,95,16.902394559330293,159744000000000000,159744000000000000,79872000000000000,39936000000000000,0.21991423999999998,,363196125907.99036,8766946.969145302,,,,,,DeepSeek Fire-Flyer 2,0.02560000000000003,https://arxiv.org/abs/2112.10668,,,,,,
Universitaet Frankfurt Goethe-NHR,Existing,Confirmed,Yes,17.20216124701935,80.48509348274128,AMD Instinct MI210,880,Germany,Goethe University Frankfurt,2023-06-01,,Public,0.3322,,"Brüningstraße 50, 65926 Frankfurt am Main, Germany","Academia,Researchers",AMD MI210,of 880 AMD MI210 graphics cards,2023-06-01,,11.680761618903976,,,,,,,880,AMD,,True,,168,17.20216124701935,159280000000000000,,159280000000000000,,,0.3322,479470198675.4967,,,,,,,Microsoft GPT-4 cluster,0.010210256410256349,https://archive.ph/cFXgi,,,,,50.096145,8.537956
TACC Lonestar6,Existing,Confirmed,Yes,17.196585130463966,79.45831228009851,NVIDIA A100 PCIe,252,United States of America,University of Texas at Austin,2022-11-14,,Public,0.16098263999999998,5591851.571063727,"10100 Burnet Rd, Austin, TX 78758","Academia,Researchers",A100,Lonestar6 hosts 84 A100 GPU nodes that are configured identically to the compute nodes with the addition of 3 NVIDIA A100 GPUs,2022-11-14,,11.688776089567543,,,,,,,252,NVIDIA,,True,,139,16.895555134799988,157248000000000000,157248000000000000,78624000000000000,39312000000000000,0.16098263999999998,,488400488400.48846,5591851.571063727,,,,,,Microsoft GPT-4 cluster,0.010079999999999933,https://web.archive.org/web/20240808090149/https://www.hpcwire.com/off-the-wire/taccs-lonestar6-supercomputer-gets-gpu-and-server-boost/,https://web.archive.org/web/20241006113309/https://docs.tacc.utexas.edu/hpc/lonestar6/,,,,30.387809,-97.726782
Microsoft Azure Immunity Bio,Existing,Confirmed,Yes,17.193820026016112,78.95401718155685,NVIDIA V100,1250,United States of America,Microsoft,2020-04-01,,Private,0.819,23994725.87279736,,Cloud,V100,"The cluster contains over 1,250  NVIDIA V100 Tensor Core high performance graphics processing units (GPUs)",2020-04-01,,11.280536124255693,,,,,,,1250,NVIDIA,,True,,25,17.193820026016112,156250000000000000,,156250000000000000,19587500000000000,0.819,,190781440781.4408,23994725.87279736,,,,,,Oak Ridge NL Summit,0.04521122685185209,https://web.archive.org/web/20240910193454/https://immunitybio.com/immunitybio-combines-supercomputing-power-with-microsoft-azure-to-target-infection-doorway-of-the-coronavirus/,,,,,39.381266,-97.922211
Eni HPC4 Phase 2,Existing,Confirmed,Yes,17.19312459835446,78.82769075406587,NVIDIA A100,250,Italy,Eni,2021-10-30,,Private,1.98653,60465865.25663597,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,"V100,A100","Comprising the bulk of the system, there are 1,375 nodes that house two Nvidia “V100 O&G” GPUs; 125 nodes have two Nvidia A100s",2021-10-30,,11.326959963230928,Eni HPC4 Phase 1,,,,NVIDIA V100,2750,3000,NVIDIA,NVIDIA,True,,82,17.62505509126116,156000000000000000,156000000000000000,421750000000000000,82092500000000000,1.98653,,212304873321.82248,60465865.25663597,,,,,,DeepSeek Fire-Flyer 2,0.024999999999999994,https://web.archive.org/web/20220124003756/https://www.hpcwire.com/2021/10/13/eni-goes-back-to-hpe-for-hpc4-refresh-via-greenlake/,https://web.archive.org/web/20250213075947/https://top500.org/system/179444/,,,,45.101078,8.859647
SberCloud Christofari,Existing,Confirmed,Yes,17.176091259055678,75.79585649429404,NVIDIA V100,1200,Russia,SberCloud,2019-11-09,"SberCloud, which owns this supercomputer, was originally owned by Sber, but was sold off to Noviye Vozmozhnosti in March 2022
Sberbank is majority owned by the Russian Government.",Private,0.799344,23406135.461951654,"Skolkovo Innovation Center
Инновационный центр Сколково
Moscow Oblast
Russia",Cloud,V100,Calculated from Top500,2019-11-09,,11.27335753962857,,,,,,,1200,NVIDIA,,True,,21,17.176091259055678,150000000000000000,,150000000000000000,18804000000000000,0.799344,,187653876178.46634,23406135.461951654,,,,,,Oak Ridge NL Summit,0.0434027777777777,https://web.archive.org/web/20230517125435/https://en.wikipedia.org/wiki/Christofari,,,,,55.526558,37.773636
EuroHPC Vega,Existing,Confirmed,Yes,17.175395831394027,75.6745831239033,NVIDIA A100 PCIe,240,Slovenia,"EuroHPC JU,Institute of Information Science (IZUM)",2021-04-01,,Public,0.1546272,21170555.63225022,"Prešernova ulica 17, 2000 Maribor, Slovenia","Cloud,European researchers",A100,With 960 CPU nodes (overall 1920 CPUs AMD Epyc 7H12 – 122000 cores) and 60 GPU nodes (overall 240 GPUs NVidia A100),2021-04-01,,11.68507994400758,,,,,,,240,NVIDIA,,True,,49,16.87436583573005,149760000000000000,149760000000000000,74880000000000000,37440000000000000,0.1546272,,484261501210.6538,6478635.807054736,20170612,21170555.63225022,"""The Vega supercomputer was jointly procured by Slovenia and the EU with an investment of €17.2 million""
This supercomputer contains a lot of CPUs, so the price might be higher than normal",,,Sunway OceanLight,0.025189112103174545,https://web.archive.org/web/20210420154820/https://www.hpcwire.com/off-the-wire/vega-first-new-eurohpc-supercomputer-to-be-delivered-in-the-eu-will-be-operational-april-2021/,https://archive.ph/fCnkQ,https://www.hpcwire.com/off-the-wire/vega-online-the-eu-first-eurohpc-supercomputer-is-operational/,,,46.563561,15.651397
Japan Atomic Energy Agency and Quantum and Radiological Science and Technology HPE SGI8600,Existing,Confirmed,Yes,17.133538908370216,68.72157655482691,NVIDIA Tesla V100 SXM2,1088,Japan,Japan Atomic Energy Agency,2020-11-01,,Public,0.7128576,22615099.765310995,Japan,"Japan Atomic Energy Agency,National Institutes for Quantum and Radiological Science and Technology",V100,Calculated from Top500,2020-11-01,,11.280536124255693,,,,,,,1088,NVIDIA,,True,,41,17.133538908370216,136000000000000000,,136000000000000000,17048960000000000,0.7128576,,190781440781.4408,22615099.765310995,,,,,,Oak Ridge NL Summit,0.039351851851851964,https://web.archive.org/web/20240331015933/https://ccse.jaea.go.jp/computer/index_eng.html,,,,,36.386493,138.59223
Petrobras Atlas,Existing,Confirmed,Yes,17.133538908370216,68.72157655482691,NVIDIA V100,1088,Brazil,Petrobras,2020-06-01,,Public/Private,0.9789,20775184.635092027,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,2020-06-01,,11.142800579862678,,,,,,,1088,NVIDIA,,True,,31,17.133538908370216,136000000000000000,,136000000000000000,17048960000000000,0.7128576,0.9789,138931453672.48953,20775184.635092027,,,,,,Oak Ridge NL Summit,0.039351851851851964,https://web.archive.org/web/20241210015105/https://www.top500.org/system/179854/,,,,,-22.909534,-43.209934
Anonymized Chinese System,Existing,Confirmed,Yes,17,70,,500,China,,2018-12-15,,Public,0.5,20000000,China,,,,,,10.079181246047623,,,,,,200,700,Anonymized,Anonymized,True,,17,15.778151250383642,100000000000000020,100000000000000020,6000000000000000,40000000000000000,0.5,,12000000000,20000000,,,,,,Oak Ridge NL Summit,0.04,,,,,,35.486703,101.901875
Jean Zay Supercomputer Phase 1,Existing,Confirmed,Yes,17.115610511674298,65.94239515003603,NVIDIA Tesla V100 SXM2 32 GB,1044,France,GENCI,2019-06-01,,Public,0.69542928,30548105.982789855,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,V100,"the system encompasses 1,528 Intel next-generation Xeon nodes and 261 GPU nodes, each with four Nvidia Tesla V100 (32GB) GPUs, 1,044 in all.",2019-06-01,,11.27335753962857,,Jean Zay Supercomputer Phase 2,,,,,1044,NVIDIA,,True,,18,17.115610511674298,130500000000000000,,130500000000000000,16359480000000000,0.69542928,,187653876178.46634,22443812.56149425,27916750,30548105.982789855,approximately €25 million for the first partition of the Jean-Zay machine,,,Oak Ridge NL Summit,0.03776041666666673,https://web.archive.org/web/20230123052742/https://www.hpcwire.com/2019/01/22/france-to-deploy-ai-focused-supercomputer-jean-zay/,https://news.cnrs.fr/opinions/computing-the-cost-of-computation,,,,48.706817,2.175654
Paper on DALL-E,Existing,Confirmed,Yes,17.107209969647865,64.67913087513097,NVIDIA V100,1024,,OpenAI,2021-02-24,,Private,0.65974272,19366409.86,,OpenAI,V100,"We trained the model using 1024, 16 GB NVIDIA V100 GPUs",2021-02-24,,11.287835362997193,,,True,Azure OpenAI GPT-3 Cluster,,,1024,NVIDIA,,,,50,17.107209969647865,128000000000000000,,128000000000000000,16046080000000000,0.65974272,,194015024523.4991,19366409.86,,,,,,Oak Ridge NL Summit,0.03703703703703699,https://arxiv.org/abs/2102.12092,,,,,,
Paper on RoBERTa,Existing,Confirmed,Yes,17.107209969647865,64.67913087513097,NVIDIA Tesla V100 DGXS 32 GB,1024,United States of America,Meta AI,2019-07-26,,Private,0.5684224,31539757.125552356,,,,"We pretrain our model using
1024 V100 GPUs for approximately one day",2019-07-26,,11.352538785676195,,,True,Meta 2017 V100 Cluster,,,1024,NVIDIA,,,,21,17.107209969647865,128000000000000000,,128000000000000000,16046080000000000,0.5684224,,225184651414.1596,31539757.125552356,,,,,,Oak Ridge NL Summit,0.03703703703703699,https://arxiv.org/abs/1907.11692,,,,,39.381266,-97.922211
Preferred Networks MN-2,Existing,Confirmed,Yes,17.107209969647865,64.67913087513097,NVIDIA V100,1024,Japan,Preferred Networks Inc,2019-07-30,,Private,0.68210688,20153662.293333333,Japan,Preferred Networks,V100,"GPUs: 1,024",2019-07-30,,11.27335753962857,,,,,,,1024,NVIDIA,,True,,21,17.107209969647865,128000000000000000,,128000000000000000,16046080000000000,0.68210688,,187653876178.46634,20153662.293333333,,,,,,Oak Ridge NL Summit,0.03703703703703699,https://web.archive.org/web/20241126234046/https://fuse.wikichip.org/news/3063/japanese-ai-startup-preferred-networks-designed-a-custom-half-petaflops-training-chip/,,,,,36.386493,138.59223
Google TPUv3 POD Generic,Existing,Confirmed,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2019-05-07,"Refers to the general commercially available pod, not to a specific cluster using the pod",Private,0.500211712,10154990.907067887,,Google,TPU v3,A TPU v3 Pod is composed of 1024 chips interconnected with high-speed links,2019-05-07,,11.401051211957368,,,,,,,1024,Google,,True,,18,17.10020506807921,125952000000000000,,125952000000000000,,0.500211712,,251797382944.9239,10154990.907067887,,,,,,Oak Ridge NL Summit,0.03644444444444458,https://web.archive.org/web/20231128061217/https://cloud.google.com/blog/products/ai-machine-learning/googles-scalable-supercomputers-for-machine-learning-cloud-tpu-pods-are-now-publicly-available-in-beta,https://web.archive.org/web/20240626015135/https://cloud.google.com/tpu/docs/v3,,,,,
Paper on GShard,Existing,Confirmed,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2020-10-30,,Private,0.49201152000000004,9765095.968227586,,Google,TPU v3,,2020-10-30,,11.408229796584491,,,True,Google MLPerf 0.7 Submission,,,1024,Google,,,,44,17.10020506807921,125952000000000000,,125952000000000000,,0.49201152000000004,,255994005994.00598,9765095.968227586,,,,,,Oak Ridge NL Summit,0.03644444444444458,https://arxiv.org/abs/2006.16668,,,,,,
Paper on LaMDA,Existing,Confirmed,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2022-01-20,,Private,0.47971123200000004,9719635.89111035,,Google,TPU v3,"""We pre-trained LaMDA on 1024 TPU-v3 chips""",2022-01-20,,11.419225180885954,,,True,Google MLPerf 0.7 Submission,,,1024,Google,,,,110,17.10020506807921,125952000000000000,,125952000000000000,,0.47971123200000004,,262557954865.64716,9719635.89111035,,,,,,DeepSeek Fire-Flyer 2,0.020184615384615474,https://arxiv.org/abs/2201.08239,,,,,,
Paper on Meena,Existing,Confirmed,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2020-01-27,,Private,0.49201152000000004,9859735.664835958,,,TPU v3,"""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)""",2020-01-27,,11.408229796584491,,,True,Google TPUv3 POD Generic,,,1024,Google,,,,29,17.10020506807921,125952000000000000,,125952000000000000,,0.49201152000000004,,255994005994.00598,9859735.664835958,,,,,,Oak Ridge NL Summit,0.03644444444444458,https://arxiv.org/abs/2001.09977,,,,,,
Paper on Noisy Student,Existing,Confirmed,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2019-11-11,,Private,0.500211712,10018621.288799293,,Google,TPU v3,,2019-11-11,,11.401051211957368,,,True,Google TPUv3 POD Generic,,,1024,Google,,,,26,17.10020506807921,125952000000000000,,125952000000000000,,0.500211712,,251797382944.9239,10018621.288799293,,,,,,Oak Ridge NL Summit,0.03644444444444458,https://arxiv.org/abs/1911.04252v4,,,,,,
Paper on Pseudo Labels,Existing,Confirmed,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2020-03-23,,Private,0.49201152000000004,9859735.664835958,,Google,TPU v3,"""Specifically, our training process runs on a cluster of 2,048
TPUv3 cores.""",2020-03-23,,11.408229796584491,,,True,Google TPUv3 POD Generic,,,1024,Google,,,,30,17.10020506807921,125952000000000000,,125952000000000000,,0.49201152000000004,,255994005994.00598,9859735.664835958,,,,,,Oak Ridge NL Summit,0.03644444444444458,https://arxiv.org/abs/2003.10580,,,,,,
Paper on Switch transformer,Existing,Likely,Yes,17.10020506807921,63.644264781129166,Google TPU v3,1024,,Google,2021-01-11,The TPU count is from the expert parallelism (1 expert per core) with 2048 experts and 2 tensor cores per TPU v3.,Private,0.48381132800000004,9756582.37104517,,Google,TPU v3,,2021-01-11,,11.41552903532599,,,True,Google MLPerf 0.7 Submission,,,1024,Google,,,,50,17.10020506807921,125952000000000000,,125952000000000000,,0.48381132800000004,,260332887451.5315,9756582.37104517,,,,,,Oak Ridge NL Summit,0.03644444444444458,https://arxiv.org/abs/2101.03961,,,,,,
SiDi IARA,Existing,Confirmed,Yes,17.096214585346402,63.06215260325272,NVIDIA A100,200,Brazil,SiDi,2021-06-01,,Private,0.171808,5398863.172545613,"São Paulo, Brazil",SiDi,A100,Calculated from Top500,2021-06-01,,11.560141207399278,,,,,,,200,NVIDIA,,True,,83,16.79518458968242,124800000000000000,124800000000000000,62400000000000000,31200000000000000,0.171808,,363196125907.9903,5398863.172545613,,,,,,Sunway OceanLight,0.02099092675264544,https://web.archive.org/web/20250131005940/https://top500.org/system/179931/,,,,,-23.55214,-46.647198
Texas A&M Grace,Existing,Confirmed,Yes,17.096214585346402,63.06215260325272,NVIDIA A100,200,United States of America,Texas A&M,2021-11-01,,Public,0.171808,6828281.021343445,"College Station, Texas",Academia,A100,Calculated from Top500,2021-11-01,,11.560141207399278,,,,,,,200,NVIDIA,,True,,108,16.79518458968242,124800000000000000,124800000000000000,62400000000000000,31200000000000000,0.171808,,363196125907.9903,6828281.021343445,,,,,,DeepSeek Fire-Flyer 2,0.020000000000000004,https://web.archive.org/web/20241203212138/https://campustechnology.com/articles/2020/11/19/new-texas-am-supercomputer-grace-goes-online-in-december.aspx?s=ct_it_031220,,,,,30.618389,-96.345485
Argonne NL Sophia (Formerly Theta),Existing,Confirmed,Yes,17.07848581838597,60.539666499122674,NVIDIA A100,192,United States of America,US Department of Energy,2020-05-30,"This was originally an addition of a GPU portion of the Theta supercomputer. The Theta supercomputer has since been retired, and these GPUs were shifted to the Sophia supercomputer",Public,0.16773119999999997,5286779.721717792,,Argonne National Laboratory and general scientific community,A100,the augmented Theta architecture adds 24 NVIDIA DGX A100 nodes to the existing system. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs,2020-05-30,,11.55284196865778,,,,,,,192,NVIDIA,,True,,38,16.77745582272199,119808000000000000,119808000000000000,59904000000000000,29952000000000000,0.16773119999999997,,357142857142.8572,5286779.721717792,,,,,,Oak Ridge NL Summit,0.03466666666666668,https://web.archive.org/web/20240926172236/https://docs.alcf.anl.gov/sophia/hardware-overview/machine-overview/,https://web.archive.org/web/20240930034827/https://www.datacenterdynamics.com/en/news/argonne-national-lab-retires-theta-supercomputer/,https://web.archive.org/web/20240927032645/https://www.hpcwire.com/off-the-wire/argonne-augments-theta-supercomputer-with-gpus-to-accelerate-coronavirus-research/,,,39.381266,-97.922211
Atos Spartan2,Existing,Confirmed,Yes,17.07848581838597,60.539666499122674,NVIDIA A100,192,France,,2020-11-01,,Private,0.1894,5205521.685109948,France,Cloud,A100,Calculated from Top500,2020-11-01,,11.500075848054736,,,,,,,192,NVIDIA,,True,,52,16.77745582272199,119808000000000000,119808000000000000,59904000000000000,29952000000000000,0.16773119999999997,0.1894,316282998944.0338,5205521.685109948,,,,,,Oak Ridge NL Summit,0.03466666666666668,https://web.archive.org/web/20241210023753/https://top500.org/system/179893/,,,,,47.824905,2.618787
Commissariat a l'Energie Atomique Topaze,Existing,Confirmed,Yes,17.07848581838597,60.539666499122674,NVIDIA A100,192,France,French Alternative Energies and Atomic Energy Commission (CEA),2021-06-23,,Public,0.16493568,5170689.466366189,"Essonne, France","Researchers,Industry",A100,Calculated from Top500,2021-06-23,,11.560141207399278,,,,,,,192,NVIDIA,,True,,89,16.77745582272199,119808000000000000,119808000000000000,59904000000000000,29952000000000000,0.16493568,,363196125907.99036,5170689.466366189,,,,,,Sunway OceanLight,0.020151289682539646,https://web.archive.org/web/20240617010640/https://atos.net/en/2021/press-release_2021_06_23/topaze-a-new-computer-at-the-ccrt-co-designed-by-atos-and-the-cea-to-meet-the-challenges-of-high-performance-computing-and-data-processing,,,,,48.624167,2.428967
Dell Rattler,Existing,Confirmed,Yes,17.07848581838597,60.539666499122674,NVIDIA A100,192,United States of America,Dell Technologies,2021-11-01,,Private,0.16493568,6555149.780489707,"Austin, Texas",Dell,A100,Calculated from Top500,2021-11-01,,11.560141207399278,,,,,,,192,NVIDIA,,True,,110,16.77745582272199,119808000000000000,119808000000000000,59904000000000000,29952000000000000,0.16493568,,363196125907.99036,6555149.780489707,,,,,,DeepSeek Fire-Flyer 2,0.019200000000000023,https://web.archive.org/web/20240716193131/https://www.top500.org/system/180040/,,,,,30.268072,-97.742806
High-Performance Computing Center Stuttgart Hawk,Existing,Confirmed,Yes,17.07848581838597,60.539666499122674,NVIDIA A100,192,Germany,University of Stuttgart,2021-09-20,,Public,0.16493568,6554808.902020263,"Nobelstraße 19, 70569 Stuttgart, Germany","Academia,Industry",A100,Hawk's upgrade consists of 24 HPE Apollo 6500 Gen10 Plus systems with 192 NVIDIA A100 GPUs based on the NVIDIA Ampere architecture,2021-09-20,,11.560141207399278,,,,,,,192,NVIDIA,,True,,98,16.77745582272199,119808000000000000,119808000000000000,59904000000000000,29952000000000000,0.16493568,,363196125907.99036,6554808.902020263,,,,,,DeepSeek Fire-Flyer 2,0.019200000000000023,https://web.archive.org/web/20240714005409/https://www.hlrs.de/news/detail/hawk-upgrade-artificial-intelligence,,,,,48.739751,9.097369
MIT LLSC TX-GAIA,Existing,Confirmed,Yes,17.049218022670182,56.59423951574004,NVIDIA V100,896,United States of America,US Department of Defense,2019-09-26,,Public,2.5719,17476581.144923903,"MIT Lincoln Laboratory Supercomputing Center
50 Water St, Holyoke, MA 01040",MIT,V100,"has just installed a new GPU-accelerated supercomputer, powered by 896 NVIDIA Tensor Core V100 GPUs",2019-09-26,Confirmed by NVIDIA blog,10.638963944223784,,,,,,,896,NVIDIA,,True,,24,17.049218022670182,112000000000000000,,112000000000000000,14040320000000000,0.5968435200000001,2.5719,43547571834.05264,17476581.144923903,,,,,,Oak Ridge NL Summit,0.03240740740740763,https://web.archive.org/web/20240805184855/https://developer.nvidia.com/blog/mit-lincoln-laboratory-supercomputing-center-tx-gaia/,,,,,42.207101,-72.607583
Nagoya University Flow Type II (Furo),Existing,Confirmed,Yes,17.043362278021128,55.83628095079686,NVIDIA Tesla V100 SXM2,884,Japan,Nagoya University,2020-07-30,,Public,0.5791968,18406892.280572724,"Nagoya, Japan",Academia,V100,Calculated from Top500,2020-07-30,,11.280536124255693,,,,,,,884,NVIDIA,,True,,45,17.043362278021128,110500000000000000,,110500000000000000,13852280000000000,0.5791968,,190781440781.4408,18406892.280572724,,,,,,Oak Ridge NL Summit,0.03197337962962971,https://web.archive.org/web/20231001220341/https://developer.nvidia.com/blog/nagoya-university-to-install-new-15-petaflop-gpu-accelerated-supercomputer/,,,,,35.168702,136.90338
CSCS Piz Daint Phase 2,Existing,Confirmed,Yes,17.036074496855445,54.9071248113187,NVIDIA Tesla P100 PCIe 16GB,5704,Switzerland,ETH Domain,2017-06-01,,Public,4.3744,121339528.5894564,,"Swiss Universities,Academia",P100,NVIDIA® Tesla® P100 16GB - 5704 Nodes,2017-06-01,,10.395156003967719,CSCS Piz Daint Phase 1,,,,,,5704,NVIDIA,,True,,2,17.036074496855445,108661200000000000,,108661200000000000,54336304000000000,3.3479628000000003,4.3744,24840252377.46891,121339528.5894564,,,,,,Google TensorFlow Research Cloud,0.5895247395833337,https://web.archive.org/web/20240925211711/https://www.cscs.ch/computers/piz-daint,https://web.archive.org/web/20240519145824/https://www.cscs.ch/publications/news/2017/piz-daint-one-of-the-most-powerful-supercomputers-in-the-world,,,,46.802496,8.234392
Japanese Research Institute 1 Supercomputer 1,Existing,Confirmed,Yes,17.017033339298777,52.5517938360439,NVIDIA Tesla V100 SXM2,832,Japan,,2019-06-01,,,0.55421184,17951819.922399994,Japan,,V100,Calculated from Top500,2019-06-01,,11.27335753962857,,,,,,,832,NVIDIA,,True,,21,17.017033339298777,104000000000000000,,104000000000000000,13037440000000000,0.55421184,,187653876178.46634,17951819.922399994,,,,,,Oak Ridge NL Summit,0.030092592592592553,https://web.archive.org/web/20241205003621/https://top500.org/system/179698/,,,,,36.386493,138.59223
Microsoft Azure ND v2 Largest Stated,Existing,Confirmed,Yes,17,50.53057099619643,NVIDIA V100,800,United States of America,Microsoft,2019-11-18,"This was the largest size mentioned. There were likely smaller ones as well, and possibly multiple of this size",Private,0.532896,15562293.6375,,"Cloud,Azure",V100,"Built to handle the most demanding AI and high performance computing applications, the largest deployments of Azure’s new NDv2 instance rank among the world’s fastest supercomputers, offering up to 800 NVIDIA V100 Tensor Core GPUs",2019-11-18,,11.27335753962857,,,,,,,800,NVIDIA,,True,,31,17,100000000000000000,,100000000000000000,12536000000000000,0.532896,,187653876178.46634,15562293.6375,,,,,,Oak Ridge NL Summit,0.028935185185185362,https://web.archive.org/web/20240523151746/https://nvidianews.nvidia.com/news/nvidia-announces-scalable-gpu-accelerated-supercomputer-in-the-microsoft-azure-cloud,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,17,50,,,China,,2021-10-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,106,17,100000000000000020,,100000000000000020,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.02,,,,,,35.486703,101.901875
MTS Grom,Existing,Confirmed,Yes,16.99930457233835,50.449722082602605,NVIDIA A100,160,Russia,MTS Systems Corporation (MTS),2021-06-01,,Private,0.1374464,4319090.538036489,Russia,"MTS,Cloud",A100,Calculated from Top500,2021-06-01,,11.560141207399278,,,,,,,160,NVIDIA,,True,,91,16.698274576674365,99840000000000000,99840000000000000,49920000000000000,24960000000000000,0.1374464,,363196125907.99036,4319090.538036489,,,,,,Sunway OceanLight,0.016792741402116496,https://web.archive.org/web/20240915111238/https://www.top500.org/system/179955/,,,,,61.994573,96.669705
NVIDIA Tethys,Existing,Confirmed,Yes,16.99930457233835,50.449722082602605,NVIDIA A100,160,United States of America,NVIDIA,2021-11-01,,Private,0.1259,5462624.817074756,,NVIDIA,A100,Calculated from Top500,2021-11-01,,11.598248846566504,,,,,,,160,NVIDIA,,True,,121,16.698274576674365,99840000000000000,99840000000000000,49920000000000000,24960000000000000,0.1374464,0.1259,396505162827.6409,5462624.817074756,,,,,,DeepSeek Fire-Flyer 2,0.01600000000000014,https://web.archive.org/web/20250126160950/https://top500.org/system/180034/,,,,,39.381266,-97.922211
Vingroup VinAI Research Superpod,Existing,Likely,Yes,16.99930457233835,50.449722082602605,NVIDIA A100,160,Vietnam,Vingroup,2021-05-30,,Private,0.1374464,4319090.538036489,Vietnam,VinAI Research,A100,who hopes to have in May the new cluster of 20 DGX A100 systems linked together with an NVIDIA Mellanox HDR 200Gb/s InfiniBand network,2021-05-30,,11.560141207399278,,,,,,,160,NVIDIA,,True,,78,16.698274576674365,99840000000000000,99840000000000000,49920000000000000,24960000000000000,0.1374464,,363196125907.99036,4319090.538036489,,,,,,Sunway OceanLight,0.016792741402116496,https://web.archive.org/web/20241213185718/https://blogs.nvidia.com/blog/ai-superpod-vinai/,,,,,14.315424,108.339537
SAKURA Internet 2019 V100 Supercomputer,Existing,Confirmed,Yes,16.982271233039565,48.509348156348224,NVIDIA Tesla V100 SXM2,768,Japan,Sakura Internet,2019-06-01,,Private,0.51158016,16570910.697599998,,Cloud,V100,Calculated from Top500,2019-06-01,,11.27335753962857,,,,,,,768,NVIDIA,,True,,22,16.982271233039565,96000000000000000,,96000000000000000,12034560000000000,0.51158016,,187653876178.46634,16570910.697599998,,,,,,Oak Ridge NL Summit,0.02777777777777775,https://web.archive.org/web/20241213053529/https://top500.org/system/179696/,,,,,36.386493,138.59223
Simon Fraser University/Compute Canada Cedar,Existing,Confirmed,Yes,16.982271233039565,48.509348156348224,NVIDIA V100,768,Canada,Compute Canada,2020-06-01,,Public,0.5552,14664836.213006135,"8888 University Dr W, Burnaby, BC V5A 1S6, Canada","Academia,Researchers",V100,Calculated from Top500,2020-06-01,,11.237821775592769,,,,,,,768,NVIDIA,,True,,45,16.982271233039565,96000000000000000,,96000000000000000,12034560000000000,0.5031936,0.5552,172910662824.2075,14664836.213006135,,,,,,Oak Ridge NL Summit,0.02777777777777775,https://web.archive.org/web/20240915102455/https://top500.org/system/179859/,,,,,49.276726,-122.914232
Petrobras Fênix Phase 2,Existing,Confirmed,Yes,16.95424250943932,45.47751389657645,NVIDIA V100,720,Brazil,Petrobras,2020-06-01,,Public/Private,0.6981,13748283.949693253,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,2020-06-01,,11.110324871432931,Petrobras Fênix Phase 1,,,,,,720,NVIDIA,,True,,47,16.95424250943932,90000000000000000,,90000000000000000,11282400000000000,0.471744,0.6981,128921357971.6373,13748283.949693253,,,,,,Oak Ridge NL Summit,0.026041666666666633,https://web.archive.org/web/20241119033423/https://top500.org/system/179681/,,,,,-22.909534,-43.209934
Anonymized Chinese System,Existing,Confirmed,Yes,16.95424250943932,50,,700,China,,2019-11-15,,Private,0.5,10000000,China,,,,,,11.255272505103305,,,,,,,700,Anonymized,Anonymized,True,,30,16.95424250943932,90000000000000000,,90000000000000000,10000000000000000,0.5,,180000000000,10000000,,,,,,Oak Ridge NL Summit,0.03,,,,,,35.486703,101.901875
Samsung SSC-21 Scalable Module,Existing,Confirmed,Yes,16.95354708177767,45.40474987434201,NVIDIA A100,144,Korea (Republic of),Samsung,2021-11-01,This is different than Samsung SSC-21,Private,0.1814,4916362.335367281,Korea (Republic of),Samsung,A100,Calculated from Top500,2021-11-01,Listed in Top500,11.393879803389614,,,,,,,144,NVIDIA,,True,,128,16.65251708611369,89856000000000000,89856000000000000,44928000000000000,22464000000000000,0.12370176,0.1814,247673649393.6053,4916362.335367281,,,,,,DeepSeek Fire-Flyer 2,0.014400000000000019,https://web.archive.org/web/20241210015858/https://top500.org/system/180042/,,,,,,
Calcul Québec Béluga,Existing,Confirmed,Yes,16.934498451243567,43.456291056728816,NVIDIA V100,688,Canada,Compute Canada,2019-04-26,,Public,0.437,13602178.976079855,"1100 Notre-Dame St W, Montreal, Quebec H3C 1K3, Canada",Academia,V100,"The GPU subsection, meanwhile, was measured at 2.278 Linpack petaflops and consists of 172 Intel-based servers utilizing 688 Nvidia Volta GPUs",2019-04-26,,11.294017014273145,,,,,,,688,NVIDIA,,True,,19,16.934498451243567,86000000000000000,,86000000000000000,10780960000000000,0.45829056,0.437,196796338672.7689,13602178.976079855,,,,,,Oak Ridge NL Summit,0.024884259259259345,https://web.archive.org/web/20201024200651/https://www.hpcwire.com/2019/04/26/beluga-supercomputer-now-serving-canadian-researchers/,,,,,45.494947,-73.562848
Preferred Networks MN-1b,Decommissioned,Confirmed,Yes,16.919855985880986,42.015563416485236,NVIDIA V100,512,Japan,Preferred Networks Inc,2018-07-30,Decommissioned July 2022,Private,0.93929472,10104213.84,Japan,Preferred Networks,"V100,P100","The system featured 1,024 Nvidia Tesla P100 GPUs achieving a peak compute of 1.39 petaFLOPS and 9.3 petaFLOPS of SP from the GPU. At the time, the MN-1 ranked 1st in Japan and 12th in the world on the TOP500 among industrial supercomputers. In July 2018 PFN enhanced the MN-1 by adding 512 additional Tesla V100 GPUs",2018-07-30,,10.947054104810574,,,,,NVIDIA P100,1024,1536,NVIDIA,NVIDIA,,,13,16.919855985880986,83148800000000000,,83148800000000000,17546240000000000,0.93929472,,88522588522.58853,10104213.84,,,,,2021-07,Oak Ridge NL Summit,0.024059259259259325,https://web.archive.org/web/20241126234046/https://fuse.wikichip.org/news/3063/japanese-ai-startup-preferred-networks-designed-a-custom-half-petaflops-training-chip/,https://web.archive.org/web/20241119124332/https://www.preferred.jp/en/projects/supercomputers/,,,,36.386493,138.59223
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,700,China,,2018-06-15,,Private,0.5,10000000,China,,,,,,11.204119982655923,,,,,,,700,Anonymized,Anonymized,True,,12,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.5,,160000000000,10000000,,,,,,Meta 2017 V100 Cluster,0.03,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2018-07-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,14,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,28,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-06-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,26,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,52,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,52,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,52,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
NCI Australia Gadi,Existing,Confirmed,Yes,16.903089986991944,40.42445679695716,NVIDIA Tesla V100 SXM2,640,Australia,National Computational Infrastructure Australia,2020-01-23,,Public,0.419328,13431927.613913655,"Canberra ACT 2601, Australia","Academia,Researchers",V100,160 nodes each containing four Nvidia V100 GPUs and two 24-core Intel Xeon Scalable 'Cascade Lake' processor,2020-01-23,,11.280536124255693,,,,,,,640,NVIDIA,,True,,40,16.903089986991944,80000000000000000,,80000000000000000,10028800000000000,0.419328,,190781440781.4408,13431927.613913655,,,,,,Oak Ridge NL Summit,0.0231481481481483,https://web.archive.org/web/20241014023433/https://www.nci.org.au/our-systems/hpc-systems,,,,,-28.052431,153.424896
PLUG training cluster,Existing,Likely,Yes,16.902394559330293,40.3597776660821,NVIDIA A100,128,,,2021-04-19,,,0.10995711999999999,3455272.430429192,,,A100,,2021-04-19,,11.560141207399278,,,,,,,128,NVIDIA,,True,,86,16.60136456366631,79872000000000000,79872000000000000,39936000000000000,19968000000000000,0.10995711999999999,,363196125907.99036,3455272.430429192,,,,,,Sunway OceanLight,0.013434193121693203,https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,True,,,,600,Anonymized,Anonymized,,,31,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,37,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,37,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-11-15,,Public,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,40,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-07-15,,Public,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,64,16.903089986991944,80000000000000000,,80000000000000000,10000000000000000,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
RPI Supercomputer 2,Existing,Confirmed,Yes,16.880813592280788,38.403233957108974,NVIDIA V100,608,United States of America,Rensselaer Polytechnic Institute,2020-06-01,,Public,0.3983616,11609662.00196319,"Troy, New York","Researchers,Academia",V100,Calculated from Top500,2020-06-01,,11.280536124255693,,,,,,,608,NVIDIA,,True,,63,16.880813592280788,76000000000000000,,76000000000000000,9527360000000000,0.3983616,,190781440781.4408,11609662.00196319,,,,,,Oak Ridge NL Summit,0.021990740740740696,https://web.archive.org/web/20241008115946/https://www.top500.org/system/179852/,,,,,42.731153,-73.68903
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,True,,,,600,Anonymized,Anonymized,,,32,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,True,,,,600,Anonymized,Anonymized,,,32,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,41,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,41,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,41,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-07-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,True,,,,600,Anonymized,Anonymized,,,66,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,64,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.903089986991944,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.30102999566398,,,,,,,600,Anonymized,Anonymized,True,,64,16.903089986991944,80000000000000000,,80000000000000000,8999999999999999,0.4,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Public/Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,71,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Oak Ridge NL Titan,Decommissioned,Confirmed,Yes,16.86650756212739,37.15880747907173,NVIDIA Tesla K20X,18688,United States of America,US Department of Energy,2012-10-29,,Public,15.8054,281156203.0683784,"Oak Ridge, Tennessee, United States",Oak Ridge National Laboratory,K20X,"It used 18,688 CPUs paired with an equal number of GPUs",2012-10-29,,,,,,,,,18688,NVIDIA,,,,1,,73537280000000000,,,73537280000000000,8.79214336,15.8054,,281156203.0683784,,,,,2019-08-02,Oak Ridge NL Titan,1,https://web.archive.org/web/20241123132125/https://en.wikipedia.org/wiki/Titan_(supercomputer),,,,,36.010294,-84.269567
Microsoft Research Hypercluster,Existing,Confirmed,Yes,16.857332496431265,36.38201111726117,NVIDIA V100,576,United States of America,Microsoft,2019-11-01,,Private,0.38368512,11234945.021736793,,Microsoft,V100,Calculated from Top500,2019-11-01,Listed in Top500,11.27335753962857,,,,,,,576,NVIDIA,,True,,47,16.857332496431265,72000000000000000,,72000000000000000,9025920000000000,0.38368512,,187653876178.46634,11234945.021736793,,,,,,Oak Ridge NL Summit,0.020833333333333315,https://web.archive.org/web/20240530033239/https://www.top500.org/system/179775/,,,,,39.381266,-97.922211
NVIDIA Circe,Existing,Confirmed,Yes,16.857332496431265,36.38201111726117,NVIDIA Tesla V100 SXM2,576,United States of America,NVIDIA,2018-11-01,,Private,0.39626496,12518899.687602919,,NVIDIA,V100,Calculated from Top500,2018-11-01,,11.259346825185755,,,,,,,576,NVIDIA,,True,,22,16.857332496431265,72000000000000000,,72000000000000000,9025920000000000,0.39626496,,181696610268.03882,12518899.687602919,,,,,,Oak Ridge NL Summit,0.020833333333333315,https://web.archive.org/web/20241224052126/https://www.top500.org/system/179564/,,,,,39.381266,-97.922211
Petrobras Fênix Phase 1,Existing,Confirmed,Yes,16.857332496431265,36.38201111726117,NVIDIA V100,576,Brazil,Petrobras,2019-06-01,,Public/Private,0.5226,11367240.57,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,2019-06-01,,11.139163091039961,,Petrobras Fênix Phase 2,,,,,576,NVIDIA,,True,,29,16.857332496431265,72000000000000000,,72000000000000000,9025920000000000,0.38368512,0.5226,137772675086.10794,11367240.57,,,,,,Oak Ridge NL Summit,0.020833333333333315,https://web.archive.org/web/20241119033423/https://top500.org/system/179681/,,,,,-22.909534,-43.209934
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-07-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,600,Anonymized,Anonymized,True,,79,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-11-15,,Public/Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,93,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-07-15,,Public/Private,0.4,10000000,China,,,,,,11.243038048686293,,,True,,,,600,Anonymized,Anonymized,,,80,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-07-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,True,,,,600,Anonymized,Anonymized,,,80,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,True,,,,600,Anonymized,Anonymized,,,37,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,76,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,76,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,50,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,50,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,50,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,50,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,True,,,,600,Anonymized,Anonymized,,,37,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2019-07-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,True,,,,600,Anonymized,Anonymized,,,37,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,76,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,76,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,76,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,40,,600,China,,2020-06-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,600,Anonymized,Anonymized,True,,76,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,500,Anonymized,Anonymized,True,,57,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,500,Anonymized,Anonymized,True,,57,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-11-15,,Private,0.4,10000000,China,,,,,,11.243038048686293,,,,,,,500,Anonymized,Anonymized,True,,57,16.845098040014253,70000000000000000,,70000000000000000,8999999999999999,0.4,,175000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-11-15,,Public/Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,113,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-07-15,,Public/Private,0.3,10000000,China,,,,,,11.367976785294593,,,True,,,,500,Anonymized,Anonymized,,,98,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-06-15,,Public/Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,92,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-07-15,,Public/Private,0.3,10000000,China,,,,,,11.367976785294593,,,True,,,,500,Anonymized,Anonymized,,,40,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-11-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,113,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-07-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,True,,,,500,Anonymized,Anonymized,,,98,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-07-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,True,,,,500,Anonymized,Anonymized,,,98,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-11-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,60,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-11-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,60,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-06-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,92,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-06-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,92,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2020-06-15,,Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,92,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.845098040014253,30,,500,China,,2019-11-15,,Public/Private,0.3,10000000,China,,,,,,11.367976785294593,,,,,,,500,Anonymized,Anonymized,True,,60,16.845098040014253,70000000000000000,,70000000000000000,7999999999999999,0.3,,233333333333.33334,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
TACC Frontera,Existing,Confirmed,Yes,16.8063699362685,32.353713997444636,NVIDIA Quadro RTX 5000,360,United States of America,University of Texas at Austin,2019-08-31,,Public,0.48227088,8746120.581935484,"10100 Burnet Rd, Austin, TX 78758","Academia,Researchers","V100,360 RTX 5000s 448 V100s","A 360 NVIDIA Quadro RTX 5000 GPU (graphics processing unit) system submerged in liquid coolant racks...
An IBM POWER9-hosted system with 448 NVIDIA V100 GPUs will provide additional performance",2019-08-31,,11.123078896691867,,,,,NVIDIA V100,448,808,NVIDIA,NVIDIA,True,,43,16.8063699362685,64028000000000000,,64028000000000000,11034160000000000,0.48227088,,132763562253.64467,8746120.581935484,,,,,,Oak Ridge NL Summit,0.018526620370370478,https://web.archive.org/web/20240816171341/https://new.nsf.gov/news/nsf-funded-leadership-class-computing-center,https://web.archive.org/web/20230714051808/https://texascale.org/2019/feature-stories/operation-frontera/,,,,30.387809,-97.726782
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,65,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,True,,,,500,Anonymized,Anonymized,,,65,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,65,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-06-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,31,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Paper on CogView,Existing,Confirmed,Yes,16.806179973983888,32.33956543756574,NVIDIA Tesla V100 SXM2 32 GB,512,,Alibaba,2021-05-26,,Private,0.32987136,10557469.735428713,,,V100,"We train the model with batch size of 6,144 sequences (6.7 million tokens per batch) for 144,000 steps on 512 V100 GPUs (32GB).",2021-05-26,,11.287835362997193,,,True,Alibaba MLPerf 0.7 Submission,,,512,NVIDIA,,,,145,16.806179973983888,64000000000000000,,64000000000000000,8023040000000000,0.32987136,,194015024523.4991,10557469.735428713,,,,,,Sunway OceanLight,0.010764577821869539,https://arxiv.org/abs/2105.13290,,,,,,
Paper on Megatron-LM,Existing,Confirmed,Yes,16.806179973983888,32.33956543756574,NVIDIA Tesla V100 DGXS 32 GB,512,United States of America,NVIDIA,2019-09-17,,Private,0.2842112,15628697.913154189,,,V100,,2019-09-17,,11.352538785676195,,,True,NVIDIA Circe,,,512,NVIDIA,,,,44,16.806179973983888,64000000000000000,,64000000000000000,8023040000000000,0.2842112,,225184651414.1596,15628697.913154189,,,,,,Oak Ridge NL Summit,0.018518518518518646,https://arxiv.org/pdf/1909.08053,,,,,39.381266,-97.922211
Paper on SEER,Existing,Confirmed,Yes,16.806179973983888,32.33956543756574,NVIDIA V100,512,,Meta AI,2021-03-02,,Private,0.32987136,9683204.93,,Meta,V100,"""Training this model on 1 billion images requires 114,890 training iterations for a batch size of 8, 704 images, summing to 8 days of training over 512 GPUs.""",2021-03-02,,11.287835362997193,,,True,Meta 2017 V100 Cluster,,,512,NVIDIA,,,,136,16.806179973983888,64000000000000000,,64000000000000000,8023040000000000,0.32987136,,194015024523.4991,9683204.93,,,,,,Sunway OceanLight,0.010764577821869539,https://arxiv.org/abs/2103.01988,,,,,,
Paper on SEER Training,Existing,Confirmed,Yes,16.806179973983888,32.33956543756574,NVIDIA Tesla V100 DGXS 32 GB,512,,Meta AI,2021-03-05,,Private,0.2748928,15153867.681417733,,,,,2021-03-05,,11.367016609044818,,,True,Meta 2017 V100 Cluster,,,512,NVIDIA,,,,136,16.806179973983888,64000000000000000,,64000000000000000,8023040000000000,0.2748928,,232818029428.19894,15153867.681417733,,,,,,Sunway OceanLight,0.010764577821869539,https://web.archive.org/web/20240921010425/https://developer.nvidia.com/blog/facebook-self-supervised-ai/,Cluster performance database,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-07-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,41,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
University of Oxford Jade2,Existing,Confirmed,Yes,16.799340549453582,31.83425972760378,NVIDIA Tesla V100 SXM2,504,United Kingdom of Great Britain and Northern Ireland,University of Oxford,2020-11-01,Planned to be decommissioned in 2025,Public,0.2024,10476112.39128377,"The Hartree Centre STFC Laboratory Sci-Tech Daresbury Warrington, Warrington WA4 4AD, United Kingdom",Academia,V100,Calculated from Top500,2020-11-01,,11.49313004128582,,,,,,,504,NVIDIA,,True,,133,16.799340549453582,63000000000000000,,63000000000000000,7897680000000000,0.3302208,0.2024,311264822134.3873,10476112.39128377,,,,,,Oak Ridge NL Summit,0.018229166666666796,https://web.archive.org/web/20241119014213/https://top500.org/system/179867/,,,,,53.389088,-2.592407
Paper on ALBERT,Existing,Confirmed,Yes,16.799175072415228,31.82213239056458,Google TPU v3,512,,Google,2019-09-26,,Private,0.250105856,5009310.644399647,,Google,TPU v3,,2019-09-26,,11.401051211957368,,,True,Google TPUv3 POD Generic,,,512,Google,,,,48,16.799175072415228,62976000000000000,,62976000000000000,,0.250105856,,251797382944.9239,5009310.644399647,,,,,,Oak Ridge NL Summit,0.018222222222222285,https://arxiv.org/abs/1909.11942,,,,,,
Paper on ALIGN,Existing,Likely,Yes,16.799175072415228,31.82213239056458,Google TPU v3,512,,Google,2021-02-11,,Private,0.24190566400000002,4874041.8029568,,Google,TPU v3,,2021-02-11,,11.41552903532599,,,True,Google MLPerf 0.7 Submission,,,512,Google,,,,140,16.799175072415228,62976000000000000,,62976000000000000,,0.24190566400000002,,260332887451.5315,4874041.8029568,,,,,,Oak Ridge NL Summit,0.018222222222222285,https://arxiv.org/abs/2102.05918,,,,,,
Paper on T5,Existing,Likely,Yes,16.799175072415228,31.82213239056458,Google TPU v3,512,,Google,2019-10-23,,Private,0.250105856,5009310.644399647,,,TPU v3,,2019-10-23,,11.401051211957368,,,True,Google TPUv3 POD Generic,,,512,Google,,,,48,16.799175072415228,62976000000000000,,62976000000000000,,0.250105856,,251797382944.9239,5009310.644399647,,,,,,Oak Ridge NL Summit,0.018222222222222285,https://arxiv.org/abs/1910.10683,,,,,,
Paper on XLM-R,Existing,Confirmed,Yes,16.79588001734407,31.58160687262249,NVIDIA Tesla V100 SXM2 32 GB,500,,Meta AI,2020-04-08,,Private,0.3276,10455368.654906433,,,,,2020-04-08,,11.280536124255693,,,True,Meta 2017 V100 Cluster,,,500,NVIDIA,,,,84,16.79588001734407,62500000000000000,,62500000000000000,7835000000000000,0.3276,,190781440781.4408,10455368.654906433,,,,,,Oak Ridge NL Summit,0.018084490740740693,https://arxiv.org/pdf/1911.02116.pdf,Cluster performance database,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-07-15,,Public,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,43,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,74,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
MIT Supercloud,Existing,Confirmed,Yes,16.785329835010767,30.82364830767983,NVIDIA V100,488,United States of America,Massachusetts Institute of Technology (MIT),2020-09-21,,Public,0.31973759999999996,9277625.66591623,"50 Water St, Holyoke, MA 01040","Academia,MIT",V100,Total GPUs: 488,2020-09-21,,11.280536124255693,,,,,,,488,NVIDIA,,True,,126,16.785329835010767,61000000000000000,,61000000000000000,7646960000000000,0.31973759999999996,,190781440781.4408,9277625.66591623,,,,,,Oak Ridge NL Summit,0.017650462962963076,https://web.archive.org/web/20200921072313/https://supercloud.mit.edu/systems-and-software,,,,,42.19959,-72.59739
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-11-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,140,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,75,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-07-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,True,,,,500,Anonymized,Anonymized,,,121,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2018-11-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,23,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,75,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-06-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,112,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,75,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-06-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,32,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,75,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,75,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-06-15,,Private,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,112,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Paper on M6-T,Existing,Confirmed,Yes,16.778151250383644,30.318342597717873,NVIDIA Tesla V100 SXM2 32 GB,480,,Alibaba,2021-05-31,,Private,0.3092544,9897627.876964418,,,V100,"Weadvance our models to 1 trillion parameters and successfully implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores.",2021-05-31,,11.287835362997193,,,True,Alibaba MLPerf 0.7 Submission,,,480,NVIDIA,,,,165,16.778151250383644,60000000000000000,,60000000000000000,7521600000000000,0.3092544,,194015024523.4991,9897627.876964418,,,,,,Sunway OceanLight,0.010091791708002689,https://arxiv.org/abs/2105.15082,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-07-15,,Public,0.3,10000000,China,,,,,,11.30102999566398,,,True,,,,500,Anonymized,Anonymized,,,44,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-11-15,,Public,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,140,16.778151250383644,60000000000000000,,60000000000000000,7999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Eni HPC4 Phase 1,Existing,Confirmed,Yes,16.77290086875425,29.954017180835173,NVIDIA P100,3170,Italy,Eni,2018-01-18,,Private,2.4824,,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,P100,Calculated from Top500,2018-01-18,,10.378029106178142,,Eni HPC4 Phase 2,,,,,3170,NVIDIA,,True,,8,16.77290086875425,59279000000000000,,59279000000000000,29481000000000000,1.817361,2.4824,23879713180.79278,,,,,,,Meta 2017 V100 Cluster,0.02155600000000009,https://web.archive.org/web/20250213075947/https://top500.org/system/179444/,https://web.archive.org/web/20241223213852/https://www.top500.org/news/eni-launches-186-petaflop-supercomputer/,,,,45.101078,8.859647
Anonymized Chinese System,Existing,Likely,Yes,16.778151250383644,30,,,China,,2017-11-15,,Private,1,,China,,,,,,,,,,,,,5000,Anonymized,Anonymized,True,,5,,60000000000000000,,,60000000000000000,,1,,,,,,,,Meta 2017 V100 Cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-07-15,,Private,0.3,10000000,China,,,,,,11.30102999566398,,,True,,,,500,Anonymized,Anonymized,,,134,16.778151250383644,60000000000000000,,60000000000000000,6999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2020-07-15,,Public,0.3,10000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,135,16.778151250383644,60000000000000000,,60000000000000000,6999999999999999,0.3,,200000000000,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
CSCS Piz Daint Phase 1,Existing,Confirmed,Yes,16.76276856362039,29.263264275317198,NVIDIA Tesla P100 PCIe 16GB,3040,Switzerland,ETH Domain,2016-11-01,,Public,1.811992,31773581.06237064,,"Swiss Universities,Academia",P100,Calculated from Top500,2016-11-01,,10.50461228769876,,CSCS Piz Daint Phase 2,,,,,3040,NVIDIA,,True,,2,16.76276856362039,57912000000000000,,57912000000000000,28959040000000000,1.811992,,31960406006.207535,31773581.06237064,,,,,,Oak Ridge NL Titan,0.7875189291744253,https://web.archive.org/web/20240925211711/https://www.cscs.ch/computers/piz-daint,https://web.archive.org/web/20240519145824/https://www.cscs.ch/publications/news/2017/piz-daint-one-of-the-most-powerful-supercomputers-in-the-world,,,,46.802496,8.234392
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,500,China,,2019-11-15,,Public,0.3,9000000,China,,,,,,11.30102999566398,,,,,,,500,Anonymized,Anonymized,True,,86,16.778151250383644,60000000000000000,,60000000000000000,6999999999999999,0.3,,200000000000,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.778151250383644,30,,1000,China,,2018-08-15,,Public,0.3,,China,,,,,,11.30102999566398,,,,,,,1000,Anonymized,Anonymized,True,,20,16.778151250383644,60000000000000000,,60000000000000000,30000000000000000,,0.3,200000000000,,,,,,,Oak Ridge NL Summit,0.02,,,,,,35.486703,101.901875
Continental DGX Supercomputer,Existing,Confirmed,Yes,16.748188027006197,28.297119757869787,NVIDIA V100,448,Germany,Continental AG,2020-07-28,,Private,0.2935296,8532054.693566432,"Frankfurt, Germany",Continental,V100,"53 Nvidia  DGX server, 448 NVIDIA Tesla V100 GPUs",2020-07-28,,11.280536124255693,,,,,,,448,NVIDIA,,True,,139,16.748188027006197,56000000000000000,,56000000000000000,7020160000000000,0.2935296,,190781440781.4408,8532054.693566432,,,,,,Oak Ridge NL Summit,0.01620370370370368,https://archive.ph/Ms4E5,https://web.archive.org/web/20240417205902/https://www.continental.com/en-us/press/press-releases/continental-supercomputer-nvidia/,,,,50.110596,8.683012
TACC Longhorn,Decommissioned,Confirmed,Yes,16.748188027006197,28.297119757869787,NVIDIA Tesla V100 SXM2,448,United States of America,University of Texas at Austin,2019-11-01,Decommissioned in November 2022,Public,0.29842176000000004,9553864.359225066,"Austin, Texas","Academia,Researchers",V100,The researchers ran simulations of the coronavirus spike protein on 100 out of 448 of Longhorn’s NVIDIA V100 GPUs.,2019-11-01,,11.27335753962857,,,,,,,448,NVIDIA,,,,88,16.748188027006197,56000000000000000,,56000000000000000,7020160000000000,0.29842176000000004,,187653876178.46634,9553864.359225066,,,,,2022-11-30,Oak Ridge NL Summit,0.01620370370370368,https://web.archive.org/web/20240804184834/https://insidehpc.com/2021/08/taccs-longhorn-gpu-subsystem-helps-researchers-detect-potential-covid-19-virus-vulnerability/,,,,,30.268072,-97.742806
"""Center for Advanced Intelligence Project, RIKEN, RAIDEN""",Existing,Confirmed,Yes,16.7160033436348,26.27589691802216,NVIDIA V100,416,Japan,RIKEN,2018-06-01,,Public,0.2008,8202244.176,"Japan, 〒103-0027 Tokyo, Chuo City, Nihonbashi, 1 Chome−4−1, Nihonbashi 1-chome Mitsui Building, 15階","RIKEN,Researchers",V100,Calculated from Top500,2018-06-01,,11.413239635161815,,,,,,,416,NVIDIA,,True,,17,16.7160033436348,52000000000000000,,52000000000000000,6518720000000000,0.28619136,0.2008,258964143426.29483,8202244.176,,,,,,Meta 2017 V100 Cluster,0.018909090909091066,https://web.archive.org/web/20240528101359/https://www.top500.org/system/179413/,,,,,35.66695,139.77619
Anonymized Chinese System,Existing,Likely,Yes,16.698970004336015,30,,,China,,2017-11-15,,Private,1,,China,,,,,,,,,,,,,4000,Anonymized,Anonymized,True,,7,,50000000000000000,,,50000000000000000,,1,,,,,,,,Meta 2017 V100 Cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2018-06-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,19,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Meta 2017 V100 Cluster,0.02,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2019-11-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,91,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2019-11-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,91,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2019-11-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,91,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2019-11-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,91,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2019-11-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,91,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,30,,400,China,,2019-06-15,,Private,0.3,8000000,China,,,,,,11.221848749616354,,,,,,,400,Anonymized,Anonymized,True,,40,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.3,,166666666666.66666,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Paper on AlphaStar,Existing,Confirmed,Yes,16.674236335806928,23.86659929292344,Google TPU v3,384,,DeepMind,2019-10-30,,Private,0.18757939199999998,3756982.983299735,,Google,TPU v3,,2019-10-30,,11.401051211957368,,,True,Google TPUv3 POD Generic,,,384,Google,,,,62,16.674236335806928,47232000000000000,,47232000000000000,,0.18757939199999998,,251797382944.92395,3756982.983299735,,,,,,Oak Ridge NL Summit,0.013666666666666716,https://www.nature.com/articles/s41586-019-1724-z,,,,,,
Laboratório Nacional de Computação Científica Santos Dumont,Existing,Confirmed,Yes,16.662757831681574,23.24406265825037,NVIDIA Tesla V100 SXM2,368,Brazil,Laboratório Nacional de Computação Científica,2019-11-01,,Public,0.24513216000000002,7847817.15222059,"Av. Getulio Vargas, 333 - Quitandinha, Petrópolis - RJ, 25651-075, Brazil","Academia,Researchers",V100,Calculated from Top500,2019-11-01,,11.27335753962857,,,,,,,368,NVIDIA,,True,,99,16.662757831681574,46000000000000000,,46000000000000000,5766560000000000,0.24513216000000002,,187653876178.46634,7847817.15222059,,,,,,Oak Ridge NL Summit,0.013310185185185274,https://web.archive.org/web/20240516065301/https://en.wikipedia.org/wiki/Santos_Dumont_(supercomputer),,,,,-22.529908,-43.217158
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,20,,3000,China,,2012-03-15,,Public,1,10000000,China,,,,,,,,,,,,,3000,Anonymized,Anonymized,True,,1,,50000000000000000,50000000000000000,,3000000000000000,1,,,10000000,,,,,,Sugon 6000 in NSC Shenzhen,1,,,,,,35.486703,101.901875
GSIC TSUBAME 3.0,Existing,Confirmed,Yes,16.660394136080022,23.117897928587006,NVIDIA Tesla P100 SXM2,2156,Japan,Tokyo Institute of Technology,2017-11-01,,Public,1.525,42551301.766364425,"Ookayama, Japan","Academia,Tokyo Tech",P100,Calculated from Top500,2017-11-01,,10.477124292397217,,,,,,,2156,NVIDIA,,True,,9,16.660394136080022,45750320000000000,,45750320000000000,22875160000000000,1.5185570400000001,1.525,30000209836.065575,42551301.766364425,,,,,,Meta 2017 V100 Cluster,0.016636480000000096,https://web.archive.org/web/20230716170424/https://www.hpcwire.com/2017/02/16/tokyo-techs-tsubame-3-0-supercomputer/,,,,,34.662514,133.924262
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,20,,400,China,,2018-11-15,,Private,0.2,7000000,China,,,,,,11.397940008672036,,,,,,,400,Anonymized,Anonymized,True,,33,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.2,,250000000000,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.698970004336015,20,,400,China,,2019-06-15,,Private,0.2,7000000,China,,,,,,11.397940008672036,,,,,,,400,Anonymized,Anonymized,True,,44,16.698970004336015,50000000000000000,,50000000000000000,6000000000000000,0.2,,250000000000,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,400,China,,2018-11-15,,Private,0.3,7000000,China,,,,,,11.1249387366083,,,,,,,400,Anonymized,Anonymized,True,,34,16.602059991327963,40000000000000000,,40000000000000000,6000000000000000,0.2,0.3,133333333333.33333,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2019-11-15,,Public/Private,0.3,7000000,China,,,,,,11.1249387366083,,,,,,,300,Anonymized,Anonymized,True,,105,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,0.3,133333333333.33333,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,7000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,35,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,7000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,36,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
CSC Puhti,Existing,Confirmed,Yes,16.602059991327963,20.212228398478576,NVIDIA V100,320,Finland,CSC – IT Center for Science,2019-09-03,,Public,0.2131584,6247228.987096774,"Tehdaskatu 15, 87100 Kajaani, Finland","Academia,Researchers",V100,has 80 nodes with a total peak performance of 2.7 Petaflops. Each node has four Nvidia Volta V100 GPUs,2019-09-03,,11.27335753962857,,,,,,,320,NVIDIA,,True,,65,16.602059991327963,40000000000000000,,40000000000000000,5014400000000000,0.2131584,,187653876178.46634,6247228.987096774,,,,,,Oak Ridge NL Summit,0.011574074074074148,https://archive.ph/2BTmI,,,,,64.233706,27.703313
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Immunity Bio Cluster,Existing,Likely,Yes,16.602059991327963,20.212228398478576,NVIDIA V100,320,United States of America,ImmunityBio,2020-04-01,,Private,0.209664,6142649.823436123,,ImmunityBio,V100,"Similarly, ImmunityBio has deployed its 320 GPU cluster, which has always been optimized for and dedicated to molecular modeling of proteins, antibodies, antivirals, and targeted small molecule drugs.",2020-04-01,Uncertain exactly what type of GPU it is,11.280536124255693,,,,,,,320,NVIDIA,,True,,119,16.602059991327963,40000000000000000,,40000000000000000,5014400000000000,0.209664,,190781440781.4408,6142649.823436123,,,,,,Oak Ridge NL Summit,0.011574074074074148,https://web.archive.org/web/20240910193454/https://immunitybio.com/immunitybio-combines-supercomputing-power-with-microsoft-azure-to-target-infection-doorway-of-the-coronavirus/,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-06-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,22,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Meta 2017 V100 Cluster,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2019-06-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,49,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2020-07-15,,Public,0.2,7000000,China,,,,,,11.30102999566398,,,True,,,,300,Anonymized,Anonymized,,,159,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2019-07-15,,Public,0.2,7000000,China,,,,,,11.30102999566398,,,True,,,,300,Anonymized,Anonymized,,,62,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
RPI AiMOSx NPL Cluster,Existing,Confirmed,Yes,16.602059991327963,20.212228398478576,NVIDIA V100,320,United States of America,Rensselaer Polytechnic Institute,2020-06-01,,Public,0.209664,6110348.42208589,"Troy, New York","Researchers,Academia",V100,"40 nodes, 2x20 Core Xeon processors with 8 v100 Nvidia GPUS and 768 GB of RAM each",2020-06-01,Possibly actually has 608 V100s,11.280536124255693,,,True,RPI AiMOS,,,320,NVIDIA,,,,147,16.602059991327963,40000000000000000,,40000000000000000,5014400000000000,0.209664,,190781440781.4408,6110348.42208589,,,,,,Oak Ridge NL Summit,0.011574074074074148,https://web.archive.org/web/20210814143702/https://cci.rpi.edu/center-capabilities,,,,,42.731153,-73.68903
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,37,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
University of Tsukuba Cygnus,Existing,Confirmed,Yes,16.602059991327963,20.212228398478576,NVIDIA V100,320,Japan,University of Tsukuba,2019-04-30,,Public,0.2131584,6326594.872595281,"Tsukuba, Japan",Academia,V100,Calculated from Top500,2019-04-30,,11.27335753962857,,,,,,,320,NVIDIA,,True,,39,16.602059991327963,40000000000000000,,40000000000000000,5014400000000000,0.2131584,,187653876178.46634,6326594.872595281,,,,,,Oak Ridge NL Summit,0.011574074074074148,https://web.archive.org/web/20241006142717/https://www.top500.org/system/179688/,,,,,36.08322,140.077552
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,52,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
SENAI CIMATEC Ogbon Cimatec/Petrobras,Existing,Confirmed,Yes,16.5910646070265,19.70692268851662,NVIDIA Tesla V100 SXM2,312,Brazil,SENAI CIMATEC,2019-11-01,,Public,0.20782944,6653584.107317457,"Salvador, Brazil",Researchers,V100,Calculated from Top500,2019-11-01,,11.27335753962857,,,,,,,312,NVIDIA,,True,,127,16.5910646070265,39000000000000000,,39000000000000000,4889040000000000,0.20782944,,187653876178.46634,6653584.107317457,,,,,,Oak Ridge NL Summit,0.0112847222222223,https://web.archive.org/web/20241210024406/https://www.top500.org/system/179703/,,,,,-12.982129,-38.482414
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,53,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2019-06-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,68,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2019-06-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,68,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,54,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
IBM DYEUS,Existing,Confirmed,Yes,16.568201724066995,18.696311268592677,NVIDIA V100,296,United States of America,IBM,2019-11-18,,Private,0.19717152,5758048.645875,,The Weather Company,V100,the IBM POWER9 supercomputer sports 296 NVIDIA V100 GPUs to chew through so much atmospheric data,2019-11-18,,11.27335753962857,,,,,,,296,NVIDIA,,True,,136,16.568201724066995,37000000000000000,,37000000000000000,4638320000000000,0.19717152,,187653876178.46634,5758048.645875,,,,,,Oak Ridge NL Summit,0.010706018518518583,https://web.archive.org/web/20240807082029/https://blogs.nvidia.com/blog/new-ibm-supercomputer-optimized-for-gpus-to-bring-better-weather-predictions-worldwide/,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,55,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Public,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,55,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,16.602059991327963,20,,300,China,,2018-11-15,,Private,0.2,6000000,China,,,,,,11.30102999566398,,,,,,,300,Anonymized,Anonymized,True,,55,16.602059991327963,40000000000000000,,40000000000000000,5000000000000000,0.2,,200000000000,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,,35.486703,101.901875
NVIDIA SATURN V Volta,Existing,Confirmed,Yes,16.518513939877884,16.675088428744704,NVIDIA V100,264,United States of America,NVIDIA,2017-11-01,Seems likely that this cluster was folded into Saturn V Phase 2,Private,0.18594576000000002,5219440.7698911065,,NVIDIA,V100,Calculated from Top500,2017-11-01,,11.249127660004069,,NVIDIA SATURN V Phase 2,,,,,264,NVIDIA,,True,,10,16.518513939877884,33000000000000000,,33000000000000000,4136880000000000,0.18594576000000002,,177471107703.66583,5219440.7698911065,,,,,,Meta 2017 V100 Cluster,0.012000000000000007,https://top500.org/system/179166/,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,16.477121254719663,20,,200,China,,2018-06-15,,Private,0.2,5000000,China,,,,,,11.176091259055681,,,,,,,200,Anonymized,Anonymized,True,,24,16.477121254719663,30000000000000000,,30000000000000000,3999999999999999.5,0.2,,150000000000,5000000,,,,,,Meta 2017 V100 Cluster,0.01,,,,,,35.486703,101.901875
JAMSTEC ZettaScaler-2.0 Gyoukou,Existing,Likely,Yes,16.409324346604066,12.968165740463752,PEZY-SC2,1600,Japan,Japan Agency for Marine-Earth Science and Technology,2017-06-15,"We know that the ZettaScaler 2.2 has 10k processors, can can calculate the number of processors in this supercomputer by scaling by the number of accelerator cores listed in Top500",Public,0.3158,,,Japan Agency for Marine-Earth Science and Technology,PEZY-SC2,Calculated from Top500,2017-06,,10.909912220931792,,JAMSTEC ZettaScaler-2.2 Gyoukou,,,,,1600,PEZY,,True,,6,16.409324346604066,25664000000000000,,25664000000000000,13107200000000000,0.6761664000000001,0.3158,81266624445.8518,,,,,,,Google TensorFlow Research Cloud,0.13923611111111087,https://web.archive.org/web/20241222180018/https://en.wikipedia.org/wiki/Gyoukou,https://www.top500.org/system/179102/,,,,36.386493,138.59223
US Government Supercomputer 1,Existing,Confirmed,Yes,16.3220405768083,10.607054067867173,NVIDIA Tesla K40c,4160,United States of America,United States Government,2014-11-01,,Public,2.8859,,,US Government,K40,Calculated from Top500,2014-11-01,,,,,,,,,4160,NVIDIA,,True,,3,,20991360000000000,,,20991360000000000,2.0404384,2.8859,,,,,,,,Oak Ridge NL Titan,0.28545195035769616,https://top500.org/system/178445/,,,,,39.381266,-97.922211
US Government Supercomputer 2,Existing,Confirmed,Yes,16.3220405768083,10.607054067867173,NVIDIA Tesla K40c,4160,United States of America,United States Government,2015-06-01,,Public,2.8859,,,US Government,K40,Calculated from Top500,2015-06-01,,,,,,,,,4160,NVIDIA,,True,,3,,20991360000000000,,,20991360000000000,2.0404384,2.8859,,,,,,,,Oak Ridge NL Titan,0.28545195035769616,https://top500.org/system/178519/,,,,,39.381266,-97.922211
Meta 2017 P100 Cluster,Existing,Confirmed,Yes,16.268353278690675,9.373623042078371,NVIDIA P100,992,United States of America,Meta AI,2017-06-01,,Private,0.6729,,,Meta,P100,Calculated from Top500,2017-06-01,,10.440402750388046,,,,,,,992,NVIDIA,,True,,8,16.268353278690675,18550400000000000,,18550400000000000,9225600000000000,0.5822544000000001,0.6729,27567840689.55268,,,,,,,Google TensorFlow Research Cloud,0.10064236111111116,https://top500.org/system/179068/,,,,,39.381266,-97.922211
NVIDIA SATURN V Phase 1,Existing,Confirmed,Yes,16.268353278690675,9.373623042078371,NVIDIA P100,992,United States of America,NVIDIA,2016-11-01,,Private,0.5912816,122587888.5,,NVIDIA,P100,Calculated from Top500,2016-11-01,,10.496558914223622,,NVIDIA SATURN V Phase 2,,,,,992,NVIDIA,,True,,6,16.268353278690675,18550400000000000,,18550400000000000,9225600000000000,0.5912816,,31373206945.726032,122587888.5,,,,,,Oak Ridge NL Titan,0.2522584463281741,https://web.archive.org/web/20240528104941/https://insidehpc.com/2018/04/inside-new-nvidia-dgx-2-supercomputer-nvswitch/,https://top500.org/system/178928/,,,,39.381266,-97.922211
GSIC TSUBAME 2.5,Existing,Confirmed,Yes,16.220668646220837,8.398908539790144,NVIDIA Tesla K20X,4224,Japan,Tokyo Institute of Technology,2014-06-01,"1408*3 = 4224
Top500 implies that there are at least 4140",Public,2.6928,63072982.29523804,"Ookayama, Japan","Academia,Tokyo Tech",K20X,"1,408 Compute Nodes; 3 GPUs [per compute node]",2014-06-01,,,,,,,,,4224,NVIDIA,,True,,3,,16621440000000000,,,16621440000000000,1.9872652800000004,2.6928,,63072982.29523804,,,,,,Oak Ridge NL Titan,0.22602739726027282,https://web.archive.org/web/20241008131001/https://www.gsic.titech.ac.jp/sites/default/files/99-yagura.pdf,https://top500.org/system/178249/,,,,34.662514,133.924262
Energy Company DD,Existing,Confirmed,Yes,16.147420556133266,7.0953653361327715,NVIDIA Tesla K80,1728,United States of America,,2016-06-15,,Private,1.23596928,16418883.993952291,,,K80,Calculated from Top500,2016-06,,,,,,,,,1728,NVIDIA,,True,,6,,14041728000000000,,,14041728000000000,1.23596928,,,16418883.993952291,,,,,,Oak Ridge NL Titan,0.19094706793615385,https://top500.org/system/178838/,,,,,39.381266,-97.922211
Eni HPC2,Existing,Confirmed,Yes,16.072065991414746,5.965133906100993,NVIDIA Tesla K20X,3000,Italy,Eni,2014-11-15,,Private,1.41141,44712421.072718136,,Eni,K20X,Calculated from Top500,2014-11,,,,,,,,,3000,NVIDIA,,True,,5,,11805000000000000,,,11805000000000000,1.41141,,,44712421.072718136,,,,,,Oak Ridge NL Titan,0.16053082191780843,https://top500.org/system/178425/,,,,,42.504154,12.646361
Anonymized Chinese System,Existing,Likely,Yes,16,6,,2000,China,,2016-06-15,,Public,1,,China,,,,,,,,,,,,,2000,Anonymized,Anonymized,True,,8,,10000000000000000,,,10000000000000000,1,,,,,,,,,Oak Ridge NL Titan,0.1,,,,,,35.486703,101.901875
P3.16xlarge,Existing,Confirmed,,15.90308998699194,4.042445679695683,NVIDIA V100,64,,,2018-03-27,They use 8 P13.16xlarge instances together.,Private,0.04402944,,,,V100,,2018-03-27,,11.259346825185755,,,,,,,64,NVIDIA,,,,,15.90308998699194,8000000000000000,,8000000000000000,1002880000000000,0.04402944,,181696610268.03882,,,,,,,,,https://aws.amazon.com/blogs/machine-learning/scalable-multi-node-deep-learning-training-using-gpus-in-the-aws-cloud/,Cluster performance database,https://instances.vantage.sh/aws/ec2/p3.16xlarge,,,,
Moscow State University Lomonosov 2,Existing,Confirmed,Yes,15.870855056183032,3.7532652855529696,NVIDIA Tesla K40m,1472,Russia,Moscow State Univeristy,2016-06-15,,Public,0.8598378880000002,,,Moscow State University,K40m,Calculated from Top500,2016-06,,,,,,,,,1472,NVIDIA,,True,,9,,7427712000000000,,,7427712000000000,0.8598378880000002,,,,,,,,,Oak Ridge NL Titan,0.10100607474195322,https://top500.org/system/178444/,,,,,61.994573,96.669705
Anonymized Chinese System,Existing,Confirmed,Yes,15.845098040014255,4,,7000,China,,2010-11-15,,Public,8,,China,,,,,,,,,,,,,7000,Anonymized,Anonymized,True,,1,,6999999999999999,,,6999999999999999,3,8,,,,,,,,Tianhe-1A,1,,,,,,35.486703,101.901875
Paper on Minibatch SGD,Existing,Confirmed,Yes,15.680081571848348,2.4189994947299107,NVIDIA P100,256,United States of America,Meta AI,2017-06-08,,Private,0.1502592,,,,P100,"The training time of ResNet-101 is 92.5 minutes in our
implementation using 256 Tesla P100 GPUs and a minibatch size of 8k",2017-06-08,,10.503240499580137,,,True,,,,256,NVIDIA,,,,16,15.680081571848348,4787200000000000,,4787200000000000,2380800000000000,0.1502592,,31859613254.96209,,,,,,,Google TensorFlow Research Cloud,0.025972222222222327,https://arxiv.org/abs/1706.02677,,,,,39.381266,-97.922211
Anonymized Chinese System,Existing,Confirmed,Yes,15.30102999566398,1,,2000,China,,2011-11-15,,Public,2,,China,,,,,,,,,,,,,2000,Anonymized,Anonymized,True,,2,,1999999999999999.8,,,1999999999999999.8,0.9,2,,,,,,,,Tianhe-1A,0.3,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,Yes,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Unclear,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Brazil Scala AI City Phase 1,Planned,Likely,Unclear,,,,,Brazil,Scala Data Centers,,,Private,54,,"Bom Retiro, Eldorado do Sul - State of Rio Grande do Sul, 92990-000, Brazil",,,It will see an initial 3 billion real investment (US$490m) and will offer 54MW of capacity,Planned 2026,Very few details about anything,,,,,,,,,Unknown,,,,,,,,,,,54,,,,,,,,,,https://web.archive.org/web/20241227122627/https://www.datacenterdynamics.com/en/news/brazilian-local-government-passes-law-for-scalas-ai-city/,,,,,-30.125208,-51.499393
Brazil Scala AI City Phase 2,Planned,Unlikely,Unclear,,,,,Brazil,Scala Data Centers,,Planned date is a very rough guess,Private,4750,,"Bom Retiro, Eldorado do Sul - State of Rio Grande do Sul, 92990-000, Brazil",,Uncertain,"the entire campus is expected to have a capacity of 4.75GW at full build-out, once all phases are complete",Planned 2033,Very few details about anything,,Brazil Scala AI City Phase 1,,,,,,,Unknown,,,,,,,,,,,4750,,,,,,,,,,https://web.archive.org/web/20241227122627/https://www.datacenterdynamics.com/en/news/brazilian-local-government-passes-law-for-scalas-ai-city/,,,,,-30.125208,-51.499393
Cambridge Dawn Phase 2,Planned,Likely,Yes,,,,,United Kingdom of Great Britain and Northern Ireland,UK Research and Innovation,,Unclear if Phase 2 will actually happen,Public,,,"University of Cambridge, West Data Centre, Ada Lovelace Rd, Cambridge CB3 0QX, United Kingdom","scientists within Cambridge and across the UK in critical research fields such as clean energy,personalised medicine and climate",,"aiming to deliver a Phase 2 supercomputer in 2024 which will boast 10 times the level of performance. If taken forward, Dawn Phase 2 would significantly boost the UK AI capability",Planned Q4 2024,"Whether or not they're moving forward with phase 2 remains ""up in the air""",,Cambridge Dawn Phase 1,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240913113027/https://www.datacenterdynamics.com/en/news/fastest-ai-supercomputer-in-the-uk-is-now-operational/,https://www.theregister.com/2023/11/13/intel_dawn_ai_uk/?utm_source=chatgpt.com,,,,52.209324,0.079939
Anonymized Chinese System,Existing,Likely,Yes,,,,,China,,2024-01-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,,,,2000,China,,2023-04-15,,Public/Private,,,China,,,,,,,,,,,,,2000,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2023-09-15,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Coreweave EcoDataCenter,Planned,Unlikely,Yes,,,NVIDIA GB200 NVL2,,Sweden,CoreWeave,,"Unclear how many GPUs will be in this cluster, but likely thousands",Private,,,"Slaggvarpsvägen 21, 791 77 Falun, Sweden",Cloud,GB200,"As part of its European expansion strategy, CoreWeave will leverage thousands of NVIDIA Blackwell GPUs",Planned 2025,,,,,,,,,,NVIDIA,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241216210211/https://ecodatacenter.tech/press/ecodatacenter-brings-the-worlds-most-advanced-ai-technology-to-sweden-3339095,,,,,60.610563,15.610352
CoreWeave Hillsboro Oregon,Planned,Likely,Yes,,,,,United States of America,CoreWeave,,"Unclear when this became operational or will be, and what type of GPUs are used",Private,,,"Hillsboro, Oregon",CoreWeave,Uncertain,"CoreWeave has secured a 36 MW lease with Digital Realty at one of its data centers in Hillsboro, Oregon, to deploy tens of thousands of GPUs in a single facility",Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250126143127/https://dgtlinfra.com/coreweave-data-center-locations/,,,,,45.522668,-122.989767
Google Council Bluff Iowa,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"10410 Bunge Ave, Council Bluffs, IA 51503",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,,41.171499,-95.79784
Google Lancaster Ohio,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"231 Whiley Rd, Lancaster, OH 43130",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,,39.724889,-82.683316
Google Lincoln Nebraska,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"W933+CH4 Lincoln, Nebraska",Google,,The other two sites are not as large yet but are ramping up fast: combining all four campuses will form a GW-scale AI training cluster by 2026. The Lincoln datacenter that is ~50 miles away will be Google’s largest individual site.,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,,40.809193,-96.701189
Google New Albany Ohio,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,450,,"1101 Beech Rd SW, New Albany, OH 43054",Google,,"""450MW campus in Ohio to train Gemini, to expand further to 540MW""
""The New Albany cluster, shown below, is set to become one of Google’s largest and is already hosting TPU v4, v5, v6.""",Planned 2025,,,,,,,,,,Unknown,,,,,,,,,,,450,,,,,,,,,,https://www.youtube.com/watch?v=hobvps-H38o,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,40.06111,-82.76446
Google Omaha Nebraska,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"11110 State St, Omaha, NE 68142",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,,41.339285,-96.086194
Google Papillion Nebraska,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,250,,"14706 Schram Rd, Omaha, NE 68138",Google,,"The Papillion campus shown below adds >250MW of capacity to Google’s operations around Omaha and Council Bluffs, which combined with the above totals north of 500MW of capacity in 2023, of which a large portion is allocated to TPUs",Planned,,,,,,,,,,Unknown,,,,,,,,,,,250,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,,41.133061,-96.144385
Google south Columbus Ohio,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"5076 S High St, Lockbourne, OH 43137",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,,39.85668,-83.0023
Graphcore Good Computer,Planned,Unlikely,Yes,,,,8192,,Graphcore,,"Unclear if this project is still happening, no news since 2022, Graphcore recently acquired by Softbank",Private,,,,,Bow IPU,"""This machine will contain 8,192 IPUs of a generation beyond the Bow processor, but further leveraging 3D wafer-on-wafer stacking technology""

""adding that the machine will deliver “over 10 exaflops” of floating-point performance""",completion unclear,"Unclear if this project is still happening, no news since 2022, Graphcore recently acquired by Softbank",,,,,,,,8192,Unknown,,,,,,,,,,,,,,,,$130M,,,,,https://web.archive.org/web/20220523232610/https://www.enterpriseai.news/2022/03/04/graphcore-announces-wafer-on-wafer-ipu-good-computer/,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2021-09-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,Hong Kong,,2024-09-15,,Public/Private,,,Hong Kong,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,22.279356,114.16255
HPE GreenLake for LLMs Q01,Existing,Unlikely,Unclear,,,,1000,Canada, Hewlett Packard Enterprise,,,Private,,,"2280 Rue Albert-Dion, Lévis, QC G7A 5M9, Canada",Cloud,"H100,L40,L4,""thousands""",GreenLake for LLMs gives users access to AI-native architecture specifically designed for AI and simulation workloads on hundreds or thousands of GPUs and CPUs at once,Planned for end of 2023,Unclear what type of GPUs they use and how many,,,,,,,,1000,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240918133952/https://www.datacenterdynamics.com/en/news/hpe-greenlake-gets-ai-and-hpc-offering/,,,,,46.677999,-71.331342
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2023-10-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2024-10-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
IndiaAI Mission Supercomputer,Planned,Likely,Yes,,,,10000,India,,,"There were earlier talk about potentially getting 25k GPUs, which still seems to be on the table. Unclear if this will be one cluster or several",Public/Private,,,,Cloud,,"""The ecosystem will comprise AI compute infrastructure of 10,000 or more Graphics Processing Units (GPUs), built through public-private partnership.""
""In October 2023, the Indian government initiated a discussion regarding a proposal to set up 25,000 GPUs in the country, with the vision to make compute accessible to Indian companies engaged in the development and use of AI.... Subsequently, in March 2024, the cabinet approved a Rs. 10,372-crore outlay for the IndiaAI Mission, under which the government aims to establish compute capacity of at least 10,000 GPUs through the public-private partnership model.""",Planned,,,,,,,,,10000,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240902090402/https://pib.gov.in/PressReleaseIframePage.aspx?PRID=2012355,https://web.archive.org/web/20240919153825/https://carnegieendowment.org/posts/2024/04/a-primer-on-compute?lang=en,,,,22.199166,78.476681
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2022-05-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Confirmed,Yes,,,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Meta Temple Texas,Planned,Likely,Yes,,,,,United States of America,Meta AI,,"They were building one design for awhile then stopped, tore it down, and started building a new design",Private,,,"NW H K Dodgen Loop & Industrial Blvd
76502 Temple
Texas, USA",Meta,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.youtube.com/watch?v=hobvps-H38o,,,,,31.135894,-97.373699
Microsoft Air Gapped Supercomputer,Existing,Confirmed,Yes,,,,,United States of America,Microsoft,2024-05-07,,Private,,,"Iowa, USA",US intelligence agencies,,"Microsoft Corp. has deployed a generative AI model entirely divorced from the internet, saying US intelligence agencies can now safely harness the powerful technology to analyze top-secret information.",2024-05-07,No details on specifics,,,,,,,,,Unknown,,True,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240626134614/https://qz.com/microsoft-offline-ai-service-chatgpt-gpt4-us-spies-1851462266,,,,,42.086951,-93.496782
Anonymized Chinese System,Planned,Likely,Yes,,,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,,,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Neevcloud planned GPUs,Planned,Likely,No,,,,40000,India,NeevCloud,,"Seems likely that this will be several clusters. Unclear how many GPUs will be in the largest cluster,",Private,,,India,Cloud,,"NeevCloud, aims to build an AI cloud infrastructure customized for Indian users, with a plan to incorporate 40,000 GPUs by 2026",Planned,,,,,,,,,40000,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250110180448/https://entrepreneurshipstudio.com/news/posts/neevcloud-launches-with-40-000-gpus-revolutionizing-ai-cloud-services-for-smes-in-india,,,,,22.199166,78.476681
NHRI Taiwan Supercomputer (NHRI-1),Planned,Likely,Yes,,,,,Taiwan,National Health Research Institutes Taiwan,,Very few details about any specifics,Public,,,Taiwan,Biomedical research,,The National Health Research Institutes (NHRI) in Taiwan has announced a partnership with Nvidia and Asus to deliver the nation’s first biomedical supercomputer,Planned,Very few details,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20220605020815/https://www.hpcwire.com/2022/04/05/nvidia-asus-cloud-team-for-taiwanese-biomedical-supercomputer/,,,,,23.777978,120.930229
NVIDIA Israel Blackwell Supercomputer,Planned,Likely,Yes,,,NVIDIA GB200 NVL2,,Israel,NVIDIA,,Very unclear how big this will actually be,Private,30,,"J3P7+8P Eliakim, Israel",,GB200,"is expected to include ""thousands"" of graphics processing units",Planned,,,,,,,,,,NVIDIA,,,,,,,,,,,30,,,,,$500 million,,,,,https://web.archive.org/web/20250116151639/https://www.aibase.com/news/14757,,,,,32.078128,34.917381
Oracle $10B Saudi Arabia Investment,Planned,Likely,No,,,,,Saudi Arabia,Oracle,,Unclear how this money will be invested,Private,,,,Cloud,Uncertain,Oracle has committed to investing $14 billion in Saudi Arabia over the next 10 years to expand its cloud and AI offerings in the region.,Planned,Unclear what this investment will entail,,,,,,,,,Unknown,,,,,,,,,,,,,,14000000,,,,,,,http://web.archive.org/web/20250515192746/https://www.datacenterdynamics.com/en/news/oracle-commits-to-invest-14bn-in-saudi-arabia-over-next-10-years/,,,,,23.384784,44.652426
OTP SambaNova Supercomputer,Planned,Unlikely,,,,,,,,,"Very unclear if this ever happened or how big it is planned on being. They say ""Europe's fastest AI supercomputer""",Private,,,Europe,,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.datacenterdynamics.com/en/news/otp-bank-selects-sambanova-systems-for-ai-supercomputer/,,,,,,
S. Korea 7th national supercomputer,Planned,Likely,Yes,,,,,Korea (Republic of),Ministry of Science and ICT,,"Unclear what precision they're using, or what type of chips",Public,,,Korea (Republic of),Researchers,,The country plans to start the development of its seventh supercomputer in 2025 with an exaflop of performance,Planned,Only able to find one news report,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240117215326/https://www.kedglobal.com/artificial-intelligence/newsView/ked202305310010,,,,,,
Singtel DC Tuas,Planned,Likely,Yes,,,NVIDIA H100 SXM5 80GB,,Singapore,Singtel,,"No information on how large this cluster will be, but the datacenter will be 58MW",Private,58,,"9 Tuas Ave 3
639408 Singapore",Cloud,"H100,GB200",Singtel’s GPUaaS will be powered by Nvidia H100 Tensor Core GPU-powered clusters that are operated in existing upgraded data centers in Singapore,Planned,,,,,,,,,,NVIDIA,,,,,,,,,,,58,,,,,,,,,,https://web.archive.org/web/20241210030636/https://www.datacenterdynamics.com/en/news/singtel-and-nscale-announce-partnership-to-boost-gpu-capacity-across-europe-and-southeast-asia/,,,,,1.32142,103.655952
SingularityNET,Planned,Likely,No,,,,,,SingularityNET,,Seems like it's decentralized and spread around the world,Private,,,,,,"SingularityNET plans to build the first of several modular compute containers that will act as decentralized hubs for a network of devices from fellow AI developers Fetch.ai, Ocean Protocol, NuNet, HyperCycle, as well as itself. Thanks to their design, the containers can be placed and relocated worldwide.",Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240724001101/https://decrypt.co/241118/singularitynet-53-million-ai-supercomputer-data-centers,,,,,,
SoftBank Planned GB200,Planned,Likely,Yes,,,NVIDIA GB200 NVL2,,Japan,Softbank,,,Private,,,Japan,Softbank,GB200,"In addition to its DGX SuperPOD, SoftBank plans to build another NVIDIA-accelerated supercomputer to run extremely compute-intensive workloads. Initial plans for the supercomputer are based on an NVIDIA Grace Blackwell platform design featuring NVIDIA GB200 NVL72 multi-node, liquid-cooled, rack-scale systems that combine NVIDIA Blackwell GPUs with power-efficient Arm-based NVIDIA Grace™ CPUs",Planned,,,,,,,,,,NVIDIA,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241116120654/https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse,,,,,36.386493,138.59223
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2020-11-15,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Confirmed,Yes,,,,500,China,,2018-07-15,,Public,,,China,,,,,,,,,,,,,500,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
TACC Horizon,Planned,Likely,Yes,,,,,United States of America,University of Texas at Austin,,"Frontera currently has 800 GPUs, which are presumably V100s. For Horizon to have 100x it's AI compute, Horizon would need several thousand GPUs",Public,,,"1300 Louis Henna Blvd, Round Rock, TX 78664","Academia,Researchers",thousands,"Horizon, will provide 10x performance improvement for simulation over the current NSF Leadership-Class Computing system, Frontera, and meet the unique scientific requirements of the NSF community. For AI applications, the leap forward will be even larger, with more than 100x improvement over Frontera",Planned 2026,No clear data yet about number of chips or what type,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240911080245/https://new.nsf.gov/news/nsf-announces-groundbreaking-computing-facility,https://web.archive.org/web/20240905170840/https://www.datacenterdynamics.com/en/news/tacc-chooses-sabey-data-centers-facility-for-new-supercomputer/,,,,30.488228,-97.647182
Tata Group GH200 supercomputer,Planned,Unlikely,Yes,,,NVIDIA GH200,,India,Tata Group,,,Private,,,India,"Tata Group,Cloud",GH200,"The first phase of this large-scale deployment is set to commence by the end of 2024, establishing one of the largest Nvidia Hopper GPU-based cloud supercomputers in India",Planned 2024,Very little information about GPU quantity,,,,,,,,,NVIDIA,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241216183257/https://www.businessworld.in/article/tata-communications-to-set-up-nvidia-hopper-gpu-supercomputer-by-2024e-plans-blackwell-gpu-expansion-in-2025-537162,,,,,22.199166,78.476681
Telangana Yotta Hyderbad AI City Cluster Phase 2,Planned,Likely,Yes,,,,25000,India,Government of Telangana,,Unclear exactly which chips they plan to use,Public,50,,"Hyderabad, India",Cloud,,"Once fully built, the AI Supercomputer powered by 25,000 GPUs and the 50 MW AI Cloud Data Centre campus",Planned,,,Telangana Yotta H1 Hyderbad AI City Cluster Phase 1,,,,,,25000,Unknown,,,,,,,,,,,50,,,,,,,,,,https://web.archive.org/web/20240906230223/https://www.crn.in/news/government-of-telangana-partners-with-yotta-to-launch-indias-largest-ai-supercomputer-of-25000-high-performance-gpus-in-a-purpose-built-50-mw-ai-cloud-data-centre-campus-in-hyderabad/,,,,,17.361362,78.474525
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,2022-09-15,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,True,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Unclear,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Tesla Buffalo Dojo,Planned,Confirmed,Yes,,,Tesla D1 Dojo,,United States of America,Tesla,,,Private,,,"1339 South Park Ave, Buffalo, NY 14220",Tesla,Dojo,"The governor is correct that this is a Dojo Supercomputer, but $500M, while a large sum, is only equivalent to a 10k H100 system from Nvidia. Tesla will spend more than that on Nvidia hardware this year.",Planned,NY governor announced plans for this in official press conference,,,,,,,,,Tesla,,,,,,,,,,,,,,,,$500M,,,,,https://web.archive.org/web/20240127130129/https://teslanorth.com/2024/01/27/tesla-ai-computer-project-buffalo/,,,,,42.85886,-78.841528
Anonymized Chinese System,Existing,Confirmed,Yes,,,,50000.00000000001,China,,2013-07-15,,Public,20,200000000,China,,,,,,,,,,,,,50000.00000000001,Anonymized,Anonymized,True,,,,,,,,30,20,,200000000,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Existing,Likely,Yes,,,,200000.00000000003,China,,2023-12-15,,Public,80,,China,,,,,,,,,,,,,200000.00000000003,Anonymized,Anonymized,True,,,,,,,,80,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Unlikely,Unclear,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Wonder Valley Datacenter Phase 1,Planned,Unlikely,Unclear,,,,,Canada,,,,Private,1400,,"Alberta, Canada",,Uncertain,"""the project's first phase will include developing a 1.4GW redundant power solution and subsequent annual rollout of redundant power in 1GW increments""
""generate and offer 7.5GW of low-cost power to hyperscalers over the next five-10 years""",Planned 2027,,,,,,,,,,Unknown,,,,,,,,,,,1400,,,,,,,,,,https://archive.ph/c5kTp,,,,,54.642672,-114.757176
Anonymized Chinese System,Existing,Likely,Unclear,,,,7000,China,,2023-04-15,,Private,,,China,,,,,,,,,,,,,7000,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,,,,10000,China,,,,Private,,,China,,,,,,,,,,,,,10000,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
Anonymized Chinese System,Planned,Likely,Yes,,,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,,35.486703,101.901875
